

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gpp_math &mdash; MOE 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.1.0 documentation" href="index.html"/>
        <link rel="up" title="C++ Files" href="cpp_tree.html"/>
        <link rel="next" title="gpp_random_test" href="gpp_random_test.html"/>
        <link rel="prev" title="gpp_python_model_selection" href="gpp_python_model_selection.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="index.html">Welcome to MOE&#8217;s documentation!</a><ul>
<li class="toctree-l2"><a class="reference internal" href="index.html#what-is-moe">What is MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#quick-install">Quick Install</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#quick-start">Quick Start</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html#source-documentation">Source Documentation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="index.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="index.html#python-files">Python Files</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="index.html#c-files">C++ Files</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="index.html#indices-and-tables">Indices and tables</a></li>
<li class="toctree-l1"><a class="reference internal" href="why_moe.html">Why Do We Need MOE?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#what-is-moe">What is MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#why-is-this-hard">Why is this hard?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-in-docker">Install in docker:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-from-source">Install from source:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#osx-tips-10-8-for-10-9-see-separate-instructions-below">OSX Tips (&lt;=10.8. For 10.9, see separate instructions below):</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#linux-tips">Linux Tips:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#cmake-tips">CMake Tips:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="objective_functions.html">Objective Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#what-is-an-objective-function">What is an objective function?</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#properties-of-an-objective-function">Properties of an objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#id1"><span class="math">\(\Phi\)</span> Objective Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#example-of-objective-functions">Example of Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples.html#minimizing-an-arbitrary-function">Minimizing an arbitrary function</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#gaussian-process-regression-given-historical-data">Gaussian Process regression given historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#hyperparameter-optimization-of-a-gaussian-process">Hyperparameter optimization of a Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#all-above-examples-combined">All above examples combined</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#setting-thresholds-for-advertising-units">Setting thresholds for advertising units</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#making-a-pull-request">Making a pull request</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style">Style</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="moe.html">moe package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe">Module contents</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="cpp_tree.html">C++ Files</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_test.html">gpp_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain_test.html">gpp_domain_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_exception.html">gpp_exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry.html">gpp_geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization.html">gpp_heuristic_expected_improvement_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra-inl.html">gpp_linear_algebra-inl</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils.html">gpp_test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_logging.html">gpp_logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance.html">gpp_covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_test.html">gpp_python_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain.html">gpp_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_common.html">gpp_python_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyperparameter_optimization_demo.html">gpp_hyperparameter_optimization_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils_test.html">gpp_test_utils_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math_test.html">gpp_math_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_expected_improvement.html">gpp_python_expected_improvement</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_model_selection.html">gpp_python_model_selection</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">gpp_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random_test.html">gpp_random_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_demo.html">gpp_expected_improvement_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random.html">gpp_random</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry_test.html">gpp_geometry_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra.html">gpp_linear_algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization_test.html">gpp_heuristic_expected_improvement_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra_test.html">gpp_linear_algebra_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_core.html">gpp_core</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization.html">gpp_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance_test.html">gpp_covariance_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_gaussian_process.html">gpp_python_gaussian_process</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_mock_optimization_objective_functions.html">gpp_mock_optimization_objective_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python.html">gpp_python</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization.html">gpp_model_selection_and_hyperparameter_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyper_and_EI_demo.html">gpp_hyper_and_EI_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_parameters.html">gpp_optimization_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_common.html">gpp_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization_test.html">gpp_model_selection_and_hyperparameter_optimization_test</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="cpp_tree.html">C++ Files</a> &raquo;</li>
      
    <li>gpp_math</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/gpp_math.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="gpp-math">
<h1>gpp_math<a class="headerlink" href="#gpp-math" title="Permalink to this headline">¶</a></h1>
<p><strong>Contents:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><a class="reference internal" href="#gpp-math-hpp">gpp_math.hpp</a></li>
<li><a class="reference internal" href="#gpp-math-cpp">gpp_math.cpp</a></li>
</ol>
</div></blockquote>
<div class="section" id="gpp-math-hpp">
<h2>gpp_math.hpp<a class="headerlink" href="#gpp-math-hpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><ol class="arabic simple">
<li>OVERVIEW OF GAUSSIAN PROCESSES AND EXPECTED IMPROVEMENT; WHAT ARE WE TRYING TO DO?</li>
<li>FILE OVERVIEW</li>
<li>IMPLEMENTATION NOTES</li>
<li>NOTATION</li>
<li>CITATIONS</li>
</ol>
<p><strong>1. OVERVIEW OF GAUSSIAN PROCESSES AND EXPECTED IMPROVEMENT; WHAT ARE WE TRYING TO DO?</strong></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">these comments are copied in Python: interfaces/__init__.py</p>
</div>
<p>At a high level, this file optimizes an objective function <span class="math">\(\, f(x)\)</span>.  This operation
requires data/uncertainties about prior and concurrent experiments as well as
a covariance function describing how these data [are expected to] relate to each
other.  The points x represent experiments. If <span class="math">\(\, f(x)\)</span> is say, survival rate for
a drug test, the dimensions of x might include dosage amount, dosage frequency,
and overall drug-use time-span.</p>
<p>The objective function is not required in closed form; instead, only the ability
to sample it at points of interest is needed.  Thus, the optimization process
cannot work with <span class="math">\(\, f(x)\)</span> directly; instead a surrogate is built via interpolation
with Gaussian Proccesses (GPs).</p>
<p>Following Rasmussen &amp; Williams (2.2), a Gaussian Process is a collection of random
variables, any finite number of which have a joint Gaussian distribution (Defn 2.1).
Hence a GP is fully specified by its mean function, <span class="math">\(\, m(x)\)</span>, and covariance function,
<span class="math">\(\, k(x,x')\)</span>.  Then we assume that a real process <span class="math">\(\, f(x)\)</span> (e.g., drug survival rate) is
distributed like:</p>
<div class="math">
\[f(x) ~ GP(m(x), k(x,x'))\]</div>
<p>with</p>
<div class="math">
\[m(x) = E[f(x)], k(x,x') = E[(f(x) - m(x))*(f(x') - m(x'))].\]</div>
<p>Then sampling from <span class="math">\(\, f(x)\)</span> is simply drawing from a Gaussian with the appropriate mean
and variance.</p>
<p>However, since we do not know <span class="math">\(\, f(x)\)</span>, we cannot precisely build its corresponding GP.
Instead, using samples from <span class="math">\(\, f(x)\)</span> (e.g., by measuring experimental outcomes), we can
iteratively improve our estimate of <span class="math">\(\, f(x)\)</span>.  See GaussianProcess class docs
and implementation docs for details on how this is done.</p>
<p>The optimization process models the objective using a Gaussian process (GP) prior
(also called a GP predictor) based on the specified covariance and the input
data (e.g., through member functions ComputeMeanOfPoints, ComputeVarianceOfPoints).  Using the GP,
we can compute the expected improvement (EI) from sampling any particular point.  EI
is defined relative to the best currently known value, and it represents what the
algorithm believes is the most likely outcome from sampling a particular point in parameter
space (aka conducting a particular experiment).</p>
<p>See ExpectedImprovementEvaluator and OnePotentialSampleExpectedImprovementEvaluator class
docs for further details on computing EI.  Both support ComputeExpectedImprovement() and
ComputeGradExpectedImprovement().</p>
<p>The dimension of the GP is equal to the number of simultaneous experiments being run;
i.e., the GP may be multivariate.  The behavior of the GP is controlled by its underlying
covariance function and the data/uncertainty of prior points (experiments).</p>
<p>With the ability the compute EI, the final step is to optimize
to find the best EI.  This is done using multistart gradient descent (MGD), in
ComputeOptimalPointsToSample(). This method wraps a MGD call and falls back on random search
if that fails. See gpp_optimization.hpp for multistart/optimization templates. This method
can evaluate and optimize EI at serval points simultaneously; e.g., if we wanted to run 4 simultaneous
experiments, we can use EI to select all 4 points at once.</p>
<p>The literature (e.g., Ginsbourger 2008) refers to these problems collectively as q-EI, where q
is a positive integer. So 1-EI is the originally dicussed usage, and the previous scenario with
multiple simultaneous points/experiments would be called 4-EI.</p>
<p>Additionally, there are use cases where we have existing experiments that are not yet complete but
we have an opportunity to start some new trials. For example, maybe we are a drug company currently
testing 2 combinations of dosage levels. We got some new funding, and can now afford to test
3 more sets of dosage parameters. Ideally, the decision on the new experiments should depend on
the existence of the 2 ongoing tests. We may not have any data from the ongoing experiments yet;
e.g., they are [double]-blind trials. If nothing else, we would not want to duplicate any
existing experiments! So we want to solve 3-EI using the knowledge of the 2 ongoing experiments.</p>
<p>We call this q,p-EI, so the previous example would be 3,2-EI. The q-EI notation is equivalent to
q,0-EI; if we do not explicitly write the value of p, it is 0. So q is the number of new
(simultaneous) experiments to select. In code, this would be the size of the output from EI
optimization (i.e., <tt class="docutils literal"><span class="pre">best_points_to_sample</span></tt>, of which there are <tt class="docutils literal"><span class="pre">q</span> <span class="pre">=</span> <span class="pre">num_to_sample</span> <span class="pre">points</span></tt>).
p is the number of ongoing/incomplete experiments to take into account (i.e., <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>
of which there are <tt class="docutils literal"><span class="pre">p</span> <span class="pre">=</span> <span class="pre">num_being_sampled</span></tt> points).</p>
<p>Back to optimization: the idea behind gradient descent is simple.  The gradient gives us the
direction of steepest ascent (negative gradient is steepest descent).  So each iteration, we
compute the gradient and take a step in that direction.  The size of the step is not specified
by GD and is left to the specific implementation.  Basically if we take steps that are
too large, we run the risk of over-shooting the solution and even diverging.  If we
take steps that are too small, it may take an intractably long time to reach the solution.
Thus the magic is in choosing the step size; we do not claim that our implementation is
perfect, but it seems to work reasonably.  See <tt class="docutils literal"><span class="pre">gpp_optimization.hpp</span></tt> for more details about
GD as well as the template definition.</p>
<p>For particularly difficult problems or problems where gradient descent&#8217;s parameters are not
well-chosen, GD can fail to converge.  If this happens, we can fall back on heuristics;
e.g., &#8216;dumb&#8217; search (i.e., evaluate EI at a large number of random points and take the best
one). Naive search lives in: ComputeOptimalPointsToSampleViaLatinHypercubeSearch&lt;&gt;().</p>
<p><strong>2. FILE OVERVIEW</strong></p>
<p>This file contains mathematical functions supporting optimal learning.
These include functions to compute characteristics of Gaussian Processes
(e.g., variance, mean) and the gradients of these quantities as well as functions to
compute and optimize the expected improvement.</p>
<p>Functions here generally require some combination of a CovarianceInterface object as well as
data about prior and current (i.e., concurrent) experiments.  These data are encapsulated in
the GaussianProcess class.  Then we build an ExpectedImprovementEvaluator object (with
associated state, see <tt class="docutils literal"><span class="pre">gpp_common.hpp</span></tt> item 5 for (Evaluator, State) relations) on top of a
GaussianProcess for computing and optimizing EI.</p>
<p>For further theoretical details about Gaussian Processes, see
Rasmussen and Williams, Gaussian Processes for Machine Learning (2006).
A bare-bones summary is provided in <tt class="docutils literal"><span class="pre">gpp_math.cpp</span></tt>.</p>
<p>For further details about expected improvement and the optimization thereof,
see Scott Clark&#8217;s PhD thesis.  Again, a summary is provided in <tt class="docutils literal"><span class="pre">gpp_math.cpp</span></tt>&#8216;s file comments.</p>
<p><strong>3. IMPLEMENTATION NOTES</strong></p>
<ol class="loweralpha">
<li><p class="first">This file has a few primary endpoints for EI optimization:</p>
<ol class="lowerroman">
<li><p class="first">ComputeOptimalPointsToSampleWithRandomStarts&lt;&gt;():</p>
<p>Solves the q,p-EI problem.</p>
<p>Takes in a gaussian_process describing the prior, domain, config, etc.; outputs the next best point(s) (experiment)
to sample (run). Uses gradient descent.</p>
</li>
<li><p class="first">ComputeOptimalPointsToSampleViaLatinHypercubeSearch&lt;&gt;():</p>
<p>Estimates the q,p-EI problem.</p>
<p>Takes in a gaussian_process describing the prior, domain, etc.; outputs the next best point(s) (experiment)
to sample (run). Uses &#8216;dumb&#8217; search.</p>
</li>
<li><p class="first">ComputeOptimalPointsToSample&lt;&gt;() (Recommended):</p>
<p>Solves the q,p-EI problem.</p>
<p>Wraps the previous two items; relies on gradient descent and falls back to &#8220;dumb&#8221; search if it fails.</p>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p>See <tt class="docutils literal"><span class="pre">gpp_math.cpp</span></tt>&#8216;s header comments for more detailed implementation notes.</p>
<p class="last">There are also several other functions with external linkage in this header; these
are provided primarily to ease testing and to permit lower level access from python.</p>
</div>
</li>
<li><p class="first">See <tt class="docutils literal"><span class="pre">gpp_common.hpp</span></tt> header comments for additional implementation notes.</p>
</li>
</ol>
<p><strong>4. NOTATION</strong></p>
<p>And domain-specific notation, following Rasmussen, Williams:</p>
<blockquote>
<div><ul class="simple">
<li><tt class="docutils literal"><span class="pre">X</span> <span class="pre">=</span> <span class="pre">points_sampled</span></tt>; this is the training data (size <tt class="docutils literal"><span class="pre">dim</span></tt> X <tt class="docutils literal"><span class="pre">num_sampled</span></tt>), also called the design matrix</li>
<li><tt class="docutils literal"><span class="pre">Xs</span> <span class="pre">=</span> <span class="pre">points_to_sample</span></tt>; this is the test data (size <tt class="docutils literal"><span class="pre">dim</span></tt> X num_to_sample``)</li>
<li><tt class="docutils literal"><span class="pre">y,</span> <span class="pre">f,</span> <span class="pre">f(x)</span> <span class="pre">=</span> <span class="pre">points_sampled_value</span></tt>, the experimental results from sampling training points</li>
<li><tt class="docutils literal"><span class="pre">K,</span> <span class="pre">K_{ij},</span> <span class="pre">K(X,X)</span> <span class="pre">=</span> <span class="pre">covariance(X_i,</span> <span class="pre">X_j)</span></tt>, covariance matrix between training inputs (<tt class="docutils literal"><span class="pre">num_sampled</span> <span class="pre">x</span> <span class="pre">num_sampled</span></tt>)</li>
<li><tt class="docutils literal"><span class="pre">Ks,</span> <span class="pre">Ks_{ij},</span> <span class="pre">K(X,Xs)</span> <span class="pre">=</span> <span class="pre">covariance(X_i,</span> <span class="pre">Xs_j)</span></tt>, covariance matrix between training and test inputs (<tt class="docutils literal"><span class="pre">num_sampled</span> <span class="pre">x</span> <span class="pre">num_to_sample</span></tt>)</li>
<li><tt class="docutils literal"><span class="pre">Kss,</span> <span class="pre">Kss_{ij},</span> <span class="pre">K(Xs,Xs)</span> <span class="pre">=</span> <span class="pre">covariance(Xs_i,</span> <span class="pre">Xs_j)</span></tt>, covariance matrix between test inputs (<tt class="docutils literal"><span class="pre">num_to_sample</span> <span class="pre">x</span> <span class="pre">num_to_sample</span></tt>)</li>
<li><tt class="docutils literal"><span class="pre">\theta</span></tt>: (vector) of hyperparameters for a covariance function</li>
</ul>
</div></blockquote>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Due to confusion with multiplication (K_* looks awkward in code comments), Rasmussen &amp; Williams&#8217; <span class="math">\(\, K_*\)</span>
notation has been repalced with <tt class="docutils literal"><span class="pre">Ks</span></tt> and <span class="math">\(\, K_{**}\)</span> is <tt class="docutils literal"><span class="pre">Kss</span></tt>.</p>
</div>
<p>Connecting to the q,p-EI notation, both the points represented by &#8220;q&#8221; and &#8220;p&#8221; are represented by <tt class="docutils literal"><span class="pre">Xs</span></tt>. Within
the GP, there is no distinction between points being sampled by ongoing experiments and new points to sample.</p>
<p><strong>5. CITATIONS</strong></p>
<p>a. Gaussian Processes for Machine Learning.
Carl Edward Rasmussen and Christopher K. I. Williams. 2006.
Massachusetts Institute of Technology.  55 Hayward St., Cambridge, MA 02142.
<a class="reference external" href="http://www.gaussianprocess.org/gpml/">http://www.gaussianprocess.org/gpml/</a> (free electronic copy)</p>
<p>b. Parallel Machine Learning Algorithms In Bioinformatics and Global Optimization (PhD Dissertation).
Part II, EPI: Expected Parallel Improvement
Scott Clark. 2012.
Cornell University, Center for Applied Mathematics.  Ithaca, NY.
<a class="reference external" href="https://github.com/sc932/Thesis">https://github.com/sc932/Thesis</a>
<a class="reference external" href="mailto:sclark&#37;&#52;&#48;yelp&#46;com">sclark<span>&#64;</span>yelp<span>&#46;</span>com</a></p>
<p>c. Differentiation of the Cholesky Algorithm.
S. P. Smith. 1995.
Journal of Computational and Graphical Statistics. Volume 4. Number 2. p134-147</p>
<p>d. A Multi-points Criterion for Deterministic Parallel Global Optimization based on Gaussian Processes.
David Ginsbourger, Rodolphe Le Riche, and Laurent Carraro.  2008.
D´epartement 3MI. Ecole Nationale Sup´erieure des Mines. 158 cours Fauriel, Saint-Etienne, France.
{ginsbourger, leriche, <a class="reference external" href="mailto:carraro}&#37;&#52;&#48;emse&#46;fr">carraro}<span>&#64;</span>emse<span>&#46;</span>fr</a></p>
<p>e. Efficient Global Optimization of Expensive Black-Box Functions.
Jones, D.R., Schonlau, M., Welch, W.J. 1998.
Journal of Global Optimization, 13, 455-492.</p>
 </p>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Functions</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1a349b2b417f39d01939fcfdfcff8d1326"></span><div class="line-block">
<div class="line"><a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1a43aad231fa6009d48201a76fd7dfb6dc"><em>OL_NONNULL_POINTERS</em></a>  void <strong>SetupExpectedImprovementState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>OnePotentialSampleExpectedImprovementEvaluator</em></a>  &amp; ei_evaluator, double const *restrict starting_point, int max_num_threads, bool configure_for_gradients, std::vector&lt; typename  <a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>OnePotentialSampleExpectedImprovementEvaluator::StateType</em></a>  &gt; * state_vector)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Set up vector of OnePotentialSampleExpectedImprovementEvaluator::StateType.</p>
<p>This is a utility function just for reducing code duplication.</p>
<p>dim is the spatial dimension, <tt class="docutils literal"><span class="pre">ei_evaluator.dim()</span></tt>
<strong>Parameters</strong>:</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">evaluator object associated w/the state objects being constructed</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">starting_point[dim]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">initial point to load into state (must be a valid point for the problem)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">configure_for_gradients:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">true if these state objects will be used to compute gradients, false otherwise</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">state_vector[arbitrary]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">vector of state objects, arbitrary size (usually 0)</td>
</tr>
</tbody>
</table>
</div></blockquote>
<dl class="docutils">
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">state_vector[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">vector of states containing <tt class="docutils literal"><span class="pre">max_num_threads</span></tt> properly initialized state objects</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a9e5d639a6981b621152394f2c29837c7"></span><div class="line-block">
<div class="line"><a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1a43aad231fa6009d48201a76fd7dfb6dc"><em>OL_NONNULL_POINTERS</em></a>  void <strong>SetupExpectedImprovementState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>ExpectedImprovementEvaluator</em></a>  &amp; ei_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int max_num_threads, bool configure_for_gradients, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, std::vector&lt; typename  <a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>ExpectedImprovementEvaluator::StateType</em></a>  &gt; * state_vector)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Set up vector of ExpectedImprovementEvaluator::StateType.</p>
<p>This is a utility function just for reducing code duplication.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">evaluator object associated w/the state objects being constructed</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">initial points to load into state (must be a valid point for the problem);
i.e., points at which to evaluate EI and/or its gradient</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrently experiments</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the p in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">configure_for_gradients:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">true if these state objects will be used to compute gradients, false otherwise</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">state_vector[arbitrary]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">vector of state objects, arbitrary size (usually 0)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">state_vector[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">vector of states containing <tt class="docutils literal"><span class="pre">max_num_threads</span></tt> properly initialized state objects</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1aae7a4aafed982bcbbc873490d67eb986"></span><div class="line-block">
<div class="line">template &lt; typename ExpectedImprovementEvaluator, typename DomainType &gt;</div>
<div class="line">void <strong>RestartedGradientDescentEIOptimization</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>ExpectedImprovementEvaluator</em></a>  &amp; ei_evaluator, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const DomainType &amp; domain, double const *restrict initial_guess, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Solve the q,p-EI problem (see ComputeOptimalPointsToSample and/or header docs) by optimizing the Expected Improvement.
Optimization is done using restarted Gradient Descent, via GradientDescentOptimizer&lt;...&gt;::Optimize() from
<tt class="docutils literal"><span class="pre">gpp_optimization.hpp</span></tt>.  Please see that file for details on gradient descent and see gpp_optimization_parameters.hpp
for the meanings of the GradientDescentParameters.</p>
<p>This function is just a simple wrapper that sets up the Evaluator&#8217;s State and calls a general template for restarted GD.</p>
<p>This function does not perform multistarting or employ any other robustness-boosting heuristcs; it only
converges if the <tt class="docutils literal"><span class="pre">initial_guess</span></tt> is close to the solution. In general,
ComputeOptimalPointsToSample() (see below) is preferred. This function is meant for:
1. easier testing;
2. if you really know what you&#8217;re doing.</p>
<p>Solution is guaranteed to lie within the region specified by <tt class="docutils literal"><span class="pre">domain</span></tt>; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">reference to object that can compute ExpectedImprovement and its spatial gradient</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">optimization_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">GradientDescentParameters object that describes the parameters controlling EI optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">initial_guess[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">initial guess for gradient descent</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name">normal_rng[1]:</th><td class="field-body">a NormalRNG object that provides the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">normal_rng[1]:</th><td class="field-body">NormalRNG object will have its state changed due to random draws</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">next_point[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points yielding the best EI according to gradient descent</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a23d19a43d4f3db85bc0fa9a434e95290"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line"><a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1a43aad231fa6009d48201a76fd7dfb6dc"><em>OL_NONNULL_POINTERS</em></a>  void <strong>ComputeOptimalPointsToSampleViaMultistartGradientDescent</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const DomainType &amp; domain, double const *restrict start_point_set, double const *restrict points_being_sampled, int num_multistarts, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, bool *restrict found_flag, double *restrict best_next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Perform multistart gradient descent (MGD) to solve the q,p-EI problem (see ComputeOptimalPointsToSample and/or
header docs).  Starts a GD run from each point in <tt class="docutils literal"><span class="pre">start_point_set</span></tt>.  The point corresponding to the
optimal EI* is stored in <tt class="docutils literal"><span class="pre">best_next_point</span></tt>.</p>
<p>* Multistarting is heuristic for global optimization. EI is not convex so this method may not find the true optimum.</p>
<p>This function wraps MultistartOptimizer&lt;&gt;::MultistartOptimize() (see <tt class="docutils literal"><span class="pre">gpp_optimization.hpp</span></tt>), which provides the multistarting
component. Optimization is done using restarted Gradient Descent, via GradientDescentOptimizer&lt;...&gt;::Optimize() from
<tt class="docutils literal"><span class="pre">gpp_optimization.hpp</span></tt>. Please see that file for details on gradient descent and see <tt class="docutils literal"><span class="pre">gpp_optimization_parameters.hpp</span></tt>
for the meanings of the GradientDescentParameters.</p>
<p>This function (or its wrappers, e.g., ComputeOptimalPointsToSampleWithRandomStarts) are the primary entry-points for
gradient descent based EI optimization in the <tt class="docutils literal"><span class="pre">optimal_learning</span></tt> library.</p>
<p>Users may prefer to call ComputeOptimalPointsToSample(), which applies other heuristics to improve robustness.</p>
<p>Currently, during optimization, we recommend that the coordinates of the initial guesses not differ from the
coordinates of the optima by more than about 1 order of magnitude. This is a very (VERY!) rough guideline for
sizing the domain and num_multistarts; i.e., be wary of sets of initial guesses that cover the space too sparsely.</p>
<p>Solution is guaranteed to lie within the region specified by <tt class="docutils literal"><span class="pre">domain</span></tt>; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This function fails ungracefully if NO improvement can be found!  In that case,
<tt class="docutils literal"><span class="pre">best_next_point</span></tt> will always be the first point in <tt class="docutils literal"><span class="pre">start_point_set</span></tt>.
<tt class="docutils literal"><span class="pre">found_flag</span></tt> will indicate whether this occured.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">optimization_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">GradientDescentParameters object that describes the parameters controlling EI optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">start_point_set[dim][num_to_sample][num_multistarts]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">set of initial guesses for MGD (one block of num_to_sample points per multistart)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_multistarts:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points in set of initial guesses</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be <tt class="docutils literal"><span class="pre">min(points_sampled_value)</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name">max_int_steps:</th><td class="field-body">maximum number of MC iterations</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">NormalRNG objects will have their state changed due to random draws</td>
</tr>
<tr class="field-even field"><th class="field-name">found_flag[1]:</th><td class="field-body">true if <tt class="docutils literal"><span class="pre">best_next_point</span></tt> corresponds to a nonzero EI</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">best_next_point[dim][num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points yielding the best EI according to MGD</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a237851c9cbeef44ccab5a933eaec4900"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line">void <strong>ComputeOptimalPointsToSampleWithRandomStarts</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const DomainType &amp; domain, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict best_next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Perform multistart gradient descent (MGD) to solve the q,p-EI problem (see ComputeOptimalPointsToSample and/or
header docs), starting from <tt class="docutils literal"><span class="pre">num_multistarts</span></tt> points selected randomly from the within th domain.</p>
<p>This function is a simple wrapper around ComputeOptimalPointsToSampleViaMultistartGradientDescent(). It additionally
generates a set of random starting points and is just here for convenience when better initial guesses are not
available.</p>
<p>See ComputeOptimalPointsToSampleViaMultistartGradientDescent() for more details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">optimization_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">GradientDescentParameters object that describes the parameters controlling EI optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be <tt class="docutils literal"><span class="pre">min(points_sampled_value)</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name">max_int_steps:</th><td class="field-body">maximum number of MC iterations</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">found_flag[1]:</th><td class="field-body">true if best_next_point corresponds to a nonzero EI</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">NormalRNG objects will have their state changed due to random draws</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">best_next_point[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points yielding the best EI according to MGD</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a15ea12234238659952ebe4bb3be97b48"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line">void <strong>EvaluateEIAtPointList</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const DomainType &amp; domain, double const *restrict initial_guesses, double const *restrict points_being_sampled, int num_multistarts, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict function_values, double *restrict best_next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Function to evaluate Expected Improvement (q,p-EI) over a specified list of <tt class="docutils literal"><span class="pre">num_multistarts</span></tt> points.
Optionally outputs the EI at each of these points.
Outputs the point of the set obtaining the maximum EI value.</p>
<p>Generally gradient descent is preferred but when they fail to converge this may be the only &#8220;robust&#8221; option.
This function is also useful for plotting or debugging purposes (just to get a bunch of EI values).</p>
<p>This function is just a wrapper that builds the required state objects and a NullOptimizer object and calls
MultistartOptimizer&lt;...&gt;::MultistartOptimize(...); see gpp_optimization.hpp.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">initial_guesses[dim][num_to_sample][num_multistarts]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">list of points at which to compute EI</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_multistarts:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points to check</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be <tt class="docutils literal"><span class="pre">min(points_sampled_value)</span></tt>)</td>
</tr>
<tr class="field-odd field"><th class="field-name">max_int_steps:</th><td class="field-body">maximum number of MC iterations</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">found_flag[1]:</th><td class="field-body">true if best_next_point corresponds to a nonzero EI</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">NormalRNG objects will have their state changed due to random draws</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">function_values[num_multistarts]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">EI evaluated at each point of <tt class="docutils literal"><span class="pre">initial_guesses</span></tt>, in the same order as
<tt class="docutils literal"><span class="pre">initial_guesses</span></tt>; never dereferenced if nullptr</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">best_next_point[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points yielding the best EI according to dumb search</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1af919ab072fd9bdb623571a3eeb0244bd"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line">void <strong>ComputeOptimalPointsToSampleViaLatinHypercubeSearch</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const DomainType &amp; domain, double const *restrict points_being_sampled, int num_multistarts, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict best_next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Perform a random, naive search to &#8220;solve&#8221; the q,p-EI problem (see ComputeOptimalPointsToSample and/or
header docs).  Evaluates EI at <tt class="docutils literal"><span class="pre">num_multistarts</span></tt> points (e.g., on a latin hypercube) to find the
point with the best EI value.</p>
<p>Generally gradient descent is preferred but when they fail to converge this may be the only &#8220;robust&#8221; option.</p>
<p>Solution is guaranteed to lie within the region specified by <tt class="docutils literal"><span class="pre">domain</span></tt>; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<p>Wraps EvaluateEIAtPointList(); constructs the input point list with a uniform random sampling from the given Domain object.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_multistarts:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of random points to check</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be <tt class="docutils literal"><span class="pre">min(points_sampled_value)</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name">max_int_steps:</th><td class="field-body">maximum number of MC iterations</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd>found_flag[1]: true if best_next_point corresponds to a nonzero EI
:uniform_generator[1]: UniformRandomGenerator object will have its state changed due to random draws
:normal_rng[max_num_threads]: NormalRNG objects will have their state changed due to random draws
:best_next_point[dim][num_to_sample]: points yielding the best EI according to dumb search</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
<p><p id="project0classoptimal__learning_1_1_gaussian_process"><em>class</em> <strong>GaussianProcess</strong></p>
<blockquote>
<div><p></p>
<p><p>Object that encapsulates Gaussian Process Priors (GPPs).  A GPP is defined by a set of
(sample point, function value, noise variance) triples along with a covariance function that relates the points.
Each point has dimension dim.  These are the training data; for example, each sample point might specify an experimental
cohort and the corresponding function value is the objective measured for that experiment.  There is one noise variance
value per function value; this is the measurement error and is treated as N(0, noise_variance) Gaussian noise.</p>
<p>GPPs estimate a real process <span class="math">\(\, f(x) = GP(m(x), k(x,x'))\)</span> (see file docs).  This class deals with building an estimator
to the actual process using measurements taken from the actual process&#8211;the (sample point, function val, noise) triple.
Then predictions about unknown points can be made by sampling from the GPP&#8211;in particular, finding the (predicted)
mean and variance.  These functions (and their gradients) are provided in ComputeMeanOfPoints, ComputeVarianceOfPoints,
etc.</p>
<p>Further mathematical details are given in the implementation comments, but we are essentially computing:</p>
<div class="line-block">
<div class="line">ComputeMeanOfPoints    : <tt class="docutils literal"><span class="pre">K(Xs,</span> <span class="pre">X)</span> <span class="pre">*</span> <span class="pre">[K(X,X)</span> <span class="pre">+</span> <span class="pre">\sigma_n^2</span> <span class="pre">I]^{-1}</span> <span class="pre">*</span> <span class="pre">y</span></tt></div>
<div class="line">ComputeVarianceOfPoints: <tt class="docutils literal"><span class="pre">K(Xs,</span> <span class="pre">Xs)</span> <span class="pre">-</span> <span class="pre">K(Xs,X)</span> <span class="pre">*</span> <span class="pre">[K(X,X)</span> <span class="pre">+</span> <span class="pre">\sigma_n^2</span> <span class="pre">I]^{-1}</span> <span class="pre">*</span> <span class="pre">K(X,Xs)</span></tt></div>
</div>
<p>This (estimated) mean and variance characterize the predicted distributions of the actual <span class="math">\(\, m(x), k(x,x')\)</span>
functions that underly our GP.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">the preceding comments are copied in Python: interfaces/gaussian_process_interface.py</p>
</div>
<p>For testing and experimental purposes, this class provides a framework for sampling points from the GP (i.e., given a
point to sample and predicted measurement noise) as well as adding additional points to an already-formed GP.  Sampling
points requires drawing from <span class="math">\(\, N(0,1)\)</span> so this class also holds PRNG state to do so via the NormalRNG object from gpp_random.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Functions that manipulate the PRNG directly or indirectly (changing state, generating points)
are NOT THREAD-SAFE. All thread-safe functions are marked const.</p>
</div>
<p>These mean/variance methods require some external state: namely, the set of potential points to sample.  Additionally,
temporaries and derived quantities depending on these &#8220;points to sample&#8221; eliminate redundant computation.  This external
state is handled through PointsToSampleState objects, which are constructed separately and filled through
PointsToSampleState::SetupState() which interacts with functions in this class.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a0a4f1f728c228534c56038bf27b0967e"></span>typedef <a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>PointsToSampleState</em></a> <strong>StateType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a1ba20aed0f2dfdf3dd36e901d36cdb72"></span>typedef <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a> <strong>NormalGeneratorType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1aa23b7ee98a2974b94ceade9227515b1a"></span>typedef NormalGeneratorType::EngineType <strong>EngineType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a6648cad9df1dd7aa31aaa275fc375629"></span><div class="line-block">
<div class="line"> <strong>GaussianProcess</strong>(const  <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  &amp; covariance_in, double const *restrict points_sampled_in, double const *restrict points_sampled_value_in, double const *restrict noise_variance_in, int dim_in, int num_sampled_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a GaussianProcess object.  All inputs are required; no default constructor nor copy/assignment are allowed.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last"><tt class="docutils literal"><span class="pre">points_sampled</span></tt> is not allowed to contain duplicate points; doing so results in singular covariance matrices.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">covariance:</th><td class="field-body">the CovarianceFunction object encoding assumptions about the GP&#8217;s behavior on our data</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_sampled[dim][num_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points that have already been sampled</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_sampled_value[num_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">values of the already-sampled points</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">noise_variance[num_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the <tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt> (noise variance) associated w/observation, points_sampled_value</td>
</tr>
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the spatial dimension of a point (i.e., number of independent params in experiment)</td>
</tr>
<tr class="field-even field"><th class="field-name">num_sampled:</th><td class="field-body">number of already-sampled points</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a27747082cb20e7b38a6c9c777a96aeab"></span><div class="line-block">
<div class="line">int <strong>dim</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a036e4847fcadc3925795d01f42efe914"></span><div class="line-block">
<div class="line">int <strong>num_sampled</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a156210b8fb610a5a097287089c691ba8"></span><div class="line-block">
<div class="line">const std::vector&lt; double &gt; &amp; <strong>points_sampled</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a85a114af06e5ef937c4922d47ccb5db4"></span><div class="line-block">
<div class="line">const std::vector&lt; double &gt; &amp; <strong>points_sampled_value</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a792bf34690daaf4375d31d9781928937"></span><div class="line-block">
<div class="line">const std::vector&lt; double &gt; &amp; <strong>noise_variance</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1ac009c7a6be7123ef11811eedb1d2c720"></span><div class="line-block">
<div class="line">void <strong>SetCovarianceHyperparameters</strong>(double const *restrict hyperparameters_new)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Change the hyperparameters of this GP&#8217;s covariance function.
Also forces recomputation of all derived quantities for GP to remain consistent.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Using this function invalidates any PointsToSampleState objects created with &#8220;this&#8221; object.
For any such objects &#8220;state&#8221;, call state.SetupState(...) to restore them.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">hyperparameters_new[<a href="#id1"><span class="problematic" id="id2">covariance_ptr_</span></a>-&gt;GetNumberOfHyperparameters]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">new hyperparameter array</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a8018d81eec8b9085170f440980ab0853"></span><div class="line-block">
<div class="line">void <strong>FillPointsToSampleState</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Sets up the PointsToSampleState object so that it can be used to compute GP mean, variance, and gradients thereof.
ASSUMES all needed space is ALREADY ALLOCATED.</p>
<p>This function should not be called directly; instead use PointsToSampleState::SetupState().</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">pointer to a PointsToSampleState object where all space has been properly allocated</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">pointer to a fully configured PointsToSampleState object. overwrites input</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Sets up precomputed quantities needed for mean, variance, and gradients thereof.  These quantities are:
<tt class="docutils literal"><span class="pre">Ks</span> <span class="pre">:=</span> <span class="pre">Ks_{k,i}</span> <span class="pre">=</span> <span class="pre">cov(X_k,</span> <span class="pre">Xs_i)</span></tt> (used by mean, variance)
Then if we need gradients:
<tt class="docutils literal"><span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">Ks</span> <span class="pre">:=</span> <span class="pre">solution</span> <span class="pre">X</span> <span class="pre">of</span> <span class="pre">K_{k,l}</span> <span class="pre">*</span> <span class="pre">X_{l,i}</span> <span class="pre">=</span> <span class="pre">Ks{k,i}</span></tt> (used by variance, grad variance)
<tt class="docutils literal"><span class="pre">gradient</span> <span class="pre">of</span> <span class="pre">Ks</span> <span class="pre">:=</span> <span class="pre">C_{d,k,i}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{k,i}}{Xs_{d,i}}</span></tt> (used by grad mean, grad variance)</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1abf09df3ce2d61e0401b3a9511e727776"></span><div class="line-block">
<div class="line">void <strong>AddPointToGP</strong>(double const *restrict new_point, double new_point_value, double noise_variance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Adds a single (point, fcn value) pair to the GP with the option of noise variance (set to 0.0 if undesired).</p>
<p>Also forces recomputation of all derived quantities for GP to remain consistent.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">new_point[dim]:</th><td class="field-body">coordinates of the new point to add</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">new_point_value:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">function value at the new point</td>
</tr>
<tr class="field-odd field"><th class="field-name">noise_variance:</th><td class="field-body">sigma_n^2 corresponding to the signal noise in measuring new_point_value</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a72b8ea7d5de98271c11be61869bcff21"></span><div class="line-block">
<div class="line">double <strong>SamplePointFromGP</strong>(double const *restrict point_to_sample, double noise_variance_this_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Sample a function value from a Gaussian Process prior, provided a point at which to sample.</p>
<p>Uses the formula <tt class="docutils literal"><span class="pre">function_value</span> <span class="pre">=</span> <span class="pre">gpp_mean</span> <span class="pre">+</span> <span class="pre">sqrt(gpp_variance)</span> <span class="pre">*</span> <span class="pre">w1</span> <span class="pre">+</span> <span class="pre">sqrt(noise_variance)</span> <span class="pre">*</span> <span class="pre">w2</span></tt>, where <tt class="docutils literal"><span class="pre">w1,</span> <span class="pre">w2</span></tt>
are draws from <span class="math">\(\, N(0,1)\)</span>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Set noise_variance to 0 if you want &#8220;accurate&#8221; draws from the GP.
BUT if the drawn (point, value) pair is meant to be added back into the GP (e.g., for testing), then this point
MUST be drawn with noise_variance equal to the noise associated with &#8220;point&#8221; as a member of &#8220;points_sampled&#8221;</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">point_to_sample[dim]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">coordinates of the point at which to generate a function value (from GP)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">noise_variance_this_point:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">if this point is to be added into the GP, it needs to be generated with its associated noise var</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>function value drawn from this GP</dd>
</dl>
</p>
<p><p>Samples function values from a GPP given a list of points.</p>
<p>Samples by: <tt class="docutils literal"><span class="pre">function_value</span> <span class="pre">=</span> <span class="pre">gpp_mean</span> <span class="pre">+</span> <span class="pre">gpp_variance</span> <span class="pre">*</span> <span class="pre">w</span></tt>, where <tt class="docutils literal"><span class="pre">w</span></tt> is a single draw from N(0,1).</p>
<p>We only draw one point at a time (i.e., <tt class="docutils literal"><span class="pre">num_to_sample</span></tt> fixed at 1).  We want multiple draws from the same GPP;
drawing many points per step would be akin to sampling multiple GPPs. Thus gpp_mean, gpp_variance, and w all have size 1.</p>
<p>If the GPP does not receive any data, then on the first step, gpp_mean = 0 and gpp_variance is just the &#8220;covariance&#8221;
of a single point. Then we iterate through the remaining points in points_sampled, generating gpp_mean, gpp_variance,
and a sample function value.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a7873e09a46323767e62d0c2f5ba918ee"></span><div class="line-block">
<div class="line">void <strong>ComputeMeanOfPoints</strong>(const  <a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  &amp; points_to_sample_state, double *restrict mean_of_points)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the mean of this GP at each of <tt class="docutils literal"><span class="pre">Xs</span></tt> (<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> should not contain duplicate points.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments are copied in Python: interfaces/gaussian_process_interface.py</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState)</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">mean_of_points[num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">mean of GP, one per GP dimension</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Calculates the mean (from the GPP) of a set of points:
<tt class="docutils literal"><span class="pre">mus</span> <span class="pre">=</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">y</span></tt>
See Rasmussen and Willians page 19 alg 2.1</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a7848596618461dc6e219b9dbd7099e4c"></span><div class="line-block">
<div class="line">void <strong>ComputeGradMeanOfPoints</strong>(const  <a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  &amp; points_to_sample_state, double *restrict grad_mu)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the gradient of the mean of this GP at each of <tt class="docutils literal"><span class="pre">Xs</span></tt> (<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>) wrt <tt class="docutils literal"><span class="pre">Xs</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> should not contain duplicate points.</p>
</div>
<p>Note that <tt class="docutils literal"><span class="pre">grad_mu</span></tt> is nominally sized: <tt class="docutils literal"><span class="pre">grad_mu[dim][num_to_sample][num_to_sample]</span></tt>.
However, for <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;=</span> <span class="pre">i,j</span> <span class="pre">&lt;</span> <span class="pre">num_to_sample</span></tt>, <tt class="docutils literal"><span class="pre">i</span> <span class="pre">!=</span> <span class="pre">j</span></tt>, <tt class="docutils literal"><span class="pre">grad_mu[d][i][j]</span> <span class="pre">=</span> <span class="pre">0</span></tt>.
(See references or implementation for further details.)
Thus, <tt class="docutils literal"><span class="pre">grad_mu</span></tt> is stored in a reduced form which only tracks the nonzero entries.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments are copied in Python: interfaces/gaussian_process_interface.py</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState)</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">grad_mu[dim][state.num_derivatives]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">gradient of the mean of the GP.  <tt class="docutils literal"><span class="pre">grad_mu[d][i]</span></tt> is
actually the gradient of <tt class="docutils literal"><span class="pre">\mu_i</span></tt> with respect to <tt class="docutils literal"><span class="pre">x_{d,i}</span></tt>, the d-th dimension of
the i-th entry of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Gradient of the mean of a GP.  Note that the output storage skips known zeros (see declaration docs for details).
See Scott Clark&#8217;s PhD thesis for more spelled out mathematical details, but this is a reasonably straightforward
differentiation of:
<tt class="docutils literal"><span class="pre">mus</span> <span class="pre">=</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">y</span></tt>
wrt <tt class="docutils literal"><span class="pre">Xs</span></tt> (so only Ks contributes derivative terms)</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a729b69525d8409dcfaeb6e9d83a3503f"></span><div class="line-block">
<div class="line">void <strong>ComputeVarianceOfPoints</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state, double *restrict var_star)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the variance (matrix) of this GP at each point of <tt class="docutils literal"><span class="pre">Xs</span></tt> (<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>).</p>
<p>The variance matrix is symmetric (in fact, SPD) and is stored in the LOWER TRIANGLE.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last"><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> should not contain duplicate points.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments are copied in Python: interfaces/gaussian_process_interface.py</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState)</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">var_star[num_to_sample][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">variance of GP evaluated at <tt class="docutils literal"><span class="pre">points_to_sample</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Mathematically, we are computing Vars (Var_star), the GP variance.  Vars is defined at the top of this file (Equation 3)
and in Rasmussen &amp; Williams, Equation 2.19:
<tt class="docutils literal"><span class="pre">L</span> <span class="pre">*</span> <span class="pre">L^T</span> <span class="pre">=</span> <span class="pre">K</span></tt>
<tt class="docutils literal"><span class="pre">V</span> <span class="pre">=</span> <span class="pre">L^-1</span> <span class="pre">*</span> <span class="pre">Ks</span></tt>
<tt class="docutils literal"><span class="pre">Vars</span> <span class="pre">=</span> <span class="pre">Kss</span> <span class="pre">-</span> <span class="pre">(V^T</span> <span class="pre">*</span> <span class="pre">V)</span></tt>
This quantity is:
<tt class="docutils literal"><span class="pre">Kss</span></tt>: the covariance between test points based on the prior distribution
minus
<tt class="docutils literal"><span class="pre">V^T</span> <span class="pre">*</span> <span class="pre">V</span></tt>: the information observations give us about the objective function</p>
<p>Notice that Vars is clearly symmetric.  <tt class="docutils literal"><span class="pre">Kss</span></tt> is SPD. And
<tt class="docutils literal"><span class="pre">V^T</span> <span class="pre">*</span> <span class="pre">V</span> <span class="pre">=</span> <span class="pre">(V^T</span> <span class="pre">*</span> <span class="pre">V)^T</span></tt> is symmetric (and is in fact SPD).</p>
<p><tt class="docutils literal"><span class="pre">V^T</span> <span class="pre">*</span> <span class="pre">V</span> <span class="pre">=</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">K_s</span></tt> is SPD because:
<tt class="docutils literal"><span class="pre">X^T</span> <span class="pre">*</span> <span class="pre">A</span> <span class="pre">*</span> <span class="pre">X</span></tt> is SPD when A is SPD AND <tt class="docutils literal"><span class="pre">X</span></tt> has full rank (<tt class="docutils literal"><span class="pre">X</span></tt> need not be square)
Ks has full rank as long as <tt class="docutils literal"><span class="pre">K</span></tt> &amp; <tt class="docutils literal"><span class="pre">Kss</span></tt> are SPD; <tt class="docutils literal"><span class="pre">K^-1</span></tt> is SPD because <tt class="docutils literal"><span class="pre">K</span></tt> is SPD.</p>
<p>It turns out that <tt class="docutils literal"><span class="pre">Vars</span></tt> is SPD.
In Equation 1 (Rasmussen &amp; Williams 2.18), it is clear that the combined covariance matrix
is SPD (as long as no duplicate points and the covariance function is valid).  A matrix of the form
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">A</span>&nbsp;&nbsp; <span class="pre">B</span> <span class="pre">]</span></tt>
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">B^T</span> <span class="pre">C</span> <span class="pre">]</span></tt>
is SPD if and only if <tt class="docutils literal"><span class="pre">A</span></tt> is SPD AND <tt class="docutils literal"><span class="pre">(C</span> <span class="pre">-</span> <span class="pre">B^T</span> <span class="pre">*</span> <span class="pre">A^-1</span> <span class="pre">*</span> <span class="pre">B)</span></tt> is SPD.  Here, <tt class="docutils literal"><span class="pre">A</span> <span class="pre">=</span> <span class="pre">K,</span> <span class="pre">B</span> <span class="pre">=</span> <span class="pre">Ks,</span> <span class="pre">C</span> <span class="pre">=</span> <span class="pre">Kss</span></tt>.
This can be shown readily:
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">A</span>&nbsp;&nbsp; <span class="pre">B</span> <span class="pre">]</span> <span class="pre">=</span> <span class="pre">[</span>&nbsp; <span class="pre">I</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">0</span> <span class="pre">]</span> <span class="pre">*</span> <span class="pre">[</span>&nbsp; <span class="pre">A</span>&nbsp;&nbsp;&nbsp; <span class="pre">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">]</span> <span class="pre">*</span> <span class="pre">[</span> <span class="pre">I</span>&nbsp;&nbsp; <span class="pre">A^-1</span> <span class="pre">*</span> <span class="pre">B</span> <span class="pre">]</span></tt>
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">B^T</span> <span class="pre">C</span> <span class="pre">]</span>&nbsp;&nbsp; <span class="pre">[</span> <span class="pre">(A^-1</span> <span class="pre">*</span> <span class="pre">B)^T</span>&nbsp; <span class="pre">I</span> <span class="pre">]</span> <span class="pre">*</span> <span class="pre">[</span>&nbsp; <span class="pre">0</span> <span class="pre">(C</span> <span class="pre">-</span> <span class="pre">B^T</span> <span class="pre">*</span> <span class="pre">A^-1</span> <span class="pre">*</span> <span class="pre">B)]</span>&nbsp;&nbsp; <span class="pre">[</span> <span class="pre">0</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">I</span>&nbsp;&nbsp;&nbsp; <span class="pre">]</span></tt>
This factorization is valid because <tt class="docutils literal"><span class="pre">A</span></tt> is SPD (and thus invertible).  Then by the <tt class="docutils literal"><span class="pre">X^T</span> <span class="pre">*</span> <span class="pre">A</span> <span class="pre">*</span> <span class="pre">X</span></tt> rule for SPD-ness,
we know the block-diagonal matrix in the center is SPD.  Hence the SPD-ness of <tt class="docutils literal"><span class="pre">V^T</span> <span class="pre">*</span> <span class="pre">V</span></tt> follows readily.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a671152f1c7f38cfd3ba22cffc6a5d569"></span><div class="line-block">
<div class="line">void <strong>ComputeGradVarianceOfPoints</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state, double *restrict grad_var)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Similar to ComputeGradCholeskyVarianceOfPoints() except this does not include the gradient terms from
the cholesky factorization.  Description will not be duplicated here.</p>
<p><p>This is just a thin wrapper that calls ComputeGradVarianceOfPointsPerPoint() in a loop <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> times.</p>
<p>See ComputeGradVarianceOfPointsPerPoint()&#8217;s function comments and implementation for more mathematical details
on the derivation, algorithm, optimizations, etc.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a79bceb2937c8e138523ed63732c77acd"></span><div class="line-block">
<div class="line">void <strong>ComputeGradCholeskyVarianceOfPoints</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state, double const *restrict chol_var, double *restrict grad_chol)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the gradient of the cholesky factorization of the variance of this GP with respect to <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.
This function accounts for the effect on the gradient resulting from
cholesky-factoring the variance matrix.  See Smith 1995 for algorithm details.</p>
<p><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> is not allowed to contain duplicate points. Violating this results in a singular variance matrix.</p>
<p>Note that <tt class="docutils literal"><span class="pre">grad_chol</span></tt> is nominally sized:
<tt class="docutils literal"><span class="pre">grad_chol[dim][num_to_sample][num_to_sample][num_to_sample]</span></tt>.
Let this be indexed <tt class="docutils literal"><span class="pre">grad_chol[d][i][j][k]</span></tt>, which is read the derivative of <tt class="docutils literal"><span class="pre">var[i][j]</span></tt>
with respect to <tt class="docutils literal"><span class="pre">x_{d,k}</span></tt> (x = <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>)</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments are copied in Python: interfaces/gaussian_process_interface.py</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState)</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">grad_chol[dim][num_to_sample][num_to_sample][state-&gt;num_derivatives]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">gradient of the cholesky-factored
variance of the GP.  <tt class="docutils literal"><span class="pre">grad_chol[d][i][j][k]</span></tt> is actually the gradients of <tt class="docutils literal"><span class="pre">var_{i,j}</span></tt> with
respect to <tt class="docutils literal"><span class="pre">x_{d,k}</span></tt>, the d-th dimension of the k-th entry of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>This is just a thin wrapper that calls ComputeGradCholeskyVarianceOfPointsPerPoint() in a loop <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> times.</p>
<p>See ComputeGradCholeskyVarianceOfPointsPerPoint()&#8217;s function comments and implementation for more mathematical
details on the algorithm.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a7657ac7fb2ca010363719b47b7fcaf83"></span><div class="line-block">
<div class="line">void <strong>SetExplicitSeed</strong>(EngineType::result_type seed)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Re-seed the random number generator with the specified seed.
See gpp_random, struct NormalRNG for details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">seed:</th><td class="field-body">new seed to set</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1aeedfb07d87e00a116bab3ccf3308bd05"></span><div class="line-block">
<div class="line">void <strong>SetRandommizedSeed</strong>(EngineType::result_type seed)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Re-seed the random number generator using a combination of the specified seed,
current time, and potentially other factors.
See gpp_random, struct NormalRNG for details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">seed:</th><td class="field-body">base value for new seed</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a5d473d09a07c5902d5f2fe78d86c1f03"></span><div class="line-block">
<div class="line">void <strong>ResetToMostRecentSeed</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Reseeds the generator with its last used seed value.
Useful for testing&#8211;e.g., can conduct multiple runs with the same initial conditions</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1afed0df459419720395d979ffaf1f19f6"></span><div class="line-block">
<div class="line"><a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * <strong>Clone</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Clones &#8220;this&#8221; GaussianProcess.</p>
<dl class="docutils">
<dt><strong>Returns</strong>:</dt>
<dd>Pointer to a constructed object that is a copy of &#8220;this&#8221;</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1afed8cb9e4d8e2c99c9c7b5995b3ff4b2"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Static Attributes</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a5e96979c452eb8ed29727e7c69f99c5e"></span>constexpr EngineType::result_type <strong>kDefaultSeed</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Protected Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a347b14458621dd3f16b7fa159cec8905"></span><div class="line-block">
<div class="line"> <strong>GaussianProcess</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; source)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Private Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1ae30fe08eb6c927b03890849d14484923"></span><div class="line-block">
<div class="line">void <strong>BuildCovarianceMatrixWithNoiseVariance</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1acbe6a6e35387c4913b2969b520f63742"></span><div class="line-block">
<div class="line">void <strong>BuildMixCovarianceMatrix</strong>(double const *restrict points_to_sample, int num_to_sample, double *restrict cov_mat)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1ac3e566afc2922e1030bfbf8f9c382737"></span><div class="line-block">
<div class="line">void <strong>ComputeGradVarianceOfPointsPerPoint</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state, int diff_index, double *restrict grad_var)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Similar to ComputeGradCholeskyVarianceOfPointsPerPoint() except this does not include the gradient terms from
the cholesky factorization.  Description will not be duplicated here.</p>
<p><p><strong>CORE IDEA</strong></p>
<p>Similar to ComputeGradCholeskyVarianceOfPoints() below, except this function does not account for the cholesky decomposition.  That is,
it produces derivatives wrt <tt class="docutils literal"><span class="pre">Xs_{d,p}</span></tt> (<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>) of:
<tt class="docutils literal"><span class="pre">Vars</span> <span class="pre">=</span> <span class="pre">Kss</span> <span class="pre">-</span> <span class="pre">(V^T</span> <span class="pre">*</span> <span class="pre">V)</span> <span class="pre">=</span> <span class="pre">Kss</span> <span class="pre">-</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">Ks</span></tt> (see ComputeVarianceOfPoints)</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">normally <tt class="docutils literal"><span class="pre">Xs_p</span></tt> would be the <tt class="docutils literal"><span class="pre">p</span></tt>-th point of Xs (all dimensions); here <tt class="docutils literal"><span class="pre">Xs_{d,p}</span></tt> more explicitly
refers to the <tt class="docutils literal"><span class="pre">d</span></tt>-th spatial dimension of the <tt class="docutils literal"><span class="pre">p</span></tt>-th point.</p>
</div>
<p>This function only returns the derivative wrt a single choice of <tt class="docutils literal"><span class="pre">p</span></tt>, as specified by <tt class="docutils literal"><span class="pre">diff_index</span></tt>.</p>
<p>Expanded index notation:
<tt class="docutils literal"><span class="pre">Vars_{i,j}</span> <span class="pre">=</span> <span class="pre">Kss_{i,j}</span> <span class="pre">-</span> <span class="pre">Ks^T_{i,l}</span> <span class="pre">*</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">Ks_{k,j}</span></tt></p>
<p>Recall <tt class="docutils literal"><span class="pre">Ks_{k,i}</span> <span class="pre">=</span> <span class="pre">cov(X_k,</span> <span class="pre">Xs_i)</span> <span class="pre">=</span> <span class="pre">cov(Xs_i,</span> <span class="pre">Xs_k)</span></tt> where <tt class="docutils literal"><span class="pre">Xs</span></tt> is <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and <tt class="docutils literal"><span class="pre">X</span></tt> is <tt class="docutils literal"><span class="pre">points_sampled</span></tt>.
(Note this is not equivalent to saying <tt class="docutils literal"><span class="pre">Ks</span> <span class="pre">=</span> <span class="pre">Ks^T</span></tt>, although this would be true if <tt class="docutils literal"><span class="pre">|Xs|</span> <span class="pre">==</span> <span class="pre">|X|</span></tt>.)
As a result of this symmetry, <tt class="docutils literal"><span class="pre">\pderiv{Ks_{k,i}}{Xs_{d,i}}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{i,k}}{Xs_{d,i}}</span></tt> (that&#8217;s <tt class="docutils literal"><span class="pre">d(cov(Xs_i,</span> <span class="pre">X_k))/d(Xs_i)</span></tt>)</p>
<p>We are being more strict with index labels than is standard to clearly specify tensor dimensions.  To be clear:
1. <tt class="docutils literal"><span class="pre">i,j</span></tt> range over <tt class="docutils literal"><span class="pre">num_to_sample</span></tt>
2. <tt class="docutils literal"><span class="pre">l,k</span></tt> are the only non-free indices; they range over <tt class="docutils literal"><span class="pre">num_sampled</span></tt>
3. <tt class="docutils literal"><span class="pre">d,p</span></tt> describe the SPECIFIC point being differentiated against in <tt class="docutils literal"><span class="pre">Xs</span></tt> (<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>): <tt class="docutils literal"><span class="pre">d</span></tt> over dimension, <tt class="docutils literal"><span class="pre">p</span></tt>* over <tt class="docutils literal"><span class="pre">num_to_sample</span></tt>
*NOTE: <tt class="docutils literal"><span class="pre">p</span></tt> is <em>fixed</em>! Unlike all other indices, <tt class="docutils literal"><span class="pre">p</span></tt> refers to a <em>SPECIFIC</em> point in the range <tt class="docutils literal"><span class="pre">[0,</span> <span class="pre">...,</span> <span class="pre">num_to_sample-1]</span></tt>.</p>
<blockquote>
<div>Thus, <tt class="docutils literal"><span class="pre">\pderiv{Ks_{k,i}}{Xs_{d,i}}</span></tt> is a 3-tensor (<tt class="docutils literal"><span class="pre">A_{d,k,i}</span></tt>) (repeated <tt class="docutils literal"><span class="pre">i</span></tt> is not summation since they denote
components of a derivative) while <tt class="docutils literal"><span class="pre">\pderiv{Ks_{i,l}}{Xs_{d,p}}</span></tt> is a 2-tensor (<tt class="docutils literal"><span class="pre">A_{d,l}</span></tt>) b/c only
<tt class="docutils literal"><span class="pre">\pderiv{Ks_{i=p,l}}{Xs_{d,p}}</span></tt> is nonzero, and <tt class="docutils literal"><span class="pre">{d,l}</span></tt> are the only remaining free indices.</div></blockquote>
<p>Then differentiating against <tt class="docutils literal"><span class="pre">Xs_{d,p}</span></tt> (recall that this is a specific point b/c p is fixed):
<tt class="docutils literal"><span class="pre">\pderiv{Vars_{i,j}}{Xs_{d,p}}</span> <span class="pre">=</span> <span class="pre">\pderiv{K_ss{i,j}}{Xs_{d,p}}</span> <span class="pre">-</span></tt>
<tt class="docutils literal"><span class="pre">(\pderiv{Ks_{i,l}}{Xs_{d,p}}</span> <span class="pre">*</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">Ks_{k,j}</span>&nbsp;&nbsp; <span class="pre">+</span>&nbsp; <span class="pre">K_s{i,l}</span> <span class="pre">*</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">\pderiv{Ks_{k,j}}{Xs_{d,p}})</span></tt>
Many of these terms are analytically known to be 0: <tt class="docutils literal"><span class="pre">\pderiv{Ks_{i,l}}{Xs_{d,p}}</span> <span class="pre">=</span> <span class="pre">0</span></tt> when <tt class="docutils literal"><span class="pre">p</span> <span class="pre">!=</span> <span class="pre">i</span></tt> (see NOTE above).
A similar statement holds for the other gradient term.</p>
<p>Observe that the second term in the parens, <tt class="docutils literal"><span class="pre">Ks_{i,l}</span> <span class="pre">*</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">\pderiv{Ks_{k,j}}{Xs_{d,p}}</span></tt>, can be reordered
to &#8220;look&#8221; like the first term.  We use three symmetries: <tt class="docutils literal"><span class="pre">K^-1{l,k}</span> <span class="pre">=</span> <span class="pre">K^-1{k,l}</span></tt>, <tt class="docutils literal"><span class="pre">Ks_{i,l}</span> <span class="pre">=</span> <span class="pre">Ks_{l,i}</span></tt>, and
<tt class="docutils literal"><span class="pre">\pderiv{Ks_{k,j}}{Xs_{d,p}}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{j,k}}{Xs_{d,p}}</span></tt>
Then we can write:
<tt class="docutils literal"><span class="pre">K_s{i,l}</span> <span class="pre">*</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">\pderiv{Ks_{k,j}}{Xs_{d,p}}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{j,k}}{Xs_{d,p}}</span> <span class="pre">*</span> <span class="pre">K^-1_{k,l}</span> <span class="pre">*</span> <span class="pre">K_s{l,i}</span></tt>
Now left and right terms have the same index ordering (i,j match; k,l are not free and thus immaterial)</p>
<p>The final result, accounting for analytic zeros is given here for convenience:
<tt class="docutils literal"><span class="pre">DVars_{d,i,j}</span> <span class="pre">\equiv</span> <span class="pre">\pderiv{Vars_{i,j}}{Xs_{d,p}}</span> <span class="pre">=</span></tt>
``  { pderiv{K_ss{i,j}}{Xs_{d,p}} - 2*pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j}   :  WHEN p == i == j``
``  { pderiv{K_ss{i,j}}{Xs_{d,p}} -   pderiv{Ks_{i,l}}{Xs_{d,p}} * K^-1_{l,k} * Ks_{k,j}   :  WHEN p == i != j``
``  { pderiv{K_ss{i,j}}{Xs_{d,p}} -   pderiv{Ks_{j,k}}{Xs_{d,p}} * K^-1_{k,l} * K_s{l,i}   :  WHEN p == j != i``
``  {                                    0                                                   :  otherwise``
The first item has a factor of 2 b/c it gets a contribution from both parts of the sum since <tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">i</span></tt> and <tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">j</span></tt>.
The ordering <tt class="docutils literal"><span class="pre">DVars_{d,i,j}</span></tt> is significant: this is the ordering (d changes the fastest) in storage.</p>
<p><strong>OPTIMIZATIONS</strong></p>
<p>Implementing this formula naively results in a large amount of redundant computation, so we now describe the optimizations
present in our implementation.</p>
<p>The first thing to notice is that the result, <tt class="docutils literal"><span class="pre">\pderiv{Vars_{i,j}}{Xs_{d,p}}</span></tt>, has a lot of 0s.  In particular, only the
<tt class="docutils literal"><span class="pre">p</span></tt>-th block row and <tt class="docutils literal"><span class="pre">p</span></tt>-th block column have nonzero entries (blocks are size <tt class="docutils literal"><span class="pre">dim</span></tt>, indexed <tt class="docutils literal"><span class="pre">d</span></tt>).  Currently,
we will not be taking advantage of this sparsity because the consumer of DVars, ComputeGradCholeskyVarianceOfPoints(),
is not implemented with sparsity in mind.</p>
<p>Similarly, the next thing to notice is that if we ignore the case <tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">i</span> <span class="pre">==</span> <span class="pre">j</span></tt>, then we see that the expressions for
<tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">i</span></tt> and <tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">j</span></tt> are actually identical (e.g., take the <tt class="docutils literal"><span class="pre">p</span> <span class="pre">==</span> <span class="pre">j</span></tt> case and exchange <tt class="docutils literal"><span class="pre">j</span> <span class="pre">=</span> <span class="pre">i</span></tt> and <tt class="docutils literal"><span class="pre">k</span> <span class="pre">=</span> <span class="pre">l</span></tt>).
So think of <tt class="docutils literal"><span class="pre">DVars</span></tt> as a block matrix; each block has dimension entries, and the blocks are indexed over
<tt class="docutils literal"><span class="pre">i</span></tt> (rows), <tt class="docutils literal"><span class="pre">j</span></tt> (cols).  Then we see that the code is block-symmetric: <tt class="docutils literal"><span class="pre">DVars_{d,i,j}</span> <span class="pre">=</span> <span class="pre">Dvars_{d,j,i}</span></tt>.
So we can compute it by filling in the <tt class="docutils literal"><span class="pre">p</span></tt>-th block column and then copy that data into the <tt class="docutils literal"><span class="pre">p</span></tt>-th block row.</p>
<p>Additionally, the derivative terms represent matrix-matrix products:
<tt class="docutils literal"><span class="pre">C_{l,j}</span> <span class="pre">=</span> <span class="pre">K^-1_{l,k}</span> <span class="pre">*</span> <span class="pre">Ks_{k,j}</span></tt> (and <tt class="docutils literal"><span class="pre">K^-1_{k,l}</span> <span class="pre">*</span> <span class="pre">Ks_{l,i}</span></tt>, which is just a change of index labels) is
a matrix product.  We compute this using back-substitutions to avoid explicitly forming <tt class="docutils literal"><span class="pre">K^-1</span></tt>.  <tt class="docutils literal"><span class="pre">C_{l,j}</span></tt>
is <tt class="docutils literal"><span class="pre">num_sampled</span></tt> X <tt class="docutils literal"><span class="pre">num_to_sample</span></tt>.
Then <tt class="docutils literal"><span class="pre">D_{d,i=p,j}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{i=p,l}}{Xs_{d,p}}</span> <span class="pre">*</span> <span class="pre">C_{l,j}</span></tt> is another matrix product (result size <tt class="docutils literal"><span class="pre">dim</span> <span class="pre">*</span> <span class="pre">num_to_sample</span></tt>)
(<tt class="docutils literal"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">p</span></tt> indicates that index <tt class="docutils literal"><span class="pre">i</span></tt> collapses out since this deriv term is zero if <tt class="docutils literal"><span class="pre">p</span> <span class="pre">!=</span> <span class="pre">i</span></tt>).
Note that we store <tt class="docutils literal"><span class="pre">\pderiv{Ks_{i=p,l}}{Xs_{d,p}}</span> <span class="pre">=</span> <span class="pre">\pderiv{Ks_{l,i=p}}{Xs_{d,p}}</span></tt> as <tt class="docutils literal"><span class="pre">A_{d,l,i}</span></tt>
and grab the <tt class="docutils literal"><span class="pre">i</span> <span class="pre">=</span> <span class="pre">p</span></tt>-th block.</p>
<p>Again, only the <tt class="docutils literal"><span class="pre">p</span></tt>-th point of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> is differentiated against; <tt class="docutils literal"><span class="pre">p</span></tt> specfied in <tt class="docutils literal"><span class="pre">diff_index</span></tt>.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a1ca23082986be3c7775d95578628584e"></span><div class="line-block">
<div class="line">void <strong>ComputeGradCholeskyVarianceOfPointsPerPoint</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>StateType</em></a>  * points_to_sample_state, int diff_index, double const *restrict chol_var, double *restrict grad_chol)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the gradient of the cholesky factorization of the variance of this GP with respect to the
<tt class="docutils literal"><span class="pre">diff_index</span></tt>-th point in <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.</p>
<p>This internal method is meant to be used by ComputeGradCholeskyVarianceOfPoints() to construct the gradient wrt all
points of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>. See that function for more details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState (configure via PointsToSampleState::SetupState)</td>
</tr>
<tr class="field-even field"><th class="field-name">diff_index:</th><td class="field-body">index of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> in {0, .. <tt class="docutils literal"><span class="pre">num_to_sample</span></tt>-1} to be differentiated against</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">ptr to a FULLY CONFIGURED PointsToSampleState; only temporary state may be mutated</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">grad_chol[dim][num_to_sample][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">gradient of the cholesky-factored
variance of the GP.  <tt class="docutils literal"><span class="pre">grad_chol[d][i][j]</span></tt> is actually the gradients of <tt class="docutils literal"><span class="pre">var_{i,j}</span></tt> with
respect to <tt class="docutils literal"><span class="pre">x_{d,k}</span></tt>, the d-th dimension of the k-th entry of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>, where
k = <tt class="docutils literal"><span class="pre">diff_index</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Differentiates the cholesky factorization of the GP variance.
<tt class="docutils literal"><span class="pre">Vars</span> <span class="pre">=</span> <span class="pre">Kss</span> <span class="pre">-</span> <span class="pre">(V^T</span> <span class="pre">*</span> <span class="pre">V)</span></tt>  (see ComputeVarianceOfPoints)
<tt class="docutils literal"><span class="pre">C</span> <span class="pre">*</span> <span class="pre">C^T</span> <span class="pre">=</span> <span class="pre">Vars</span></tt>
This function differentiates <tt class="docutils literal"><span class="pre">C</span></tt> wrt the <tt class="docutils literal"><span class="pre">p</span></tt>-th point of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>; <tt class="docutils literal"><span class="pre">p</span></tt> specfied in <tt class="docutils literal"><span class="pre">diff_index</span></tt></p>
<p>Just as users of a lower triangular matrix <tt class="docutils literal"><span class="pre">L[i][j]</span></tt> should not access the upper triangle (<tt class="docutils literal"><span class="pre">j</span> <span class="pre">&gt;</span> <span class="pre">i</span></tt>), users of
the result of this function, <tt class="docutils literal"><span class="pre">grad_chol[d][i][j]</span></tt>, should not access the upper <em>block</em> triangle with <tt class="docutils literal"><span class="pre">j</span> <span class="pre">&gt;</span> <span class="pre">i</span></tt>.</p>
<p>See Smith 1995 for full details of computing gradients of the cholesky factorization</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a525f3a2cae0a170b7925c7d3622749a7"></span><div class="line-block">
<div class="line">void <strong>RecomputeDerivedVariables</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Recomputes (including resizing as needed) the derived quantities in this class.
This function should be called any time state variables are changed.</p>
 </p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1ac1881908657eba98f40da476be0c6698"></span>int <strong>dim_</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of <tt class="docutils literal"><span class="pre">points_sampled</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1ab7b228f4c03cf7700f3b8d3be9794873"></span>int <strong>num_sampled_</strong></p>
<blockquote>
<div><p>number of points in <tt class="docutils literal"><span class="pre">points_sampled</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a430bc4ffcb629edb4bdc5766cbfffe54"></span>std::unique_ptr&lt;  <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  &gt; <strong>covariance_ptr_</strong></p>
<blockquote>
<div><p>covariance class (for computing covariance and its gradients) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a0b4b241a7161698b50bcf462bc9f4a41"></span>std::vector&lt; double &gt; <strong>points_sampled_</strong></p>
<blockquote>
<div><p>coordinates of already-sampled points, <tt class="docutils literal"><span class="pre">X</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1abc1a69f726fd772cfc0daee807337943"></span>std::vector&lt; double &gt; <strong>points_sampled_value_</strong></p>
<blockquote>
<div><p>function values at points_sampled, <tt class="docutils literal"><span class="pre">y</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a712e7db3eb82c29d2359edb40426a2de"></span>std::vector&lt; double &gt; <strong>noise_variance_</strong></p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt>, the noise variance </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a7703f449267a1256f712af7722fadc57"></span>std::vector&lt; double &gt; <strong>K_chol_</strong></p>
<blockquote>
<div><p>cholesky factorization of <tt class="docutils literal"><span class="pre">K</span></tt> (i.e., <tt class="docutils literal"><span class="pre">K(X,X)</span></tt> covariance matrix (prior), includes noise variance) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1af26b773eee0adfb2422d8de8cc76d400"></span>std::vector&lt; double &gt; <strong>K_inv_y_</strong></p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">y</span></tt>; computed WITHOUT forming <tt class="docutils literal"><span class="pre">K^-1</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gaussian_process_1a196f018b2928a9e8fc2e9785831de110"></span><a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalGeneratorType</em></a> <strong>normal_rng_</strong></p>
<blockquote>
<div><p>Normal PRNG for use with sampling points from GP. </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0structoptimal__learning_1_1_points_to_sample_state"><em>class</em> <strong>PointsToSampleState</strong></p>
<blockquote>
<div><p></p>
<p><p>This object holds the state needed for a GaussianProcess object characterize the distribution of function values arising from
sampling the GP at a list of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.  This object is required by the GaussianProcess to access functionality for
computing the mean, variance, and spatial gradients thereof.</p>
<p>The &#8220;independent variables&#8221; for this object are <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>. These points are both the &#8220;p&#8221; and the &#8220;q&#8221; in q,p-EI;
i.e., they are the parameters of both ongoing experiments and new predictions. Recall that in q,p-EI, the q points are
called <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and the p points are called <tt class="docutils literal"><span class="pre">points_being_sampled.</span></tt> Here, we need to make predictions about
both point sets with the GP, so we simply call the union of point sets <tt class="docutils literal"><span class="pre">points_to_sample.</span></tt></p>
<p>In GP computations, there is really no distinction between the &#8220;q&#8221; and &#8220;p&#8221; points from EI, <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and
<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>, respectively. However, in EI optimization, we only need gradients of GP quantities wrt
<tt class="docutils literal"><span class="pre">points_to_sample</span></tt>, so users should build PointsToSampleState() with <tt class="docutils literal"><span class="pre">num_derivatives</span> <span class="pre">=</span> <span class="pre">num_to_sample</span></tt>.</p>
<p>Once constructed, this object provides the SetupState() function to update it for computations at different sets of
potential points to sample.</p>
<p>See general comments on State structs in <tt class="docutils literal"><span class="pre">gpp_common.hpp</span></tt>&#8216;s header docs.</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a5b9436bc0a0cbbeba53640cec5cfa548"></span><div class="line-block">
<div class="line"> <strong>PointsToSampleState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, double const *restrict points_to_sample_in, int num_to_sample_in, int num_derivatives_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a PointsToSampleState object with new <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.
Ensures all state variables &amp; temporaries are properly sized.
Properly sets all state variables so that GaussianProcess&#8217;s mean, variance (and gradients thereof) functions can be called.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object&#8217;s state is INVALIDATED if the gaussian_process used in construction is mutated!
SetupState() should be called again in such a situation.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Using this object to compute gradients when <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> := 0 results in UNDEFINED BEHAVIOR.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points at which to compute GP-derived quantities (mean, variance, etc.)</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of points being sampled concurrently</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_derivatives:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">configure this object to compute <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> derivative terms wrt
points_to_sample[:][0:num_derivatives]; 0 means no gradient computation will be performed.</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a770b1c98740e579b97c93f1e371cee94"></span><div class="line-block">
<div class="line"> <strong>PointsToSampleState</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>PointsToSampleState</em></a>  &amp;&amp; OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a076e4a73c3375138f945ed0d4f5f1061"></span><div class="line-block">
<div class="line">void <strong>SetupState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, double const *restrict points_to_sample_in, int num_to_sample_in, int num_derivatives_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Configures this object with new <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.
Ensures all state variables &amp; temporaries are properly sized.
Properly sets all state variables so that GaussianProcess&#8217;s mean, variance (and gradients thereof) functions can be called.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object&#8217;s state is INVALIDATED if the gaussian_process used in SetupState is mutated!
SetupState() should be called again in such a situation.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Using this object to compute gradients when <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> := 0 results in UNDEFINED BEHAVIOR.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points at which to compute GP-derived quantities (mean, variance, etc.)</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of points being sampled concurrently</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_derivatives:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">configure this object to compute <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> derivative terms wrt
points_to_sample[:][0:num_derivatives]; 0 means no gradient computation will be performed.</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a7cd1191e1c9ca27124d4268269e44ab8"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>PointsToSampleState</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a9c8500d246a4ac913178cf5dad61d718"></span>const int <strong>dim</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of <tt class="docutils literal"><span class="pre">points_sampled</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a23af34de18e22733a6e22d9cdbac631b"></span>int <strong>num_sampled</strong></p>
<blockquote>
<div><p>number of points alerady sampled </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a6f87cfa7d12f27bdc775131e42bdce55"></span>int <strong>num_to_sample</strong></p>
<blockquote>
<div><p>number of points currently being sampled </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a8ae85d9d04d1f0292ec5a409302840e5"></span>int <strong>num_derivatives</strong></p>
<blockquote>
<div><p></p>
<p>this object can compute <tt class="docutils literal"><span class="pre">num_derivatives</span></tt> derivative terms wrt points_to_sample[:][0:num_derivatives]; 0 means no gradient computation will be performed </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a5a0c83654a0eb3d0f5b72637a88b3909"></span>std::vector&lt; double &gt; <strong>points_to_sample</strong></p>
<blockquote>
<div><p>points to make predictions about, <tt class="docutils literal"><span class="pre">Xs</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a9806d4c1e0127e8a0a4bb4f52fe8086d"></span>std::vector&lt; double &gt; <strong>K_star</strong></p>
<blockquote>
<div><p>the &#8220;mixed&#8221; covariance matrix: <tt class="docutils literal"><span class="pre">Ks,</span> <span class="pre">Ks_{ij},</span> <span class="pre">K(X,Xs)</span> <span class="pre">=</span> <span class="pre">covariance(X_i,</span> <span class="pre">Xs_j)</span></tt>, covariance matrix between training and test inputs (<tt class="docutils literal"><span class="pre">num_sampled</span> <span class="pre">x</span> <span class="pre">num_to_sample</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a4df6fcc744396efa177ea6d005e61915"></span>std::vector&lt; double &gt; <strong>grad_K_star</strong></p>
<blockquote>
<div><p>the gradient of mixed covariance matrix, <tt class="docutils literal"><span class="pre">Ks</span></tt>, wrt <tt class="docutils literal"><span class="pre">Xs</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1ac4c18c736a522efa0b496a84078fbc79"></span>std::vector&lt; double &gt; <strong>V</strong></p>
<blockquote>
<div><p>the variance matrix (output from the GP) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a7bb4768684ad59c0d1a3572b0f1a92f7"></span>std::vector&lt; double &gt; <strong>K_inv_times_K_star</strong></p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">K^{-1}</span> <span class="pre">Ks</span></tt> (computed without taking an inverse) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_points_to_sample_state_1a034f0b4c1b5ba46a58df2331fafc61d2"></span>std::vector&lt; double &gt; <strong>grad_cov</strong></p>
<blockquote>
<div><p>the gradient of covariance(x_1, x_2) wrt x_1 </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>class</em> <strong>ExpectedImprovementEvaluator</strong></p>
<blockquote>
<div><p></p>
<p><p>A class to encapsulate the computation of expected improvement and its spatial gradient. This class handles the
general EI computation case using monte carlo integration; it can support q,p-EI optimization. It is designed to work
with any GaussianProcess.  Additionally, this class has no state and within the context of EI optimization, it is
meant to be accessed by const reference only.</p>
<p>The random numbers needed for EI computation will be passed as parameters instead of contained as members to make
multithreading more straightforward.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a6c8cef236d5dc1e333259682d64bb009"></span>typedef <a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>ExpectedImprovementState</em></a> <strong>StateType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1afd269f7e4683ebf55214812a30297a32"></span><div class="line-block">
<div class="line"> <strong>ExpectedImprovementEvaluator</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process_in, int num_mc_iterations, double best_so_far)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a ExpectedImprovementEvaluator object.  All inputs are required; no default constructor nor copy/assignment are allowed.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_mc_iterations:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of monte carlo iterations</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">best (minimum) objective function value (in <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt>)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1af3e1fe673239fdbf08658647e6fc8b96"></span><div class="line-block">
<div class="line">int <strong>dim</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a5d841afce79c2bf63cf55640e2338c8d"></span><div class="line-block">
<div class="line">const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * <strong>gaussian_process</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a9d90bf2033121dc5deda0294a042a089"></span><div class="line-block">
<div class="line">double <strong>ComputeObjectiveFunction</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>StateType</em></a>  * ei_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Wrapper for ComputeExpectedImprovement(); see that function for details.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1aac254f730e701661b274f991fa6529cc"></span><div class="line-block">
<div class="line">void <strong>ComputeGradObjectiveFunction</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>StateType</em></a>  * ei_state, double *restrict grad_EI)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Wrapper for ComputeGradExpectedImprovement(); see that function for details.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a1036d12576b0779c475ac3a993cdfa83"></span><div class="line-block">
<div class="line">double <strong>ComputeExpectedImprovement</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>StateType</em></a>  * ei_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the expected improvement <tt class="docutils literal"><span class="pre">EI(Xs)</span> <span class="pre">=</span> <span class="pre">E_n[[f^*_n(X)</span> <span class="pre">-</span> <span class="pre">min(f(Xs_1),...,f(Xs_m))]^+]</span></tt>, where <tt class="docutils literal"><span class="pre">Xs</span></tt>
are potential points to sample (union of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>) and <tt class="docutils literal"><span class="pre">X</span></tt> are
already sampled points.  The <tt class="docutils literal"><span class="pre">^+</span></tt> indicates that the expression in the expectation evaluates to 0 if it
is negative.  <tt class="docutils literal"><span class="pre">f^*(X)</span></tt> is the MINIMUM over all known function evaluations (<tt class="docutils literal"><span class="pre">points_sampled_value</span></tt>),
whereas <tt class="docutils literal"><span class="pre">f(Xs)</span></tt> are <em>GP-predicted</em> function evaluations.</p>
<p><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> is the &#8220;q&#8221; and <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> is the &#8220;p&#8221; in q,p-EI.</p>
<p>In words, we are computing the expected improvement (over the current <tt class="docutils literal"><span class="pre">best_so_far</span></tt>, best known
objective function value) that would result from sampling (aka running new experiments) at
<tt class="docutils literal"><span class="pre">points_to_sample</span></tt> with <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> concurrent/ongoing experiments.</p>
<p>In general, the EI expression is complex and difficult to evaluate; hence we use Monte-Carlo simulation to approximate it.
When faster (e.g., analytic) techniques are available, we will prefer them.</p>
<p>The idea of the MC approach is to repeatedly sample at the union of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and
<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>. This is analogous to gaussian_process_interface.sample_point_from_gp,
but we sample <tt class="docutils literal"><span class="pre">num_union</span></tt> points at once:
<tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">\mu</span> <span class="pre">+</span> <span class="pre">Lw</span></tt>
where <tt class="docutils literal"><span class="pre">\mu</span></tt> is the GP-mean, <tt class="docutils literal"><span class="pre">L</span></tt> is the <tt class="docutils literal"><span class="pre">chol_factor(GP-variance)</span></tt> and <tt class="docutils literal"><span class="pre">w</span></tt> is a vector
of <tt class="docutils literal"><span class="pre">num_union</span></tt> draws from N(0, 1). Then:
<tt class="docutils literal"><span class="pre">improvement_per_step</span> <span class="pre">=</span> <span class="pre">max(max(best_so_far</span> <span class="pre">-</span> <span class="pre">y),</span> <span class="pre">0.0)</span></tt>
Observe that the inner <tt class="docutils literal"><span class="pre">max</span></tt> means only the smallest component of <tt class="docutils literal"><span class="pre">y</span></tt> contributes in each iteration.
We compute the improvement over many random draws and average.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into ExpectedImprovementInterface.compute_expected_improvement() in interfaces/expected_improvement_interface.py.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">properly configured state object</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">state with temporary storage modified; <tt class="docutils literal"><span class="pre">normal_rng</span></tt> modified</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>the expected improvement from sampling <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> with <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> concurrent experiments</dd>
</dl>
</p>
<p><p>Let <tt class="docutils literal"><span class="pre">Ls</span> <span class="pre">*</span> <span class="pre">Ls^T</span> <span class="pre">=</span> <span class="pre">Vars</span></tt> and <tt class="docutils literal"><span class="pre">w</span></tt> = vector of IID normal(0,1) variables
Then:
<tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mus</span> <span class="pre">+</span> <span class="pre">Ls</span> <span class="pre">*</span> <span class="pre">w</span></tt>  (Equation 4, from file docs)
simulates drawing from our GP with mean mus and variance Vars.</p>
<p>Then as given in the file docs, we compute the improvement:
Then the improvement for this single sample is:
<tt class="docutils literal"><span class="pre">I</span> <span class="pre">=</span> <span class="pre">{</span> <span class="pre">best_known</span> <span class="pre">-</span> <span class="pre">min(y)</span>&nbsp;&nbsp; <span class="pre">if</span> <span class="pre">(best_known</span> <span class="pre">-</span> <span class="pre">min(y)</span> <span class="pre">&gt;</span> <span class="pre">0)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">(Equation</span> <span class="pre">5</span> <span class="pre">from</span> <span class="pre">file</span> <span class="pre">docs)</span></tt>
``    {          0               else``
This is implemented as <tt class="docutils literal"><span class="pre">max_{y}</span> <span class="pre">(best_known</span> <span class="pre">-</span> <span class="pre">y)</span></tt>.  Notice that improvement takes the value 0 if it would be negative.</p>
<p>Since we cannot compute <tt class="docutils literal"><span class="pre">min(y)</span></tt> directly, we do so via monte-carlo (MC) integration.  That is, we draw from the GP
repeatedly, computing improvement during each iteration, and averaging the result.</p>
<p>See Scott&#8217;s PhD thesis, sec 6.2.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments here are copied to _compute_expected_improvement_monte_carlo() in python_version/expected_improvement.py</p>
</div>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a67437cb2da0b5d7262c9fd4eaa61edca"></span><div class="line-block">
<div class="line">void <strong>ComputeGradExpectedImprovement</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>StateType</em></a>  * ei_state, double *restrict grad_EI)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the (partial) derivatives of the expected improvement with respect to each point of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>.
As with ComputeExpectedImprovement(), this computation accounts for the effect of <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>
concurrent experiments.</p>
<p><tt class="docutils literal"><span class="pre">points_to_sample</span></tt> is the &#8220;q&#8221; and <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> is the &#8220;p&#8221; in q,p-EI..</p>
<p>In general, the expressions for gradients of EI are complex and difficult to evaluate; hence we use
Monte-Carlo simulation to approximate it. When faster (e.g., analytic) techniques are available, we will prefer them.</p>
<p>The MC computation of grad EI is similar to the computation of EI (decsribed in
compute_expected_improvement). We differentiate <tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">\mu</span> <span class="pre">+</span> <span class="pre">Lw</span></tt> wrt <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>;
only terms from the gradient of <tt class="docutils literal"><span class="pre">\mu</span></tt> and <tt class="docutils literal"><span class="pre">L</span></tt> contribute. In EI, we computed:
<tt class="docutils literal"><span class="pre">improvement_per_step</span> <span class="pre">=</span> <span class="pre">max(max(best_so_far</span> <span class="pre">-</span> <span class="pre">y),</span> <span class="pre">0.0)</span></tt>
and noted that only the smallest component of <tt class="docutils literal"><span class="pre">y</span></tt> may contribute (if it is &gt; 0.0).
Call this index <tt class="docutils literal"><span class="pre">winner</span></tt>. Thus in computing grad EI, we only add gradient terms
that are attributable to the <tt class="docutils literal"><span class="pre">winner</span></tt>-th component of <tt class="docutils literal"><span class="pre">y</span></tt>.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into ExpectedImprovementInterface.compute_expected_improvement() in interfaces/expected_improvement_interface.py.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">properly configured state object</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">state with temporary storage modified; <tt class="docutils literal"><span class="pre">normal_rng</span></tt> modified</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">grad_EI[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">gradient of EI, <tt class="docutils literal"><span class="pre">\pderiv{EI(Xq</span> <span class="pre">\cup</span> <span class="pre">Xp)}{Xq_{d,i}}</span></tt> where <tt class="docutils literal"><span class="pre">Xq</span></tt> is <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>
and <tt class="docutils literal"><span class="pre">Xp</span></tt> is <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> (grad EI from sampling <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> with
<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> concurrent experiments wrt each dimension of the points in <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Computes gradient of EI (see ExpectedImprovementEvaluator::ComputeGradExpectedImprovement) wrt points_to_sample (stored in
union_of_points[0:num_to_sample]).</p>
<p>Mechanism is similar to the computation of EI, where points&#8217; contributions to the gradient are thrown out of their
corresponding <tt class="docutils literal"><span class="pre">improvement</span> <span class="pre">&lt;=</span> <span class="pre">0.0</span></tt>.</p>
<p>Thus <tt class="docutils literal"><span class="pre">\nabla(\mu)</span></tt> only contributes when the <tt class="docutils literal"><span class="pre">winner</span></tt> (point w/best improvement this iteration) is the current point.
That is, the gradient of <tt class="docutils literal"><span class="pre">\mu</span></tt> at <tt class="docutils literal"><span class="pre">x_i</span></tt> wrt <tt class="docutils literal"><span class="pre">x_j</span></tt> is 0 unless <tt class="docutils literal"><span class="pre">i</span> <span class="pre">==</span> <span class="pre">j</span></tt> (and only this result is stored in
<tt class="docutils literal"><span class="pre">ei_state-&gt;grad_mu</span></tt>).  The interaction with <tt class="docutils literal"><span class="pre">ei_state-&gt;grad_chol_decomp</span></tt> is harder to know a priori (like with
<tt class="docutils literal"><span class="pre">grad_mu</span></tt>) and has a more complex structure (rank 3 tensor), so the derivative wrt <tt class="docutils literal"><span class="pre">x_j</span></tt> is computed fully, and
the relevant submatrix (indexed by the current <tt class="docutils literal"><span class="pre">winner</span></tt>) is accessed each iteration.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments here are copied to _compute_grad_expected_improvement_monte_carlo() in python_version/expected_improvement.py</p>
</div>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1adcb50267f78ed0d340cc1554cc86d300"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>ExpectedImprovementEvaluator</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1ae5df3ace2b63741c34e88636df0ecb11"></span>const int <strong>dim_</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of points_sampled) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a108befbd557e2663e94c40bc49627deb"></span>int <strong>num_mc_iterations_</strong></p>
<blockquote>
<div><p>number of monte carlo iterations </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1ae07d6698329fedd438e42203d697cf01"></span>double <strong>best_so_far_</strong></p>
<blockquote>
<div><p>best (minimum) objective function value (in points_sampled_value) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_expected_improvement_evaluator_1a9bc5a98618aa671dc0c65c49f3d654b5"></span>const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * <strong>gaussian_process_</strong></p>
<blockquote>
<div><p>pointer to gaussian process used in EI computations </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0structoptimal__learning_1_1_expected_improvement_state"><em>class</em> <strong>ExpectedImprovementState</strong></p>
<blockquote>
<div><p></p>
<p><p>State object for ExpectedImprovementEvaluator.  This tracks the points being sampled in concurrent experiments
(<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>) ALONG with the points currently being evaluated via expected improvement for future experiments
(called <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>); these are the p and q of q,p-EI, respectively.  <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> joined with
<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> is stored in <tt class="docutils literal"><span class="pre">union_of_points</span></tt> in that order.</p>
<p>This struct also tracks the state of the GaussianProcess that underlies the expected improvement computation: the GP state
is built to handle the initial <tt class="docutils literal"><span class="pre">union_of_points</span></tt>, and subsequent updates to <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> in this object also update
the GP state.</p>
<p>This struct also holds a pointer to a random number generator needed for Monte Carlo integrated EI computations.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Users MUST guarantee that multiple state objects DO NOT point to the same RNG (in a multithreaded env).</p>
</div>
<p>See general comments on State structs in <tt class="docutils literal"><span class="pre">gpp_common.hpp</span></tt>&#8216;s header docs.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a92e59b1178926d318d5d09ad4b0fbb6a"></span>typedef <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>ExpectedImprovementEvaluator</em></a> <strong>EvaluatorType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a1579dd76b1bb5207b168d3c35f4e3a0a"></span><div class="line-block">
<div class="line"> <strong>ExpectedImprovementState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample_in, int num_being_sampled_in, bool configure_for_gradients, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs an ExpectedImprovementState object with a specified source of randomness for the purpose of computing EI
(and its gradient) over the specified set of points to sample.
This establishes properly sized/initialized temporaries for EI computation, including dependent state from the
associated Gaussian Process (which arrives as part of the ei_evaluator).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object is invalidated if the associated ei_evaluator is mutated.  SetupState() should be called to reset.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Using this object to compute gradients when <tt class="docutils literal"><span class="pre">configure_for_gradients</span></tt> := false results in UNDEFINED BEHAVIOR.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test points for GP predictions)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points being sampled in concurrent experiments</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points being sampled in concurrent experiments (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">configure_for_gradients:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">true if this object will be used to compute gradients, false otherwise</td>
</tr>
<tr class="field-odd field"><th class="field-name">normal_rng[1]:</th><td class="field-body">pointer to a properly initialized* NormalRNG object</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">* The NormalRNG object must already be seeded.  If multithreaded computation is used for EI, then every state object
must have a different NormalRNG (different seeds, not just different objects).</p>
</div>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1af12d2f71fb04b31cfa0291c94f2d3df3"></span><div class="line-block">
<div class="line"> <strong>ExpectedImprovementState</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>ExpectedImprovementState</em></a>  &amp;&amp; OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a11f244647c229e703269cf77bf899cca"></span><div class="line-block">
<div class="line">int <strong>GetProblemSize</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a20659c303bc40b035b66424b2a5f682b"></span><div class="line-block">
<div class="line">void <strong>GetCurrentPoint</strong>(double *restrict points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Get the <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>: potential future samples whose EI (and/or gradients) are being evaluated</p>
<dl class="docutils">
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">potential future samples whose EI (and/or gradients) are being evaluated</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a26f442858baaf6c0f5ed51e5d17fcb25"></span><div class="line-block">
<div class="line">void <strong>UpdateCurrentPoint</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Change the potential samples whose EI (and/or gradient) are being evaluated.
Update the state&#8217;s derived quantities to be consistent with the new points.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">potential future samples whose EI (and/or gradients) are being evaluated</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1ac13e50a50934806c9ac06f1d94e4f7aa"></span><div class="line-block">
<div class="line">void <strong>SetupState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Configures this state object with new <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>, the location of the potential samples whose EI is to be evaluated.
Ensures all state variables &amp; temporaries are properly sized.
Properly sets all dependent state variables (e.g., GaussianProcess&#8217;s state) for EI evaluation.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object&#8217;s state is INVALIDATED if the <tt class="docutils literal"><span class="pre">ei_evaluator</span></tt> (including the GaussianProcess it depends on) used in
SetupState is mutated! SetupState() should be called again in such a situation.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">potential future samples whose EI (and/or gradients) are being evaluated</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1ab75cdcdfb4d1d8d94333638a1ac0f7ca"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_expected_improvement_state"><em>ExpectedImprovementState</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a49ed2ac0194b515491f7d562bed403d1"></span>const int <strong>dim</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of <tt class="docutils literal"><span class="pre">points_sampled</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1ab4d5ccffc908680cf16357bfa523d57c"></span>const int <strong>num_to_sample</strong></p>
<blockquote>
<div><p>number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a40ed8dc0e1d41c0b59bfe2606c91de12"></span>const int <strong>num_being_sampled</strong></p>
<blockquote>
<div><p>number of points being sampled concurrently (i.e., the &#8220;p&#8221; in q,p-EI) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a9cae7548b81d9cad038d22638d333fdd"></span>const int <strong>num_derivatives</strong></p>
<blockquote>
<div><p>number of derivative terms desired (usually 0 for no derivatives or num_to_sample) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1af201d0ffcb0edc238b874e1d292fe2d6"></span>const int <strong>num_union</strong></p>
<blockquote>
<div><p>number of points in union_of_points: num_to_sample + num_being_sampled </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a16dec083b099aa9c888a06f01c291395"></span>std::vector&lt; double &gt; <strong>union_of_points</strong></p>
<blockquote>
<div><p></p>
<p>points currently being sampled; this is the union of the points represented by &#8220;q&#8221; and &#8220;p&#8221; in q,p-EI <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> is stored first in memory, immediately followed by <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt> </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1af33ee514ca5c498e91ca38c1888ce28a"></span><a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>GaussianProcess::StateType</em></a> <strong>points_to_sample_state</strong></p>
<blockquote>
<div><p>gaussian process state </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a98a2be7c17bbb8067f824a4156168203"></span><a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * <strong>normal_rng</strong></p>
<blockquote>
<div><p>random number generator </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1ad07bf7af213a02c229ebefc41f3c403a"></span>std::vector&lt; double &gt; <strong>to_sample_mean</strong></p>
<blockquote>
<div><p>the mean of the GP evaluated at union_of_points </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a8c46e3e7b0b9689fa2897a1fbf247abd"></span>std::vector&lt; double &gt; <strong>grad_mu</strong></p>
<blockquote>
<div><p>the gradient of the GP mean evaluated at union_of_points, wrt union_of_points[0:num_to_sample] </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1ac2b4de23542dce28f26dea5960d4b3fe"></span>std::vector&lt; double &gt; <strong>cholesky_to_sample_var</strong></p>
<blockquote>
<div><p>the cholesky (<tt class="docutils literal"><span class="pre">LL^T</span></tt>) factorization of the GP variance evaluated at union_of_points </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1aca6d60490bb2bb676b2294e91811ba42"></span>std::vector&lt; double &gt; <strong>grad_chol_decomp</strong></p>
<blockquote>
<div><p>the gradient of the cholesky (<tt class="docutils literal"><span class="pre">LL^T</span></tt>) factorization of the GP variance evaluated at union_of_points wrt union_of_points[0:num_to_sample] </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a60af3363bd943243e6581868733e4ee7"></span>std::vector&lt; double &gt; <strong>EI_this_step_from_var</strong></p>
<blockquote>
<div><p>improvement (per mc iteration) evaluated at each of union_of_points </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a78ce7e2df5a271e258cb65d2c2d66b5d"></span>std::vector&lt; double &gt; <strong>aggregate</strong></p>
<blockquote>
<div><p>tracks the aggregate grad EI from all mc iterations </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1aa0a172e911773e4908c1c95b8a66a4a2"></span>std::vector&lt; double &gt; <strong>normals</strong></p>
<blockquote>
<div><p>normal rng draws </p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Static Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_expected_improvement_state_1a7e44ab2eb36d10c8249cd22d5b9018c5"></span><div class="line-block">
<div class="line">std::vector&lt; double &gt; <strong>BuildUnionOfPoints</strong>(double const *restrict points_to_sample, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, int dim)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Create a vector with the union of points_to_sample and points_being_sampled (the latter is appended to the former).</p>
<p>Note the l-value return. Assigning the return to a std::vector&lt;double&gt; or passing it as an argument to the ctor
will result in copy-elision or move semantics; no copying/performance loss.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>::</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test points for GP predictions)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points being sampled in concurrent experiments</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">number of potential future samples; gradients are evaluated wrt these points (i.e., the &#8220;q&#8221; in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points being sampled in concurrent experiments (i.e., the &#8220;p&#8221; in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the number of spatial dimensions of each point array</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>std::vector&lt;double&gt; with the union of the input arrays: points_being_sampled is <em>appended</em> to points_to_sample</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>class</em> <strong>OnePotentialSampleExpectedImprovementEvaluator</strong></p>
<blockquote>
<div><p></p>
<p><p>This is a specialization of the ExpectedImprovementEvaluator class for when the number of potential samples is 1; i.e.,
<tt class="docutils literal"><span class="pre">num_to_sample</span> <span class="pre">==</span> <span class="pre">1</span></tt> and the number of concurrent samples is 0; i.e. <tt class="docutils literal"><span class="pre">num_being_sampled</span> <span class="pre">==</span> <span class="pre">0</span></tt>.
In other words, this class only supports the computation of 1,0-EI.  In this case, we have analytic formulas
for computing EI and its gradient.</p>
<p>Thus this class does not perform any explicit numerical integration, nor do its EI functions require access to a
random number generator.</p>
<p>This class&#8217;s methods have some parameters that are unused or redundant.  This is so that the interface matches that of
the more general ExpectedImprovementEvaluator.</p>
<p>For other details, see ExpectedImprovementEvaluator for more complete description of what EI is and the outputs of
EI and grad EI computations.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a2ed126f0169d0b3cec7f351933632623"></span>typedef <a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>OnePotentialSampleExpectedImprovementState</em></a> <strong>StateType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a2506f06621e4214a1ea661a8102f0b05"></span><div class="line-block">
<div class="line"> <strong>OnePotentialSampleExpectedImprovementEvaluator</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process_in, double best_so_far)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a OnePotentialSampleExpectedImprovementEvaluator object.  All inputs are required; no default constructor nor copy/assignment are allowed.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name">best_so_far:</th><td class="field-body">best (minimum) objective function value (in <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt>)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a74b2aec1b1771ce5f6670ded5bbc382f"></span><div class="line-block">
<div class="line">int <strong>dim</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1ace2769b5c292680fe0e0c0b976cf6c91"></span><div class="line-block">
<div class="line">const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * <strong>gaussian_process</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a0d8f44fa9bcaf77493fc11f60c2ab199"></span><div class="line-block">
<div class="line">double <strong>ComputeObjectiveFunction</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>StateType</em></a>  * ei_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Wrapper for ComputeExpectedImprovement(); see that function for details.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a1b5cfeeb3deeba17f68b0117a91eb11a"></span><div class="line-block">
<div class="line">void <strong>ComputeGradObjectiveFunction</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>StateType</em></a>  * ei_state, double *restrict grad_EI)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Wrapper for ComputeGradExpectedImprovement(); see that function for details.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a1f0975ac8fbb28f24b73ff5ea26f3b45"></span><div class="line-block">
<div class="line">double <strong>ComputeExpectedImprovement</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>StateType</em></a>  * ei_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the expected improvement <tt class="docutils literal"><span class="pre">EI(Xs)</span> <span class="pre">=</span> <span class="pre">E_n[[f^*_n(X)</span> <span class="pre">-</span> <span class="pre">min(f(Xs_1),...,f(Xs_m))]^+]</span></tt></p>
<p>Uses analytic formulas to evaluate the expected improvement.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">properly configured state object</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">state with temporary storage modified</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>the expected improvement from sampling <tt class="docutils literal"><span class="pre">point_to_sample</span></tt></dd>
</dl>
</p>
<p><p>Uses analytic formulas to compute EI when <tt class="docutils literal"><span class="pre">num_to_sample</span> <span class="pre">=</span> <span class="pre">1</span></tt> and <tt class="docutils literal"><span class="pre">num_being_sampled</span> <span class="pre">=</span> <span class="pre">0</span></tt> (occurs only in 1,0-EI).
In this case, the single-parameter (posterior) GP is just a Gaussian.  So the integral in EI (previously eval&#8217;d with MC)
can be computed &#8216;exactly&#8217; using high-accuracy routines for the pdf &amp; cdf of a Gaussian random variable.</p>
<p>See Ginsbourger, Le Riche, and Carraro.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a56edea4e46e13a8fa99524e96ad75cc8"></span><div class="line-block">
<div class="line">void <strong>ComputeGradExpectedImprovement</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>StateType</em></a>  * ei_state, double *restrict grad_EI exp_grad_EI)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Computes the (partial) derivatives of the expected improvement with respect to the point to sample.</p>
<p>Uses analytic formulas to evaluate the spatial gradient of the expected improvement.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">properly configured state object</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_state[1]:</th><td class="field-body">state with temporary storage modified</td>
</tr>
<tr class="field-even field"><th class="field-name">grad_EI[dim]:</th><td class="field-body">gradient of EI, <tt class="docutils literal"><span class="pre">\pderiv{EI(x)}{x_d}</span></tt>, where <tt class="docutils literal"><span class="pre">x</span></tt> is <tt class="docutils literal"><span class="pre">points_to_sample</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
</p>
<p><p>Differentiates OnePotentialSampleExpectedImprovementEvaluator::ComputeExpectedImprovement wrt
<tt class="docutils literal"><span class="pre">points_to_sample</span></tt> (which is just ONE point; i.e., 1,0-EI).
Again, this uses analytic formulas in terms of the pdf &amp; cdf of a Gaussian since the integral in EI (and grad EI)
can be evaluated exactly for this low dimensional case.</p>
<p>See Ginsbourger, Le Riche, and Carraro.</p>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a4a6ff4a6ee72e4d3a73328edbd2f0c49"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>OnePotentialSampleExpectedImprovementEvaluator</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1adddb667159b1f7cf7b757ab58af03408"></span>const int <strong>dim_</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of <tt class="docutils literal"><span class="pre">points_sampled</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a1bbad8714558609caddbf245868504b3"></span>double <strong>best_so_far_</strong></p>
<blockquote>
<div><p>best (minimum) objective function value (in <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1a4c0a40357072693dda0727116ce6f5b3"></span>const boost::math::normal_distribution&lt; double &gt; <strong>normal_</strong></p>
<blockquote>
<div><p>normal distribution object </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator_1acfc1ec21fd9b674b0b1786ab0d04c938"></span>const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * <strong>gaussian_process_</strong></p>
<blockquote>
<div><p>pointer to gaussian process used in EI computations </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>class</em> <strong>OnePotentialSampleExpectedImprovementState</strong></p>
<blockquote>
<div><p></p>
<p><p>State object for OnePotentialSampleExpectedImprovementEvaluator.  This tracks the <em>ONE</em> <tt class="docutils literal"><span class="pre">point_to_sample</span></tt>
being evaluated via expected improvement.</p>
<p>This is just a special case of ExpectedImprovementState; see those class docs for more details.
See general comments on State structs in <tt class="docutils literal"><span class="pre">gpp_common.hpp</span></tt>&#8216;s header docs.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1ae2f889b14bc7ac84f78efe9077fd6121"></span>typedef <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>OnePotentialSampleExpectedImprovementEvaluator</em></a> <strong>EvaluatorType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a360fafae0a3d2214d4bc98cf1a039247"></span><div class="line-block">
<div class="line"> <strong>OnePotentialSampleExpectedImprovementState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict point_to_sample_in, bool configure_for_gradients)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs an OnePotentialSampleExpectedImprovementState object for the purpose of computing EI
(and its gradient) over the specified point to sample.
This establishes properly sized/initialized temporaries for EI computation, including dependent state from the
associated Gaussian Process (which arrives as part of the <tt class="docutils literal"><span class="pre">ei_evaluator</span></tt>).</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object is invalidated if the associated ei_evaluator is mutated.  SetupState() should be called to reset.</p>
</div>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Using this object to compute gradients when <tt class="docutils literal"><span class="pre">configure_for_gradients</span></tt> := false results in UNDEFINED BEHAVIOR.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">point_to_sample[dim]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">point at which to evaluate EI and/or its gradient to check their value in future experiments (i.e., test point for GP predictions)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">configure_for_gradients:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">true if this object will be used to compute gradients, false otherwise</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1ae1c4e453a1a8f1cb5844a8b9de50a42d"></span><div class="line-block">
<div class="line"> <strong>OnePotentialSampleExpectedImprovementState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict points_to_sample, double const *restrict  OL_UNUSED, int  OL_UNUSED, int  OL_UNUSED, bool configure_for_gradients, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructor wrapper to match the signature of the ctor for ExpectedImprovementState().</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a27e3d90956713751164efca0cf2527f3"></span><div class="line-block">
<div class="line"> <strong>OnePotentialSampleExpectedImprovementState</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>OnePotentialSampleExpectedImprovementState</em></a>  &amp;&amp; OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a07022fea0a1a3f04472e5f0d724d9cd3"></span><div class="line-block">
<div class="line">int <strong>GetProblemSize</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1ac1702b47d038c542ad73ee92c5fab853"></span><div class="line-block">
<div class="line">void <strong>GetCurrentPoint</strong>(double *restrict point_to_sample_out)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Get <tt class="docutils literal"><span class="pre">point_to_sample</span></tt>: the potential future sample whose EI (and/or gradients) is being evaluated</p>
<dl class="docutils">
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">point_to_sample[dim]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">potential sample whose EI is being evaluted</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a1d3f958ce6289297cc53afa64e08d462"></span><div class="line-block">
<div class="line">void <strong>UpdateCurrentPoint</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict point_to_sample_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Change the potential sample whose EI (and/or gradient) is being evaluated.
Update the state&#8217;s derived quantities to be consistent with the new point.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">point_to_sample[dim]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">potential future sample whose EI (and/or gradients) is being evaluated</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a91e1d82f0b34a3d9af6e4c6c517bd32a"></span><div class="line-block">
<div class="line">void <strong>SetupState</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_one_potential_sample_expected_improvement_evaluator"><em>EvaluatorType</em></a>  &amp; ei_evaluator, double const *restrict point_to_sample_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Configures this state object with a new <tt class="docutils literal"><span class="pre">point_to_sample</span></tt>, the location of the potential sample whose EI is to be evaluated.
Ensures all state variables &amp; temporaries are properly sized.
Properly sets all dependent state variables (e.g., GaussianProcess&#8217;s state) for EI evaluation.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">This object&#8217;s state is INVALIDATED if the ei_evaluator (including the GaussianProcess it depends on) used in
SetupState is mutated! SetupState() should be called again in such a situation.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">ei_evaluator:</th><td class="field-body">expected improvement evaluator object that specifies the parameters &amp; GP for EI evaluation</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">point_to_sample[dim]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">potential future sample whose EI (and/or gradients) is being evaluated</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a96ba0a073d87443a1727e9f866516f23"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state"><em>OnePotentialSampleExpectedImprovementState</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a11186f208e36b48b4908921b22a3b7cc"></span>const int <strong>dim</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of <tt class="docutils literal"><span class="pre">points_sampled</span></tt>) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1acfc843f8611f798663a831d3c701e22a"></span>const int <strong>num_to_sample</strong></p>
<blockquote>
<div><p>number of points to sample (i.e., the &#8220;q&#8221; in q,p-EI); MUST be 1 </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1aba470109e51ee94693d33bd02a150ebf"></span>const int <strong>num_derivatives</strong></p>
<blockquote>
<div><p>number of derivative terms desired (usually 0 for no derivatives or num_to_sample) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a0c6084594c436a98522d16895e06ffcc"></span>std::vector&lt; double &gt; <strong>point_to_sample</strong></p>
<blockquote>
<div><p>point at which to evaluate EI and/or its gradient (e.g., to check its value in future experiments) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a36d27f8399557ded2c77e56e264aaa20"></span><a class="reference internal" href="#project0structoptimal__learning_1_1_points_to_sample_state"><em>GaussianProcess::StateType</em></a> <strong>points_to_sample_state</strong></p>
<blockquote>
<div><p>gaussian process state </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1ae6ff5ee5c740086f668887b53c2300ef"></span>std::vector&lt; double &gt; <strong>grad_mu</strong></p>
<blockquote>
<div><p>the gradient of the GP mean evaluated at point_to_sample, wrt point_to_sample </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_one_potential_sample_expected_improvement_state_1a8262dba473a2c9c602201b7dc2f641b9"></span>std::vector&lt; double &gt; <strong>grad_chol_decomp</strong></p>
<blockquote>
<div><p>the gradient of the sqrt of the GP variance evaluated at point_to_sample wrt point_to_sample </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div></blockquote>
</p>
</div>
<div class="section" id="gpp-math-cpp">
<h2>gpp_math.cpp<a class="headerlink" href="#gpp-math-cpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><p>These comments are getting to be of some length, so here&#8217;s a table of contents:</p>
<ol class="arabic simple">
<li>FILE OVERVIEW</li>
<li>IMPLEMENTATION NOTES</li>
<li>MATHEMATICAL OVERVIEW<ol class="loweralpha">
<li>GAUSSIAN PROCESSES</li>
<li>SAMPLING FROM GPs</li>
<li>EXPECTED IMPROVEMENT</li>
</ol>
</li>
<li>CODE DESIGN/LAYOUT OVERVIEW:<ol class="loweralpha">
<li>class GaussianProcess</li>
<li>class ExpectedImprovementEvaluator, OnePotentialSampleExpectedImprovementEvaluator</li>
<li>function ComputeOptimalPointsToSampleWithRandomStarts()</li>
</ol>
</li>
<li>CODE HIERARCHY / CALL-TREE</li>
</ol>
<p><strong>1. FILE OVERVIEW</strong></p>
<p>Implementations of functions for Gaussian Processes (mean, variance of GPs and their gradients) and for
computing and optimizing Expected Improvement (EI).</p>
<p><strong>2. IMPLEMENTATION NOTES</strong></p>
<p>See gpp_math.hpp file docs and gpp_common.hpp for a few important implementation notes
(e.g., restrict, memory allocation, matrix storage style, etc), as well as citation details.</p>
<p>Additionally, the matrix looping idioms used in this file deserve further mention: see the file comments
for gpp_common.hpp, item 8 for further details.  In summary, using matrix-vector-multiply as an example, we do:</p>
<div class="highlight-python"><div class="highlight"><pre>for (int i = 0; i &lt; m; ++i) {
  y[i] = 0;
  for (int j = 0; j &lt; n; ++j) {
    y[i] += A[j]*x[j];
  }
  A += n;
}
</pre></div>
</div>
<p><strong>3. MATHEMATICAL OVERVIEW</strong></p>
<p>Next, we provide a high-level discussion of GPs and the EI optimization process used in this file.  See
Rasmussen &amp; Williams for more details on the former and Scott Clark&#8217;s thesis for details on the latter.  This segment
is more focused on concepts and mathematical ideas.  We subsequently discuss how the classes and functions
in this file map onto these mathematical concepts.  If it wasn&#8217;t clear, please read the file comments for
gpp_math.hpp before continuing (a conceptual overview).</p>
<p><strong>3a. GAUSSIAN PROCESSES</strong></p>
<p>First, a Gaussian Process (GP) is defined as a collection of normally distributed random variables (RVs); these
RVs are not independent nor identically-distributed (i.e., all normal but different mean/var) in general.  Since
the GP is a collection of RVs, it defines a distribution over FUNCTIONS.  So drawing from the GP realizes
one particular function.</p>
<p>Now let X = training data; these are our experimental independent variables
let f = training data observed values; this is our (SCALAR) dependent-variable
So for <tt class="docutils literal"><span class="pre">(X_i,</span> <span class="pre">f_i)</span></tt> pairs, we say:
<tt class="docutils literal"><span class="pre">f</span> <span class="pre">~</span> <span class="pre">GP(0,</span> <span class="pre">cov(X,X))</span> <span class="pre">/equiv</span> <span class="pre">N(0,</span> <span class="pre">cov(X,X))</span></tt>
the training data, f, is distributed like a (multi-variate) Gaussian with mean 0 and <tt class="docutils literal"><span class="pre">variance</span> <span class="pre">=</span> <span class="pre">cov(X,X)</span></tt>.
Drawing from this GP requires conditioning on the result satisfying the training data.  That is, the realized
function must pass through all points <tt class="docutils literal"><span class="pre">(X,f)</span></tt>.  Between these, &#8220;essentially any&#8221; behavior is possible, although certain
behaviors are more likely as specified via <tt class="docutils literal"><span class="pre">cov(X,X)</span></tt>.
Note that the GP has 0 mean (and no signal variance) to specify that it passes through X,f exactly.  Nonzero mean
would shift the entire distribution so that it passes through <tt class="docutils literal"><span class="pre">(X,f+mu)</span></tt>.</p>
<p>In the following, K(X,X) is the covariance function.  It&#8217;s given as an input to this whole process and is critical
in informing the behavior of the GP.  The covariance function describes how related we (a priori) believe prior
points are to each other.
In code, the covariance function is specified through the CovarianceInterface class.</p>
<p>In a noise-free setting (signal noise modifies <tt class="docutils literal"><span class="pre">K</span></tt> to become <tt class="docutils literal"><span class="pre">K</span> <span class="pre">+</span> <span class="pre">\sigma^2</span> <span class="pre">*</span> <span class="pre">Id</span></tt>, <tt class="docutils literal"><span class="pre">Id</span></tt> being identity), the joint
distribution of training inputs, <tt class="docutils literal"><span class="pre">f</span></tt>, and test outputs, <tt class="docutils literal"><span class="pre">fs</span></tt>, is:
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">f</span>&nbsp; <span class="pre">]</span>&nbsp; <span class="pre">~</span> <span class="pre">N(</span> <span class="pre">0,</span> <span class="pre">[</span> <span class="pre">K(X,X)</span>&nbsp;&nbsp; <span class="pre">K(X,Xs)</span>&nbsp; <span class="pre">]</span>&nbsp; <span class="pre">=</span> <span class="pre">[</span> <span class="pre">K</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">Ks</span>&nbsp; <span class="pre">]</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">(Equation</span> <span class="pre">1,</span> <span class="pre">Rasmussen</span> <span class="pre">&amp;</span> <span class="pre">Williams</span> <span class="pre">2.18)</span></tt>
<tt class="docutils literal"><span class="pre">[</span> <span class="pre">fs</span> <span class="pre">]</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">[</span> <span class="pre">K(Xs,X)</span>&nbsp; <span class="pre">K(Xs,Xs)</span> <span class="pre">]</span>&nbsp;&nbsp;&nbsp; <span class="pre">[</span> <span class="pre">Ks^T</span>&nbsp; <span class="pre">Kss</span> <span class="pre">]</span></tt>
where the test outputs are drawn from the prior.
<tt class="docutils literal"><span class="pre">K(X,X)</span></tt> and <tt class="docutils literal"><span class="pre">K(Xs,Xs)</span></tt> are computed in BuildCovarianceMatrix()
<tt class="docutils literal"><span class="pre">K(X,Xs)</span></tt> is computed by BuildMixCovarianceMatrix(); and <tt class="docutils literal"><span class="pre">K(Xs,X)</span></tt> is its transpose.
<tt class="docutils literal"><span class="pre">K</span> <span class="pre">+</span> <span class="pre">\sigma^2</span></tt> is computed in BuildCovarianceMatrixWithNoiseVariance(); almost all practical uses of GPs and EI will
be over data with nonzero noise variance.  However this is immaterial to the rest of the discussion here.</p>
<p><strong>3b. SAMPLING FROM GPs</strong></p>
<p>So to obtain the posterior distribution, fs, we again sample this joint prior and throw out any function
realizations that do not satisfy the observations (i.e., pass through all <tt class="docutils literal"><span class="pre">(X,f)</span></tt> pairs).  This is expensive.</p>
<p>Instead, we can use math to compute the posterior by conditioning it on the prior:
<tt class="docutils literal"><span class="pre">fs</span> <span class="pre">|</span> <span class="pre">Xs,X,f</span> <span class="pre">~</span> <span class="pre">N(</span> <span class="pre">mus,</span> <span class="pre">Vars)</span></tt>
where <tt class="docutils literal"><span class="pre">mus</span> <span class="pre">=</span> <span class="pre">K(Xs,X)</span> <span class="pre">*</span> <span class="pre">K(X,X)^-1</span> <span class="pre">*</span> <span class="pre">f</span> <span class="pre">=</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">f,</span>&nbsp; <span class="pre">(Equation</span> <span class="pre">2,</span> <span class="pre">Rasmussen</span> <span class="pre">&amp;</span> <span class="pre">Williams</span> <span class="pre">2.19)</span></tt>
which is computed in GaussianProcess::ComputeMeanOfPoints.
and  <tt class="docutils literal"><span class="pre">Vars</span> <span class="pre">=</span> <span class="pre">K(Xs,Xs)</span> <span class="pre">-</span> <span class="pre">K(Xs,X)</span> <span class="pre">*</span> <span class="pre">K(X,X)^-1</span> <span class="pre">*</span> <span class="pre">K(X,Xs)</span> <span class="pre">=</span> <span class="pre">Kss</span> <span class="pre">-</span> <span class="pre">Ks^T</span> <span class="pre">*</span> <span class="pre">K^-1</span> <span class="pre">*</span> <span class="pre">Ks,</span> <span class="pre">(Equation</span> <span class="pre">3,</span> <span class="pre">Rasumussen</span> <span class="pre">&amp;</span> <span class="pre">Williams</span> <span class="pre">2.19)</span></tt>
which is implemented in GaussianProcess::ComputeVarianceOfPoints (and provably SPD).</p>
<p>Now we can draw from this multi-variate Gaussian by:
<tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">mus</span> <span class="pre">+</span> <span class="pre">L</span> <span class="pre">*</span> <span class="pre">w</span>&nbsp;&nbsp;&nbsp; <span class="pre">(Equation</span> <span class="pre">4)</span></tt>
where <tt class="docutils literal"><span class="pre">L</span> <span class="pre">*</span> <span class="pre">L^T</span> <span class="pre">=</span> <span class="pre">Vars</span></tt> (cholesky-factorization) and w is a vector of samples from <tt class="docutils literal"><span class="pre">N(0,1)</span></tt>
Note that if our GP has 10 dimensions (variables), then y contains 10 sample values.</p>
<p><strong>3c. EXPECTED IMPROVEMENT</strong></p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">these comments are copied in Python: interfaces/expected_improvement_interface.py</p>
</div>
<p>Then the improvement for this single sample is:
<tt class="docutils literal"><span class="pre">I</span> <span class="pre">=</span> <span class="pre">{</span> <span class="pre">best_known</span> <span class="pre">-</span> <span class="pre">min(y)</span>&nbsp;&nbsp; <span class="pre">if</span> <span class="pre">(best_known</span> <span class="pre">-</span> <span class="pre">min(y)</span> <span class="pre">&gt;</span> <span class="pre">0)</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">(Equation</span> <span class="pre">5)</span></tt>
``    {          0               else``</p>
<p>And the expected improvement, EI, can be computed by averaging repeated computations of I; i.e., monte-carlo integration.
This is done in ExpectedImprovementEvaluator::ComputeExpectedImprovement(); we can also compute the gradient. This
computation is needed in the optimization of q,p-EI.</p>
<p>There is also a special, analytic case of EI computation that does not require monte-carlo integration. This special
case can only be used to compute 1,0-EI (and its gradient). Still this can be very useful (e.g., the heuristic
optimization in gpp_heuristic_expected_improvement_optimization.hpp estimates q,0-EI by repeatedly solving
1,0-EI).</p>
<p>From there, since EI is taken from a sum of gaussians, we expect it to be reasonably smooth
and apply multistart, restarted gradient descent to find the optimum.  The use of gradient descent
implies the need for all of the various &#8220;grad&#8221; functions, e.g., GP::ComputeGradMeanOfPoints().
This is handled starting in the highest level functions of file, ComputeOptimalPointsToSample().</p>
<p><strong>4. CODE OVERVIEW</strong></p>
<p>Finally, we give some further details about how the previous ideas map into the code.  We begin with an overview
of important classes and functions in this file, and end by going over the call stack for the EI optimization entry point.</p>
<p><strong>4a. First, the GaussianProcess (GP) class</strong></p>
<p>The GaussianProcess class abstracts the handling of GPs and their properties; quickly going over the functionality: it
provides methods for computing mean, variance, cholesky of variance, and their gradients (wrt spatial dimensions).
GP also allows the user to sample function values from it, distributed according to the GP prior.  Lastly GP provides
the ability to change the hyperparameters of its covariance function (although currently you cannot change the
covariance function; this would not be difficult to add).</p>
<p>Computation-wise, GaussianProcess also makes precomputation and preallocation convenient.  The class tracks all of its
inputs (e.g., <tt class="docutils literal"><span class="pre">X</span></tt>, <tt class="docutils literal"><span class="pre">f</span></tt>, noise var, covariance) as well as quantities that are derivable from <em>only</em> these inputs; e.g.,
<tt class="docutils literal"><span class="pre">K</span></tt>, cholesky factorization of <tt class="docutils literal"><span class="pre">K</span></tt>, <tt class="docutils literal"><span class="pre">K^-1*y</span></tt>.  Thus repeated calculations with the GP over the same training data avoids
(very expensive) factorizations of <tt class="docutils literal"><span class="pre">K</span></tt>.</p>
<p>A last note about GP: it uses the State idiom laid out in gpp_common.hpp.  The associated state is PointsToSampleState.
PointsToSampleState tracks the current &#8220;test&#8221; data set, points_to_sample&#8211;the set of currently running experiments,
possibly including the current point(s) being optimized. In the q,p-EI terminology, PointsToSampleState tracks the
union of <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> and <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>. PointsToSampleState preallocates all vectors needed by GP&#8217;s
member functions; it also precomputes (per <tt class="docutils literal"><span class="pre">points_to_sample</span></tt> update) some derived quantities that are used repeatedly
by GP member functions.</p>
<p>In current usage, users generally will not need to access GaussianProcess&#8217;s member functions directly; instead these are
used indirectly when users compute or optimize EI.  Plotting/visualization might be one reason to call GP members directly.</p>
<p><strong>4b. Next, the ExpectedImprovementEvaluator and OnePotentialSampleExpectedImprovementEvaulator classes</strong></p>
<p>ExpectedImprovementEvaluator abstracts the computation of EI and its gradient.  This class references a single
GaussianProcess that it uses to compute EI/grad EI as described above.  Equations 4, 5 above detailed the EI computation;
further details can be found below in the call tree discussion as well as in the implementation docs for these
functions.  The gradient of EI is implemented similarly; see implementation docs for details on the one subtlety.</p>
<p>OnePotentialSample is a special case of ExpectedImprovementEvaluator. With <tt class="docutils literal"><span class="pre">num_to_sample</span> <span class="pre">=</span> <span class="pre">1</span></tt> and <tt class="docutils literal"><span class="pre">num_being_sampled</span> <span class="pre">=</span> <span class="pre">0</span></tt>
(only occurs in 1,0-EI evaluation/optimization), there is only one experiment to worry about and no concurrent events.
This simplifies the EI computation substantially (multi-dimensional Gaussians become a simple one dimensional case)
and we can write EI analytically in terms of the PDF and CDF of a N(0,1) normal distribution (which are evaluated
numerically by boost). No monte-carlo necessary!</p>
<p>ExpectedImprovementEvaluator and OnePotentialSample have corresponding State classes as well.  These are similar
to each other except OnePotentialSample does not have a NormalRNG pointer (since it does no MC integration) and some
temporaries are dropped since they have size 1.  But for the general EI&#8217;s State, the NormalRNG pointer must reference
a different object for each thread!  Notably, both EI State classes construct their own GaussianProcess::StateType
object for use with GP members.  As long as there is only one EI state per thread, This ensures thread safety since there
is never a reason (or a way) for multiple threads to accidentally use the same GP state.  Finally, the EI state classes
hold some pre-allocated vectors for use as local temporaries by EI and GradEI computation.</p>
<p><strong>4c. And finally, we discuss selecting optimal experiments with ComputeOptimalPointsToSampleWithRandomStarts()</strong></p>
<p>This function is the top of the hierarchy for EI optimization.  It encompasses a multistart, restarted gradient descent
method.  Since this is not a convex optimization problem, there could be multiple local optima (or even 0 optima).  So
we start GD from multiple locations (multistart) as a heuristic in hopes of finding the global optima.</p>
<p>See the file comments of gpp_optimization.hpp for more details on the base gradient descent implementation and the restart
component of restarted gradient descent.</p>
<p><strong>5. CODE HIERARCHY / CALL-TREE</strong></p>
<p>For obtaining multiple new points to sample (q,p-EI), we have two main paths for optimization: multistart gradient
descent and &#8216;dumb&#8217; search. The optimization hierarchy looks like (these optimization functions are in the header;
they are templates):
ComputeOptimalPointsToSampleWithRandomStarts&lt;...&gt;(...)  (selects random points; defined in math.hpp)</p>
<ul>
<li><p class="first">Solves q,p-EI.</p>
</li>
<li><p class="first">Selects random starting locations based on random sampling from the domain (e.g., latin hypercube)</p>
</li>
<li><p class="first">This calls:</p>
<p>ComputeOptimalPointsToSampleViaMultistartGradientDescent&lt;...&gt;(...)  (multistart gradient descent)</p>
<ul>
<li><p class="first">Switches into analytic OnePotentialSample case when appropriate</p>
</li>
<li><p class="first">Multithreaded over starting locations</p>
</li>
<li><p class="first">Optimizes with restarted gradient descent; collects results and updates the solution as new optima are found</p>
</li>
<li><p class="first">This calls:</p>
<p>MultistartOptimizer&lt;...&gt;::MultistartOptimize(...) for multistarting (see gpp_optimization.hpp) which in turn uses
GradientDescentOptimizer::Optimize&lt;ObjectiveFunctionEvaluator, Domain&gt;() (see gpp_optimization.hpp)</p>
</li>
</ul>
</li>
</ul>
<p>ComputeOptimalPointsToSampleViaLatinHypercubeSearch&lt;...&gt;(...)  (defined in gpp_math.hpp)</p>
<ul>
<li><p class="first">Estimates q,p-EI with a &#8216;dumb&#8217; search.</p>
</li>
<li><p class="first">Selects random starting locations based on random sampling from the domain (e.g., latin hypercube)</p>
</li>
<li><p class="first">This calls:</p>
<p>EvaluateEIAtPointList&lt;...&gt;(...)</p>
<ul>
<li><p class="first">Evaluates EI at each starting location</p>
</li>
<li><p class="first">Switches into analytic OnePotentialSample case when appropriate</p>
</li>
<li><p class="first">Multithreaded over starting locations</p>
</li>
<li><p class="first">This calls:</p>
<p>MultistartOptimizer&lt;...&gt;::MultistartOptimize(...) for multistarting (see gpp_optimization.hpp)</p>
</li>
</ul>
</li>
</ul>
<p>ComputeOptimalPointsToSample&lt;...&gt;(...)  (defined in gpp_math.cpp)</p>
<ul class="simple">
<li>Solves q,p-EI</li>
<li>Tries ComputeOptimalPointsToSampleWithRandomStarts() first.</li>
<li>If that fails, switches to ComputeOptimalPointsToSampleViaLatinHypercubeSearch().</li>
</ul>
<p>So finally we will overview the function calls for EI calculation.  We limit our discussion to the general MC case;
the analytic case is similar and simpler.
ExpectedImprovementEvaluator::ComputeExpectedImprovement()  (computes EI)</p>
<ul class="simple">
<li>Computes GP.mean, GP.variance, cholesky(GP.variance)</li>
<li>MC integration: samples from the GP repeatedly (Equation 4) and computes the improvement (Equation 5), averaging the result
See function comments for more details.</li>
<li>Calls out to GP::ComputeMeanOfPoints(), GP:ComputeVarianceOfPoints, ComputeCholeskyFactorL, NormalRNG::operator(),
and TriangularMatrixVectorMultiply</li>
</ul>
<p>ExpectedImprovementEvaluator::ComputeGradExpectedImprovement()  (computes gradient of EI)</p>
<ul class="simple">
<li>Compute GP.mean, variance, cholesky(variance), grad mean, grad variance, grad cholesky variance</li>
<li>MC integration: Equation 4, 5 as before to compute improvement each step
Only have grad EI contributions when improvement &gt; 0.
Care is needed because only the point yielding the largest improvement contributes to the gradient.
See function comments for more details.</li>
</ul>
<p>We will not detail the call tree once inside of GaussianProcess.  The mathematical formulas for the mean and variance
were already described above (Equation 2, 3).  Function docs (in this file) further detail/cite the formulas and
relevant derivations for gradients of these quantities.  Suffice to say there&#8217;s a lot of linear algebra.  Read on
(to those fcn docs) for further details but this does little to expose the important concepts behind EI and GP.</p>
 </p>
<em>Defines</em><blockquote>
<div><p><span class="target" id="project0gpp__math_8cpp_1acee0e2f7c2e99fe8c3ee4201e3d08939"></span><strong>OL_CHOL_VAR</strong>(i, j)</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0gpp__math_8cpp_1a766817843cda6ab78b324f06aa0c2faf"></span><strong>OL_GRAD_CHOL</strong>(m, i, j)</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Functions</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1a33a1adf04d13230ae3e8b8fd62247071"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line">void <strong>ComputeOptimalPointsToSample</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const DomainType &amp; domain, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool lhc_search_only, int num_lhc_samples, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>This is a simple wrapper around ComputeOptimalPointsToSampleWithRandomStarts() and
ComputeOptimalPointsToSampleViaLatinHypercubeSearch(). That is, this method attempts multistart gradient descent
and falls back to latin hypercube search if gradient descent fails (or is not desired).</p>
<p>TODO(GH-77): Instead of random search, we may want to fall back on the methods in
<tt class="docutils literal"><span class="pre">gpp_heuristic_expected_improvement_optimization.hpp</span></tt> if gradient descent fails; esp for larger q
(even <tt class="docutils literal"><span class="pre">q</span> <span class="pre">\approx</span> <span class="pre">4</span></tt>), latin hypercube search does a pretty terrible job.
This is more for general q,p-EI as these two things are equivalent for 1,0-EI.</p>
</p>
<p><p>Solve the q,p-EI problem (see header docs) by optimizing the Expected Improvement.
Uses multistart gradient descent, &#8220;dumb&#8221; search, and/or other heuristics to perform the optimization.</p>
<p>This is the primary entry-point for EI optimization in the optimal_learning library. It offers our best shot at
improving robustness by combining higher accuracy methods like gradient descent with fail-safes like random/grid search.</p>
<p>Returns the optimal set of q points to sample CONCURRENTLY by solving the q,p-EI problem.  That is, we may want to run 4
experiments at the same time and maximize the EI across all 4 experiments at once while knowing of 2 ongoing experiments
(4,2-EI). This function handles this use case. Evaluation of q,p-EI (and its gradient) for q &gt; 1 or p &gt; 1 is expensive
(requires monte-carlo iteration), so this method is usually very expensive.</p>
<p>Wraps ComputeOptimalPointsToSampleWithRandomStarts() and ComputeOptimalPointsToSampleViaLatinHypercubeSearch().</p>
<p>Compared to ComputeHeuristicPointsToSample() (<tt class="docutils literal"><span class="pre">gpp_heuristic_expected_improvement_optimization.hpp</span></tt>), this function
makes no external assumptions about the underlying objective function. Instead, it utilizes a feature of the
GaussianProcess that allows the GP to account for ongoing/incomplete experiments.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into multistart_expected_improvement_optimization() in cpp_wrappers/expected_improvement.py.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">values</span></tt>, <tt class="docutils literal"><span class="pre">noise_variance</span></tt>, derived quantities)
that describes the underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">optimization_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">GradientDescentParameters object that describes the parameters controlling EI optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see <tt class="docutils literal"><span class="pre">gpp_domain.hpp</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points_being_sampled[dim][num_being_sampled]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">points that are being sampled in concurrent experiments</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">how many simultaneous experiments you would like to run (i.e., the q in q,p-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently (i.e., the p in q,p-EI)</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be <tt class="docutils literal"><span class="pre">min(points_sampled_value)</span></tt>)</td>
</tr>
<tr class="field-even field"><th class="field-name">max_int_steps:</th><td class="field-body">maximum number of MC iterations</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">lhc_search_only:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">whether to ONLY use latin hypercube search (and skip gradient descent EI opt)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_lhc_samples:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of samples to draw if/when doing latin hypercube search</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a vector of NormalRNG objects that provide the (pesudo)random source for MC integration</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">found_flag[1]:</th><td class="field-body">true if best_points_to_sample corresponds to a nonzero EI if sampled simultaneously</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">normal_rng[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">NormalRNG objects will have their state changed due to random draws</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">best_points_to_sample[num_to_sample*dim]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">point yielding the best EI according to MGD</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1adee8cf0f068385ea3a55abfece46fe31"></span><div class="line-block">
<div class="line">template void <strong>ComputeOptimalPointsToSample</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const  <a class="reference internal" href="gpp_domain.html#project0classoptimal__learning_1_1_tensor_product_domain"><em>TensorProductDomain</em></a>  &amp; domain, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool lhc_search_only, int num_lhc_samples, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a148f4669804937b28c2449f913b3ee50"></span><div class="line-block">
<div class="line">template void <strong>ComputeOptimalPointsToSample</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const  <a class="reference internal" href="gpp_domain.html#project0classoptimal__learning_1_1_simplex_intersect_tensor_product_domain"><em>SimplexIntersectTensorProductDomain</em></a>  &amp; domain, double const *restrict points_being_sampled, int num_to_sample, int num_being_sampled, double best_so_far, int max_int_steps, int max_num_threads, bool lhc_search_only, int num_lhc_samples, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_normal_r_n_g"><em>NormalRNG</em></a>  * normal_rng, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpp_random_test.html" class="btn btn-neutral float-right" title="gpp_random_test"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpp_python_model_selection.html" class="btn btn-neutral" title="gpp_python_model_selection"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Yelp and Cornell Collaboration.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>