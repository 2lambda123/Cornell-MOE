<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood &mdash; MOE 0.1.0 documentation</title>
    
    <link rel="stylesheet" href="../../../../../../../_static/default.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../../../../../../_static/breathe.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../../../../../../',
        VERSION:     '0.1.0',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../../../../../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../../../../../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../../../../../../_static/doctools.js"></script>
    <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="shortcut icon" href="../../../../../../../_static/favicon.ico"/>
    <link rel="top" title="MOE 0.1.0 documentation" href="../../../../../../../index.html" />
    <link rel="up" title="moe" href="../../../../../../moe.html" /> 
  </head>
  <body>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../../../genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="../../../../../../../http-routingtable.html" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../../../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../../../../index.html">MOE 0.1.0 documentation</a> &raquo;</li>
          <li><a href="../../../../../../index.html" >Module code</a> &raquo;</li>
          <li><a href="../../../../../../moe.html" accesskey="U">moe</a> &raquo;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <h1>Source code for moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood</h1><div class="highlight"><pre>
<span class="c"># -*- coding: utf-8 -*-</span>
<span class="sd">r&quot;&quot;&quot;Tools to compute log likelihood-like measures of model fit and optimize them (wrt the hyperparameters of covariance) to select the best model for a given set of historical data.</span>

<span class="sd">See the file comments in interfaces/log_likelihood_interface.py for an overview of log likelihood-like metrics and their role</span>
<span class="sd">in model selection. This file provides hooks to implementations of two such metrics in C++: Log Marginal Likelihood and</span>
<span class="sd">Leave One Out Cross Validation Log Pseudo-Likelihood.</span>

<span class="sd">.. Note: This is a copy of the file comments in gpp_model_selection_and_hyperparameter_optimization.hpp.</span>
<span class="sd">  See this file&#39;s comments and interfaces.log_likelihood_interface for more details as well as the hpp and corresponding .cpp file.</span>

<span class="sd">a) LOG MARGINAL LIKELIHOOD (LML):</span>
<span class="sd">(Rasmussen &amp; Williams, 5.4.1)</span>
<span class="sd">The Log Marginal Likelihood measure comes from the ideas of Bayesian model selection, which use Bayesian inference</span>
<span class="sd">to predict distributions over models and their parameters.  The cpp file comments explore this idea in more depth.</span>
<span class="sd">For now, we will simply state the relevant result.  We can build up the notion of the &quot;marginal likelihood&quot;:</span>
<span class="sd">probability(observed data GIVEN sampling points (``X``), model hyperparameters, model class (regression, GP, etc.)),</span>
<span class="sd">which is denoted: ``p(y|X,\theta,H_i)`` (see the cpp file comments for more).</span>

<span class="sd">So the marginal likelihood deals with computing the probability that the observed data was generated from (another</span>
<span class="sd">way: is easily explainable by) the given model.</span>

<span class="sd">The marginal likelihood is in part paramaterized by the model&#39;s hyperparameters; e.g., as mentioned above.  Thus</span>
<span class="sd">we can search for the set of hyperparameters that produces the best marginal likelihood and use them in our model.</span>
<span class="sd">Additionally, a nice property of the marginal likelihood optimization is that it automatically trades off between</span>
<span class="sd">model complexity and data fit, producing a model that is reasonably simple while still explaining the data reasonably</span>
<span class="sd">well.  See the cpp file comments for more discussion of how/why this works.</span>

<span class="sd">In general, we do not want a model with perfect fit and high complexity, since this implies overfit to input noise.</span>
<span class="sd">We also do not want a model with very low complexity and poor data fit: here we are washing the signal out with</span>
<span class="sd">(assumed) noise, so the model is simple but it provides no insight on the data.</span>

<span class="sd">This is not magic.  Using GPs as an example, if the covariance function is completely mis-specified, we can blindly</span>
<span class="sd">go through with marginal likelihood optimization, obtain an &quot;optimal&quot; set of hyperparameters, and proceed... never</span>
<span class="sd">realizing that our fundamental assumptions are wrong.  So care is always needed.</span>

<span class="sd">b) LEAVE ONE OUT CROSS VALIDATION (LOO-CV):</span>
<span class="sd">(Rasmussen &amp; Williams, Chp 5.4.2)</span>
<span class="sd">In cross validation, we split the training data, X, into two sets--a sub-training set and a validation set.  Then we</span>
<span class="sd">train a model on the sub-training set and test it on the validation set.  Since the validation set comes from the</span>
<span class="sd">original training data, we can compute the error.  In effect we are examining how well the model explains itself.</span>

<span class="sd">Leave One Out CV works by considering n different validation sets, one at a time.  Each point of X takes a turn</span>
<span class="sd">being the sole member of the validation set.  Then for each validation set, we compute a log pseudo-likelihood, measuring</span>
<span class="sd">how probable that validation set is given the remaining training data and model hyperparameters.</span>

<span class="sd">Again, we can maximize this quanitity over hyperparameters to help us choose the &quot;right&quot; set for the GP.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">copy</span>

<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">import</span> <span class="nn">moe.build.GPP</span> <span class="kn">as</span> <span class="nn">C_GP</span>
<span class="kn">import</span> <span class="nn">moe.optimal_learning.EPI.src.python.cpp_wrappers.cpp_utils</span> <span class="kn">as</span> <span class="nn">cpp_utils</span>
<span class="kn">import</span> <span class="nn">moe.optimal_learning.EPI.src.python.geometry_utils</span> <span class="kn">as</span> <span class="nn">geometry_utils</span>
<span class="kn">from</span> <span class="nn">moe.optimal_learning.EPI.src.python.interfaces.log_likelihood_interface</span> <span class="kn">import</span> <span class="n">GaussianProcessLogLikelihoodInterface</span>
<span class="kn">from</span> <span class="nn">moe.optimal_learning.EPI.src.python.interfaces.optimization_interface</span> <span class="kn">import</span> <span class="n">OptimizableInterface</span>


<div class="viewcode-block" id="multistart_hyperparameter_optimization"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.multistart_hyperparameter_optimization">[docs]</a><span class="k">def</span> <span class="nf">multistart_hyperparameter_optimization</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="p">,</span> <span class="n">hyperparameter_optimization_parameters</span><span class="p">,</span> <span class="n">hyperparameter_domain</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">randomness</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">max_num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">status</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">r&quot;&quot;&quot;Select the hyperparameters that maximize the specified log likelihood measure of model fit (over the historical data) within the specified domain.</span>

<span class="sd">    See GaussianProcessLogMarginalLikelihood and GaussianProcessLeaveOneOutLogLikelihood for an overview of some</span>
<span class="sd">    example log likelihood-like measures.</span>

<span class="sd">    Optimizers are: null (&#39;dumb&#39; search), gradient descent, newton</span>
<span class="sd">    Newton is the suggested optimizer.</span>

<span class="sd">    &#39;dumb&#39; search means this will just evaluate the objective log likelihood measure at num_multistarts &#39;points&#39;</span>
<span class="sd">    (hyperparameters) in the domain, uniformly sampled using latin hypercube sampling.</span>
<span class="sd">    The hyperparameter_optimization_parameters input specifies the desired optimization technique as well as parameters controlling</span>
<span class="sd">    its behavior (see cpp_wrappers.optimization_parameters.py).</span>

<span class="sd">    See gpp_python_common.cpp for C++ enum declarations laying out the options for objective and optimizer types.</span>

<span class="sd">    Currently, during optimization, we recommend that the coordinates of the initial guesses not differ from the</span>
<span class="sd">    coordinates of the optima by more than about 1 order of magnitude. This is a very (VERY!) rough guideline for</span>
<span class="sd">    sizing the domain and gd_parameters.num_multistarts; i.e., be wary of sets of initial guesses that cover the space too sparsely.</span>

<span class="sd">    Note that the domain here must be specified in LOG-10 SPACE!</span>

<span class="sd">    Solution is guaranteed to lie within the region specified by &quot;domain&quot;; note that this may not be a</span>
<span class="sd">    true optima (i.e., the gradient may be substantially nonzero).</span>

<span class="sd">    .. WARNING:: this function fails if NO improvement can be found!  In that case,</span>
<span class="sd">       the output will always be the first randomly chosen point. status will report failure.</span>

<span class="sd">    :param log_likelihood_evaluator: object specifying which log likelihood measure to optimize</span>
<span class="sd">    :type log_likelihood_evaluator: cpp_wrappers.log_likelihood.LogLikelihood</span>
<span class="sd">    :param hyperparameter_optimization_parameters: object specifying the desired optimization method and parameters controlling its behavior (e.g., tolerance, iterations, etc.)</span>
<span class="sd">    :type hyperparameter_optimization_parameters: cpp_wrappers.optimization_parameters.HyperparameterOptimizationParameters</span>
<span class="sd">    :param hyperparameter_domain: log10-space [min, max] bounds for each hyperparameter (e.g., [-2, 1] means the hyperparameter is restricted to [0.01, 10])</span>
<span class="sd">    :type hyperparameter_domain: iterable of num_hyperparameters ClosedInterval</span>
<span class="sd">    :param randomness: RNGs used by C++ to generate initial guesses</span>
<span class="sd">    :type randomness: RandomnessSourceContainer (C++ object; e.g., from C_GP.RandomnessSourceContainer())</span>
<span class="sd">    :param max_num_threads: maximum number of threads to use, &gt;= 1</span>
<span class="sd">    :type max_num_threads: int</span>
<span class="sd">    :param status: status messages from C++ (e.g., reporting on optimizer success, etc.)</span>
<span class="sd">    :type status: dict</span>
<span class="sd">    :return: hyperparameters that maximize the specified log likelihood measure within the specified domain</span>
<span class="sd">    :rtype: array of float64 with shape (log_likelihood_evaluator.num_hyperparameters)</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># Guess &quot;reasonable&quot; hyperparameter constraints if none are given.</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">hyperparameter_domain</span><span class="p">:</span>
        <span class="n">hyperparameter_domain</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">num_hyperparameters</span><span class="p">):</span>
            <span class="n">hyperparameter_domain</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">geometry_utils</span><span class="o">.</span><span class="n">ClosedInterval</span><span class="p">(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">))</span>

    <span class="c"># Create enough randomness sources if none are specified.</span>
    <span class="k">if</span> <span class="n">randomness</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">randomness</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">RandomnessSourceContainer</span><span class="p">(</span><span class="n">max_num_threads</span><span class="p">)</span>  <span class="c"># create randomness for max_num_threads</span>
        <span class="n">randomness</span><span class="o">.</span><span class="n">SetRandomizedUniformGeneratorSeed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># set seed based on less repeatable factors (e.g,. time)</span>
        <span class="n">randomness</span><span class="o">.</span><span class="n">SetRandomizedNormalRNGSeed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c"># set seed baesd on thread id &amp; less repeatable factors (e.g,. time)</span>

    <span class="c"># status must be an initialized dict for the call to C++.</span>
    <span class="k">if</span> <span class="n">status</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">status</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="n">hyperparameter_optimization_parameters</span><span class="o">.</span><span class="n">objective_type</span> <span class="o">=</span> <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_log_likelihood_type</span>
    <span class="n">hyperparameters_opt</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">multistart_hyperparameter_optimization</span><span class="p">(</span>
        <span class="n">hyperparameter_optimization_parameters</span><span class="p">,</span>  <span class="c"># HyperparameterOptimizationParameters object (see cpp_wrappers.optimization_parameters)</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">hyperparameter_domain</span><span class="p">),</span>  <span class="c"># domain of hyperparameters in LOG-10 SPACE</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled</span><span class="p">),</span>  <span class="c"># points already sampled</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_value</span><span class="p">),</span>  <span class="c"># objective value at each sampled point</span>
        <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
        <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify_hyperparameters</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">get_hyperparameters</span><span class="p">()),</span>  <span class="c"># hyperparameters, e.g., [signal variance, [length scales]]; see cppify_hyperparameter docs, C++ python interface docs</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_noise_variance</span><span class="p">),</span>  <span class="c"># noise variance, one value per sampled point</span>
        <span class="n">max_num_threads</span><span class="p">,</span>
        <span class="n">randomness</span><span class="p">,</span>  <span class="c"># C++ RandomnessSourceContainer that holds a UniformRandomGenerator object</span>
        <span class="n">status</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">hyperparameters_opt</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="evaluate_log_likelihood_at_hyperparameter_list"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.evaluate_log_likelihood_at_hyperparameter_list">[docs]</a><span class="k">def</span> <span class="nf">evaluate_log_likelihood_at_hyperparameter_list</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="p">,</span> <span class="n">hyperparameters_to_evaluate</span><span class="p">,</span> <span class="n">max_num_threads</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute the specified log likelihood measure at each input set of hyperparameters.</span>

<span class="sd">    Generally gradient descent is preferred but when they fail to converge this may be the only &quot;robust&quot; option.</span>
<span class="sd">    This function is also useful for plotting or debugging purposes (just to get a bunch of log likelihood values).</span>

<span class="sd">    Calls into evaluate_log_likelihood_at_hyperparameter_list() in src/cpp/GPP_python_model_selection.cpp.</span>

<span class="sd">    :param log_likelihood_evaluator: object specifying which log likelihood measure to evaluate</span>
<span class="sd">    :type log_likelihood_evaluator: cpp_wrappers.log_likelihood.LogLikelihood</span>
<span class="sd">    :param hyperparameters_to_evaluate: the hyperparameters at which to compute the specified log likelihood</span>
<span class="sd">    :type hyperparameters_to_evaluate: array of float64 with shape (num_to_eval, log_likelihood_evaluator.num_hyperparameters)</span>
<span class="sd">    :param max_num_threads: maximum number of threads to use, &gt;= 1</span>
<span class="sd">    :type max_num_threads: int</span>
<span class="sd">    :return: log likelihood value at each specified set of hyperparameters</span>
<span class="sd">    :rtype: array of float64 with shape (hyperparameters_to_evaluate.shape[0])</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c"># We could just call log_likelihood_evaluator.compute_log_likelihood() in a loop, but instead we do</span>
    <span class="c"># the looping in C++ where it can be multithreaded.</span>
    <span class="n">log_likelihood_list</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">evaluate_log_likelihood_at_hyperparameter_list</span><span class="p">(</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">hyperparameters_to_evaluate</span><span class="p">),</span>  <span class="c"># hyperparameters at which to compute log likelihood</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled</span><span class="p">),</span>  <span class="c"># points already sampled</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_value</span><span class="p">),</span>  <span class="c"># objective value at each sampled point</span>
        <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
        <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
        <span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_log_likelihood_type</span><span class="p">,</span>  <span class="c"># log likelihood measure to eval (e.g., LogLikelihoodTypes.log_marginal_likelihood, see gpp_python_common.cpp for enum declaration)</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify_hyperparameters</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">get_hyperparameters</span><span class="p">()),</span>  <span class="c"># hyperparameters, e.g., [signal variance, [length scales]]; see cppify_hyperparameter docs, C++ python interface docs</span>
        <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="n">log_likelihood_evaluator</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_noise_variance</span><span class="p">),</span>  <span class="c"># noise variance, one value per sampled point</span>
        <span class="n">hyperparameters_to_evaluate</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>  <span class="c"># number of hyperparameter points to evaluate</span>
        <span class="n">max_num_threads</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">log_likelihood_list</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood">[docs]</a><span class="k">class</span> <span class="nc">GaussianProcessLogLikelihood</span><span class="p">(</span><span class="n">GaussianProcessLogLikelihoodInterface</span><span class="p">,</span> <span class="n">OptimizableInterface</span><span class="p">):</span>

    <span class="sd">r&quot;&quot;&quot;Class for computing log likelihood-like measures of model fit via C++ wrappers (currently log marginal and leave one out cross validation).</span>

<span class="sd">    See GaussianProcessLogMarginalLikelihood and GaussianProcessLeaveOneOutLogLikelihood classes below for some more</span>
<span class="sd">    details on these metrics. Users may find it more convenient to construct these objects instead of a LogLikelihood</span>
<span class="sd">    object directly. Since these various metrics are fairly different, the member function docs in this class will</span>
<span class="sd">    remain generic.</span>

<span class="sd">    See gpp_model_selection_and_hyperparameter_optimization.hpp/cpp for further overview and in-depth discussion, respectively.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">covariance_function</span><span class="p">,</span> <span class="n">historical_data</span><span class="p">,</span> <span class="n">log_likelihood_type</span><span class="o">=</span><span class="n">C_GP</span><span class="o">.</span><span class="n">LogLikelihoodTypes</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a LogLikelihood object that knows how to call C++ for evaluation of member functions.</span>

<span class="sd">        :param covariance_function: covariance object encoding assumptions about the GP&#39;s behavior on our data</span>
<span class="sd">        :type covariance_function: Covariance object exposing hyperparameters (e.g., from cpp_wrappers.covariance)</span>
<span class="sd">        :param historical_data: object specifying the already-sampled points, the objective value at those points, and the noise variance associated with each observation</span>
<span class="sd">        :type historical_data: HistoricalData object</span>
<span class="sd">        :param log_likelihood_type: enum specifying which log likelihood measure to compute</span>
<span class="sd">        :type log_likelihood_type: GPP.LogLikelihoodTypes</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">covariance_function</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span> <span class="o">=</span> <span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">historical_data</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">_log_likelihood_type</span> <span class="o">=</span> <span class="n">log_likelihood_type</span>

    <span class="nd">@property</span>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.dim"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.dim">[docs]</a>    <span class="k">def</span> <span class="nf">dim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of spatial dimensions.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">dim</span>
</div>
    <span class="nd">@property</span>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.num_hyperparameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.num_hyperparameters">[docs]</a>    <span class="k">def</span> <span class="nf">num_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of hyperparameters.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">num_hyperparameters</span>
</div>
    <span class="nd">@property</span>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.problem_size"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.problem_size">[docs]</a>    <span class="k">def</span> <span class="nf">problem_size</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return the number of independent parameters to optimize.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_hyperparameters</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.get_hyperparameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.get_hyperparameters">[docs]</a>    <span class="k">def</span> <span class="nf">get_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the hyperparameters (array of float64 with shape (num_hyperparameters)) of this covariance.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">get_hyperparameters</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.set_hyperparameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.set_hyperparameters">[docs]</a>    <span class="k">def</span> <span class="nf">set_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set hyperparameters to the specified hyperparameters; ordering must match.&quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">hyperparameters</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.get_current_point"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.get_current_point">[docs]</a>    <span class="k">def</span> <span class="nf">get_current_point</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Get the current_point (array of float64 with shape (problem_size)) at which this object is evaluating the objective function, ``f(x)``.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_hyperparameters</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.set_current_point"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.set_current_point">[docs]</a>    <span class="k">def</span> <span class="nf">set_current_point</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">current_point</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Set current_point to the specified point; ordering must match.</span>

<span class="sd">        :param current_point: current_point at which to evaluate the objective function, ``f(x)``</span>
<span class="sd">        :type current_point: array of float64 with shape (problem_size)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">current_point</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_log_likelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">compute_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">r&quot;&quot;&quot;Compute the _log_likelihood_type measure at the specified hyperparameters.</span>

<span class="sd">        :return: value of log_likelihood evaluated at hyperparameters (``LL(y | X, \theta)``)</span>
<span class="sd">        :rtype: float64</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">compute_log_likelihood</span><span class="p">(</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled</span><span class="p">),</span>  <span class="c"># points already sampled</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_value</span><span class="p">),</span>  <span class="c"># objective value at each sampled point</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_likelihood_type</span><span class="p">,</span>  <span class="c"># log likelihood measure to eval (e.g., LogLikelihoodTypes.log_marginal_likelihood, see gpp_python_common.cpp for enum declaration)</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">_hyperparameters</span><span class="p">),</span>  <span class="c"># hyperparameters, e.g., [signal variance, [length scales]]; see _cppify_hyperparameter docs, C++ python interface docs</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_noise_variance</span><span class="p">),</span>  <span class="c"># noise variance, one value per sampled point</span>
        <span class="p">)</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_objective_function"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_objective_function">[docs]</a>    <span class="k">def</span> <span class="nf">compute_objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper for compute_log_likelihood; see that function&#39;s docstring.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_log_likelihood</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_grad_log_likelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_grad_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">compute_grad_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">r&quot;&quot;&quot;Compute the gradient (wrt hyperparameters) of the _log_likelihood_type measure at the specified hyperparameters.</span>

<span class="sd">        :return: grad_log_likelihood: i-th entry is ``\pderiv{LL(y | X, \theta)}{\theta_i}``</span>
<span class="sd">        :rtype: array of float64 with shape (num_hyperparameters)</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">grad_log_marginal</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">compute_hyperparameter_grad_log_likelihood</span><span class="p">(</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled</span><span class="p">),</span>  <span class="c"># points already sampled</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_value</span><span class="p">),</span>  <span class="c"># objective value at each sampled point</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dim</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">num_sampled</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_log_likelihood_type</span><span class="p">,</span>  <span class="c"># log likelihood measure to eval (e.g., LogLikelihoodTypes.log_marginal_likelihood, see gpp_python_common.cpp for enum declaration)</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify_hyperparameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_covariance</span><span class="o">.</span><span class="n">_hyperparameters</span><span class="p">),</span>  <span class="c"># hyperparameters, e.g., [signal variance, [length scales]]; see _cppify_hyperparameter docs, C++ python interface docs</span>
            <span class="n">cpp_utils</span><span class="o">.</span><span class="n">cppify</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">_historical_data</span><span class="o">.</span><span class="n">points_sampled_noise_variance</span><span class="p">),</span>  <span class="c"># noise variance, one value per sampled point</span>
        <span class="p">)</span>
        <span class="k">return</span> <span class="n">numpy</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">grad_log_marginal</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_grad_objective_function"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_grad_objective_function">[docs]</a>    <span class="k">def</span> <span class="nf">compute_grad_objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper for compute_grad_log_likelihood; see that function&#39;s docstring.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_grad_log_likelihood</span><span class="p">()</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_hessian_log_likelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_hessian_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">compute_hessian_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;We do not currently support computation of the (hyperparameter) hessian of log likelihood-like metrics.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&#39;Currently C++ does not expose Hessian computation of log likelihood-like metrics.&#39;</span><span class="p">)</span>
</div>
<div class="viewcode-block" id="GaussianProcessLogLikelihood.compute_hessian_objective_function"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogLikelihood.compute_hessian_objective_function">[docs]</a>    <span class="k">def</span> <span class="nf">compute_hessian_objective_function</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Wrapper for compute_hessian_log_likelihood; see that function&#39;s docstring.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_hessian_log_likelihood</span><span class="p">()</span>

</div></div>
<div class="viewcode-block" id="GaussianProcessLogMarginalLikelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLogMarginalLikelihood">[docs]</a><span class="k">class</span> <span class="nc">GaussianProcessLogMarginalLikelihood</span><span class="p">(</span><span class="n">GaussianProcessLogLikelihood</span><span class="p">):</span>

    <span class="sd">r&quot;&quot;&quot;Class for computing the Log Marginal Likelihood, ``log(p(y | X, \theta))``.</span>

<span class="sd">    That is, the probability of observing the training values, y, given the training points, X,</span>
<span class="sd">    and hyperparameters (of the covariance function), ``\theta``.</span>

<span class="sd">    This is a measure of how likely it is that the observed values came from our Gaussian Process Prior.</span>

<span class="sd">    .. Note: This is a copy of LogMarginalLikelihoodEvaluator&#39;s class comments in gpp_model_selection_and_hyperparameter_optimization.hpp.</span>
<span class="sd">      See this file&#39;s comments and interfaces.log_likelihood_interface for more details as well as the hpp and corresponding .cpp file.</span>

<span class="sd">    Given a particular covariance function (including hyperparameters) and</span>
<span class="sd">    training data ((point, function value, measurement noise) tuples), the log marginal likelihood is the log probability that</span>
<span class="sd">    the data were observed from a Gaussian Process would have generated the observed function values at the given measurement</span>
<span class="sd">    points.  So log marginal likelihood tells us &quot;the probability of the observations given the assumptions of the model.&quot;</span>
<span class="sd">    Log marginal sits well with the Bayesian Inference camp.</span>
<span class="sd">    (Rasmussen &amp; Williams p118)</span>

<span class="sd">    This quantity primarily deals with the trade-off between model fit and model complexity.  Handling this trade-off is automatic</span>
<span class="sd">    in the log marginal likelihood calculation.  See Rasmussen &amp; Williams 5.2 and 5.4.1 for more details.</span>

<span class="sd">    We can use the log marginal likelihood to determine how good our model is.  Additionally, we can maximize it by varying</span>
<span class="sd">    hyperparameters (or even changing covariance functions) to improve our model quality.  Hence this class provides access</span>
<span class="sd">    to functions for computing log marginal likelihood and its hyperparameter gradients.</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">covariance_function</span><span class="p">,</span> <span class="n">historical_data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a LogLikelihood object configured for Log Marginal Likelihood computation; see superclass ctor for details.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessLogMarginalLikelihood</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">covariance_function</span><span class="p">,</span> <span class="n">historical_data</span><span class="p">,</span> <span class="n">log_likelihood_type</span><span class="o">=</span><span class="n">C_GP</span><span class="o">.</span><span class="n">LogLikelihoodTypes</span><span class="o">.</span><span class="n">log_marginal_likelihood</span><span class="p">)</span>

</div>
<div class="viewcode-block" id="GaussianProcessLeaveOneOutLogLikelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLeaveOneOutLogLikelihood">[docs]</a><span class="k">class</span> <span class="nc">GaussianProcessLeaveOneOutLogLikelihood</span><span class="p">(</span><span class="n">GaussianProcessLogLikelihood</span><span class="p">):</span>

    <span class="sd">r&quot;&quot;&quot;Class for computing the Leave-One-Out Cross Validation (LOO-CV) Log Pseudo-Likelihood.</span>

<span class="sd">    Given a particular covariance function (including hyperparameters) and training data ((point, function value, measurement noise)</span>
<span class="sd">    tuples), the log LOO-CV pseudo-likelihood expresses how well the model explains itself.</span>

<span class="sd">    .. Note: This is a copy of LeaveOneOutLogLikelihoodEvaluator&#39;s class comments in gpp_model_selection_and_hyperparameter_optimization.hpp.</span>
<span class="sd">      See this file&#39;s comments and interfaces.log_likelihood_interface for more details as well as the hpp and corresponding .cpp file.</span>

<span class="sd">    That is, cross validation involves splitting the training set into a sub-training set and a validation set.  Then we measure</span>
<span class="sd">    the log likelihood that a model built on the sub-training set could produce the values in the validation set.</span>

<span class="sd">    Leave-One-Out CV does this process ``|y|`` times: on the i-th run, the sub-training set is (X,y) with the i-th point removed</span>
<span class="sd">    and the validation set is the i-th point.  Then the predictive performance of each sub-model are aggregated into a</span>
<span class="sd">    psuedo-likelihood.</span>

<span class="sd">    This quantity primarily deals with the internal consistency of the model--how well it explains itself.  The LOO-CV</span>
<span class="sd">    likelihood gives an &quot;estimate for the predictive probability, whether or not the assumptions of the model may be</span>
<span class="sd">    fulfilled.&quot; It is a more frequentist view of model selection. (Rasmussen &amp; Williams p118)</span>
<span class="sd">    See Rasmussen &amp; Williams 5.3 and 5.4.2 for more details.</span>

<span class="sd">    As with the log marginal likelihood, we can use this quantity to measure the performance of our model.  We can also</span>
<span class="sd">    maximize it (via hyperparameter modifications or covariance function changes) to improve model performance.</span>
<span class="sd">    It has also been argued that LOO-CV is better at detecting model mis-specification (e.g., wrong covariance function)</span>
<span class="sd">    than log marginal measures (Rasmussen &amp; Williams p118).</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">covariance_function</span><span class="p">,</span> <span class="n">historical_data</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a LogLikelihood object configured for Leave One Out Cross Validation Log Pseudo-Likelihood computation; see superclass ctor for details.&quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GaussianProcessLeaveOneOutLogLikelihood</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span><span class="n">covariance_function</span><span class="p">,</span> <span class="n">historical_data</span><span class="p">,</span> <span class="n">log_likelihood_type</span><span class="o">=</span><span class="n">C_GP</span><span class="o">.</span><span class="n">LogLikelihoodTypes</span><span class="o">.</span><span class="n">leave_one_out_log_likelihood</span><span class="p">)</span>

<div class="viewcode-block" id="GaussianProcessLeaveOneOutLogLikelihood.compute_hessian_log_likelihood"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.log_likelihood.GaussianProcessLeaveOneOutLogLikelihood.compute_hessian_log_likelihood">[docs]</a>    <span class="k">def</span> <span class="nf">compute_hessian_log_likelihood</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hyperparameters</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;The (hyperparameter) hessian of LOO-CV has not been implemented in C++ yet.&quot;&quot;&quot;</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="s">&#39;Currently C++ does not support Hessian computation of LOO-CV.&#39;</span><span class="p">)</span></div></div>
</pre></div>

          </div>
        </div>
      </div>
      <div class="sphinxsidebar">
        <div class="sphinxsidebarwrapper">
<div id="searchbox" style="display: none">
  <h3>Quick search</h3>
    <form class="search" action="../../../../../../../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    <p class="searchtip" style="font-size: 90%">
    Enter search terms or a module, class or function name.
    </p>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../../../../../../../genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="../../../../../../../http-routingtable.html" title="HTTP Routing Table"
             >routing table</a> |</li>
        <li class="right" >
          <a href="../../../../../../../py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li><a href="../../../../../../../index.html">MOE 0.1.0 documentation</a> &raquo;</li>
          <li><a href="../../../../../../index.html" >Module code</a> &raquo;</li>
          <li><a href="../../../../../../moe.html" >moe</a> &raquo;</li> 
      </ul>
    </div>
    <div class="footer">
        &copy; Copyright 2014, Yelp and Cornell Collaboration.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.2.2.
    </div>
  </body>
</html>