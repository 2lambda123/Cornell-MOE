

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters &mdash; MOE 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../../../../../../../_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="../../../../../../../_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="../../../../../../../_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.1.0 documentation" href="../../../../../../../index.html"/>
        <link rel="up" title="moe" href="../../../../../../moe.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="../../../../../../../index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../../../../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../moe.html">moe package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../../../../../../moe.html#module-moe">Module contents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../gpp_math_hpp.html">gpp_math.hpp</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../../../../../gpp_geometry_hpp.html">gpp_geometry.hpp</a></li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../../../../../../../index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../../../../../../../index.html">Docs</a> &raquo;</li>
      
          <li><a href="../../../../../../index.html">Module code</a> &raquo;</li>
      
          <li><a href="../../../../../../moe.html">moe</a> &raquo;</li>
      
    <li>moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters</li>
      <li class="wy-breadcrumbs-aside">
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <h1>Source code for moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters</h1><div class="highlight"><pre>
<span class="c"># -*- coding: utf-8 -*-</span>
<span class="sd">r&quot;&quot;&quot;Classes (and utilities for constructing them) that specify the behavior of C++ optimization routines like Gradient Descent and Newton.</span>

<span class="sd">C++ expects input objects to have a certain format; the classes in this file make it convenient to put data into the expected</span>
<span class="sd">format. Generally the C++ optimizers want to know the objective function (what), optimization method (how), domain (where, etc.</span>
<span class="sd">along with paramters like number of iterations, tolerances, etc.</span>

<span class="sd">These Python classes/functions wrap the C++ structs in: gpp_optimization_parameters.hpp.</span>

<span class="sd">The \*OptimizationParameters structs contain the high level details--what to optimize, how to do it, etc. explicitly. And the hold</span>
<span class="sd">a reference to a C++ struct containing parameters for the specific optimizer. The build_*_parameters() helper functions provide</span>
<span class="sd">wrappers around these C++ objects&#39; constructors.</span>

<span class="sd">&quot;&quot;&quot;</span>
<span class="kn">import</span> <span class="nn">numpy</span>

<span class="kn">import</span> <span class="nn">moe.build.GPP</span> <span class="kn">as</span> <span class="nn">C_GP</span>


<div class="viewcode-block" id="build_newton_parameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters.build_newton_parameters">[docs]</a><span class="k">def</span> <span class="nf">build_newton_parameters</span><span class="p">(</span><span class="n">num_multistarts</span><span class="p">,</span> <span class="n">max_num_steps</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">time_factor</span><span class="p">,</span> <span class="n">max_relative_change</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="sd">r&quot;&quot;&quot;Build a NewtonParameters (C++ object) via its ctor; this object specifies multistarted Newton behavior and is required by C++ Newton optimization.</span>

<span class="sd">    .. Note:: See gpp_optimization_parameters.hpp for more details.</span>
<span class="sd">       The following comments are copied from NewtonParameters struct in gpp_optimization_parameters.hpp.</span>

<span class="sd">    Diagonal dominance control: gamma and time_factor:</span>
<span class="sd">    On i-th newton iteration, we add ``1/(time_factor*gamma^(i+1)) * I`` to the Hessian to improve robustness</span>

<span class="sd">    Choosing a small gamma (e.g., ``1.0 &lt; gamma &lt;= 1.01``) and time_factor (e.g., ``0 &lt; time_factor &lt;= 1.0e-3``)</span>
<span class="sd">    leads to more consistent/stable convergence at the cost of slower performance (and in fact</span>
<span class="sd">    for gamma or time_factor too small, gradient descent is preferred).  Conversely, choosing more</span>
<span class="sd">    aggressive values may lead to very fast convergence at the cost of more cases failing to</span>
<span class="sd">    converge.</span>

<span class="sd">    ``gamma = 1.01``, ``time_factor = 1.0e-3`` should lead to good robustness at reasonable speed.  This should be a fairly safe default.</span>
<span class="sd">    ``gamma = 1.05, time_factor = 1.0e-1`` will be several times faster but not as robust.</span>
<span class="sd">    for &quot;easy&quot; problems, these can be much more aggressive, e.g., ``gamma = 2.0``, ``time_factor = 1.0e1`` or more</span>

<span class="sd">    :param num_multistarts: number of initial guesses to try in multistarted newton</span>
<span class="sd">    :type num_multistarts: int &gt; 0</span>
<span class="sd">    :param max_num_steps: maximum number of newton iterations (per initial guess)</span>
<span class="sd">    :type max_num_steps: int &gt; 0</span>
<span class="sd">    :param gamma: exponent controlling rate of time_factor growth (see function comments)</span>
<span class="sd">    :type gamma: float64 &gt; 1.0</span>
<span class="sd">    :param time_factor: initial amount of additive diagonal dominance (see function comments)</span>
<span class="sd">    :type time_factor: float64 &gt; 0.0</span>
<span class="sd">    :param max_relative_change: max change allowed per update (as a relative fraction of current distance to wall)</span>
<span class="sd">    :type max_relative_change: float64 in [0, 1]</span>
<span class="sd">    :param tolerance: when the magnitude of the gradient falls below this value, stop</span>
<span class="sd">    :type tolerance: float64 &gt;= 0.0</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">newton_data</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">NewtonParameters</span><span class="p">(</span>
        <span class="n">num_multistarts</span><span class="p">,</span>
        <span class="n">max_num_steps</span><span class="p">,</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">gamma</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">time_factor</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">max_relative_change</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">tolerance</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">newton_data</span>

</div>
<div class="viewcode-block" id="build_gradient_descent_parameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters.build_gradient_descent_parameters">[docs]</a><span class="k">def</span> <span class="nf">build_gradient_descent_parameters</span><span class="p">(</span><span class="n">num_multistarts</span><span class="p">,</span> <span class="n">max_num_steps</span><span class="p">,</span> <span class="n">max_num_restarts</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">pre_mult</span><span class="p">,</span> <span class="n">max_relative_change</span><span class="p">,</span> <span class="n">tolerance</span><span class="p">):</span>
    <span class="sd">r&quot;&quot;&quot;Build a GradientDescentParameters (C++ object) via its ctor; this object specifies multistarted GD behavior and is required by C++ GD optimization.</span>

<span class="sd">    .. Note:: See gpp_optimization_parameters.hpp for more details.</span>
<span class="sd">       The following comments are copied from GradientDescentParameters struct in gpp_optimization_parameters.hpp.</span>

<span class="sd">    Iterations:</span>
<span class="sd">    The total number of gradient descent steps is at most ``num_multistarts * max_num_steps * max_num_restarts``</span>
<span class="sd">    Generally, allowing more iterations leads to a better solution but costs more time.</span>

<span class="sd">    Learning Rate:</span>
<span class="sd">    GD may be implemented using a learning rate: ``pre_mult * (i+1)^{-\gamma}``, where i is the current iteration</span>
<span class="sd">    Larger gamma causes the GD step size to (artificially) scale down faster.</span>
<span class="sd">    Smaller pre_mult (artificially) shrinks the GD step size.</span>
<span class="sd">    Generally, taking a very large number of small steps leads to the most robustness; but it is very slow.</span>

<span class="sd">    Tolerances:</span>
<span class="sd">    Larger relative changes are potentially less robust but lead to faster convergence.</span>
<span class="sd">    Large tolerances run faster but may lead to high errors or false convergence (e.g., if the tolerance is 1.0e-3 and the learning</span>
<span class="sd">    rate control forces steps to fall below 1.0e-3 quickly, then GD will quit &quot;successfully&quot; without genuinely converging.)</span>

<span class="sd">    :param num_multistarts: number of initial guesses to try in multistarted gradient descent (suggest: a few hundred)</span>
<span class="sd">    :type num_multistarts: int &gt; 0</span>
<span class="sd">    :param max_num_steps: maximum number of gradient descent iterations per restart (suggest: 200-1000)</span>
<span class="sd">    :type max_num_steps: int &gt; 0</span>
<span class="sd">    :param max_num_restarts: maximum number of gradient descent restarts, the we are allowed to call gradient descent.  Should be &gt;= 2 as a minimum (suggest: 10-20)</span>
<span class="sd">    :type max_num_restarts: int &gt; 0</span>
<span class="sd">    :param gamma: exponent controlling rate of step size decrease (see struct docs or GradientDescentOptimizer) (suggest: 0.5-0.9)</span>
<span class="sd">    :type gamma: double &gt; 1.0</span>
<span class="sd">    :param pre_mult: scaling factor for step size (see struct docs or GradientDescentOptimizer) (suggest: 0.1-1.0)</span>
<span class="sd">    :type pre_mult: double &gt; 0.0</span>
<span class="sd">    :param max_relative_change: max change allowed per GD iteration (as a relative fraction of current distance to wall)</span>
<span class="sd">           (suggest: 0.5-1.0 for less sensitive problems like EI; 0.02 for more sensitive problems like hyperparameter opt)</span>
<span class="sd">    :type max_relative_change: float64 in [0, 1]</span>
<span class="sd">    :param tolerance: when the magnitude of the gradient falls below this value OR we will not move farther than tolerance</span>
<span class="sd">           (e.g., at a boundary), stop.  (suggest: 1.0e-7)</span>
<span class="sd">    :type tolerance: float64 &gt;= 0.0</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">gd_data</span> <span class="o">=</span> <span class="n">C_GP</span><span class="o">.</span><span class="n">GradientDescentParameters</span><span class="p">(</span>
        <span class="n">num_multistarts</span><span class="p">,</span>
        <span class="n">max_num_steps</span><span class="p">,</span>
        <span class="n">max_num_restarts</span><span class="p">,</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">gamma</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">pre_mult</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">max_relative_change</span><span class="p">),</span>
        <span class="n">numpy</span><span class="o">.</span><span class="n">float64</span><span class="p">(</span><span class="n">tolerance</span><span class="p">),</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">gd_data</span>

</div>
<div class="viewcode-block" id="HyperparameterOptimizationParameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters.HyperparameterOptimizationParameters">[docs]</a><span class="k">class</span> <span class="nc">HyperparameterOptimizationParameters</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="sd">r&quot;&quot;&quot;Container for parameters that specify the behavior of a hyperparameter optimizer.</span>

<span class="sd">    We use slots to enforce a &quot;type.&quot; Typo&#39;ing a member name will error, not add a new field.</span>
<span class="sd">    This class is passed to C++, so it is convenient to be strict about its structure.</span>

<span class="sd">    :ivar objective_type: (*C_GP.LogLikelihoodTypes*) which log likelihood measure to use as the metric of model quality</span>
<span class="sd">      e.g., log marginal likelihood, leave one out cross validation log pseudo-likelihood.</span>
<span class="sd">      This attr is set via the cpp_wrappers.log_likelihood.LogLikelihood object used with optimization.</span>
<span class="sd">    :ivar optimizer_type: (*C_GP.OptimizerTypes*) which optimizer to use (e.g., dumb search, gradient dsecent, Newton)</span>
<span class="sd">    :ivar num_random_samples: (*int &gt;= 0*) number of samples to try if using &#39;dumb&#39; search</span>
<span class="sd">    :ivar optimizer_parameters: (*C_GP.\*Parameters* struct, matching ``optimizer_type``) parameters to control</span>
<span class="sd">      derviative-based optimizers, e.g., step size control, number of steps tolerance, etc.</span>

<span class="sd">    .. NOTE:: ``optimizer_parameters`` this MUST be a C++ object whose type matches objective_type. e.g., if objective_type</span>
<span class="sd">      is kNewton, then this must be built via C_GP.NewtonParameters() (wrapper: build_newton_parameters())</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s">&#39;objective_type&#39;</span><span class="p">,</span> <span class="s">&#39;optimizer_type&#39;</span><span class="p">,</span> <span class="s">&#39;num_random_samples&#39;</span><span class="p">,</span> <span class="s">&#39;optimizer_parameters&#39;</span><span class="p">,</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_type</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_random_samples</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">optimizer_parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct HyperparameterOptimizationParameters that specifies hyperparameter optimization behavior.&quot;&quot;&quot;</span>
        <span class="c"># see gpp_python_common.cpp for .*_type enum definitions. .*_type variables must be from those enums (NOT integers)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">objective_type</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c"># set via the LogLikelihood object passed to optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_type</span> <span class="o">=</span> <span class="n">optimizer_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_random_samples</span> <span class="o">=</span> <span class="n">num_random_samples</span>  <span class="c"># number of samples to &#39;dumb&#39; search over</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_parameters</span> <span class="o">=</span> <span class="n">optimizer_parameters</span>  <span class="c"># must match the optimizer_type</span>

</div>
<div class="viewcode-block" id="ExpectedImprovementOptimizationParameters"><a class="viewcode-back" href="../../../../../../../moe.optimal_learning.EPI.src.python.cpp_wrappers.html#moe.optimal_learning.EPI.src.python.cpp_wrappers.optimization_parameters.ExpectedImprovementOptimizationParameters">[docs]</a><span class="k">class</span> <span class="nc">ExpectedImprovementOptimizationParameters</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>

    <span class="sd">r&quot;&quot;&quot;Container for parameters that specify the behavior of a expected improvement optimizer.</span>

<span class="sd">    We use slots to enforce a &quot;type.&quot; Typo&#39;ing a member name will error, not add a new field.</span>
<span class="sd">    This class is passed to C++, so it is convenient to be strict about its structure.</span>

<span class="sd">    :ivar domain_type: (*C_GP.DomainTypes*) type of domain that we are optimizing expected improvement over (e.g., tensor, simplex)</span>
<span class="sd">    :ivar optimizer_type: (*C_GP.OptimizerTypes*) which optimizer to use (e.g., dumb search, gradient dsecent)</span>
<span class="sd">    :ivar num_random_samples: (*int &gt;= 0*) number of samples to try if using &#39;dumb&#39; search or if generating more</span>
<span class="sd">      than one simultaneous sample with dumb search fallback enabled</span>
<span class="sd">    :ivar optimizer_parameters: (*C_GP.\*Parameters* struct, matching ``optimizer_type``) parameters to control</span>
<span class="sd">      derviative-based optimizers, e.g., step size control, number of steps tolerance, etc.</span>

<span class="sd">    .. NOTE:: ``optimizer_parameters`` MUST be a C++ object whose type matches objective_type. e.g., if objective_type</span>
<span class="sd">      is kGradientDescent, then this must be built via C_GP.GradientDescentParameters object</span>
<span class="sd">      (wrapper: build_gradient_descent_parameters())</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">__slots__</span> <span class="o">=</span> <span class="p">(</span><span class="s">&#39;domain_type&#39;</span><span class="p">,</span> <span class="s">&#39;optimizer_type&#39;</span><span class="p">,</span> <span class="s">&#39;num_random_samples&#39;</span><span class="p">,</span> <span class="s">&#39;optimizer_parameters&#39;</span><span class="p">,</span> <span class="p">)</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer_type</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">num_random_samples</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">optimizer_parameters</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct ExpectedImprovementOptimizationParameters that specifies EI optimization behavior.&quot;&quot;&quot;</span>
        <span class="c"># see gpp_python_common.cpp for .*_type enum definitions. .*_type variables must be from those enums (NOT integers)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">domain_type</span> <span class="o">=</span> <span class="bp">None</span>  <span class="c"># set via the DomainInterface object passed to optimization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_type</span> <span class="o">=</span> <span class="n">optimizer_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_random_samples</span> <span class="o">=</span> <span class="n">num_random_samples</span>  <span class="c"># number of samples to &#39;dumb&#39; search over</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer_parameters</span> <span class="o">=</span> <span class="n">optimizer_parameters</span>  <span class="c"># must match the optimizer_type</span>
        <span class="c"># NOTE: need both num_random_samples AND optimizer_parameters if generating &gt; 1 sample</span>
        <span class="c"># using gradient descent optimization</span></div>
</pre></div>

          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Yelp and Cornell Collaboration.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'../../../../../../../',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="../../../../../../../_static/jquery.js"></script>
      <script type="text/javascript" src="../../../../../../../_static/underscore.js"></script>
      <script type="text/javascript" src="../../../../../../../_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="../../../../../../../_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>