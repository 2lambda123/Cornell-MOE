

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>moe.optimal_learning.python package &mdash; MOE 0.2.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.2.0 documentation" href="index.html"/>
        <link rel="up" title="moe.optimal_learning package" href="moe.optimal_learning.html"/>
        <link rel="next" title="moe.optimal_learning.python.cpp_wrappers package" href="moe.optimal_learning.python.cpp_wrappers.html"/>
        <link rel="prev" title="moe.optimal_learning.cpp package" href="moe.optimal_learning.cpp.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="why_moe.html">Why Do We Need MOE?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#other-methods">Other Methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-in-docker">Install in docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-from-source">Install from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#osx-tips">OSX Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-boost">Building Boost</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#linux-tips">Linux Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#cmake-tips">CMake Tips</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe_math.html">How does MOE work?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#build-a-gaussian-process-gp-with-the-historical-data">Build a Gaussian Process (GP) with the historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#optimize-the-hyperparameters-of-the-gaussian-process">Optimize the hyperparameters of the Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#find-the-point-s-of-highest-expected-improvement-ei">Find the point(s) of highest Expected Improvement (EI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#return-the-point-s-to-sample-then-repeat">Return the point(s) to sample, then repeat</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="demo_tutorial.html">Demo Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="demo_tutorial.html#the-interactive-demo">The Interactive Demo</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pretty_endpoints.html">Pretty Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="objective_functions.html">Objective Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#what-is-an-objective-function">What is an objective function?</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#properties-of-an-objective-function">Properties of an objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#id1"><span class="math">\(\Phi\)</span> Objective Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#example-of-objective-functions">Example of Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="bandit.html">Multi-Armed Bandits</a><ul>
<li class="toctree-l2"><a class="reference internal" href="bandit.html#what-is-the-multi-armed-bandit-problem">What is the multi-armed bandit problem?</a></li>
<li class="toctree-l2"><a class="reference internal" href="bandit.html#applications">Applications</a></li>
<li class="toctree-l2"><a class="reference internal" href="bandit.html#policies">Policies</a></li>
<li class="toctree-l2"><a class="reference internal" href="bandit.html#pointers">Pointers</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples.html#minimizing-an-arbitrary-function">Minimizing an arbitrary function</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#gaussian-process-regression-given-historical-data">Gaussian Process regression given historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#hyperparameter-optimization-of-a-gaussian-process">Hyperparameter optimization of a Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#all-above-examples-combined">All above examples combined</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#setting-thresholds-for-advertising-units">Setting thresholds for advertising units</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#making-a-pull-request">Making a pull request</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style">Style</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#versioning">Versioning</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#releasing-for-maintainers">Releasing (For Maintainers)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="faq.html#what-license-is-moe-released-under">What license is MOE released under?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#when-should-i-use-moe">When should I use MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#what-is-the-time-complexity-of-moe">What is the time complexity of MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-do-i-cite-moe">How do I cite MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#why-does-moe-take-so-long-to-return-the-next-points-to-sample-for-some-inputs">Why does MOE take so long to return the next points to sample for some inputs?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-do-i-bootstrap-moe-what-initial-data-does-it-need">How do I bootstrap MOE? What initial data does it need?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-many-function-evaluations-do-i-need-before-moe-is-done">How many function evaluations do I need before MOE is &#8220;done&#8221;?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-many-function-evaluations-do-i-perform-before-i-update-the-hyperparameters-of-the-gp">How many function evaluations do I perform before I update the hyperparameters of the GP?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#will-you-accept-my-pull-request">Will you accept my pull request?</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="moe.html">moe package</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe_examples.html">moe_examples package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.combined_example">moe_examples.combined_example module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.hyper_opt_of_gp_from_historical_data">moe_examples.hyper_opt_of_gp_from_historical_data module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.mean_and_var_of_gp_from_historic_data">moe_examples.mean_and_var_of_gp_from_historic_data module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.next_point_via_simple_endpoint">moe_examples.next_point_via_simple_endpoint module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples">Module contents</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="cpp_tree.html">C++ Files</a><ul>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_test.html">gpp_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain_test.html">gpp_domain_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_gpu.html">gpp_expected_improvement_gpu</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization_test.html">gpp_heuristic_expected_improvement_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra_test.html">gpp_linear_algebra_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry.html">gpp_geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization.html">gpp_heuristic_expected_improvement_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra-inl.html">gpp_linear_algebra-inl</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils.html">gpp_test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_logging.html">gpp_logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance.html">gpp_covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_test.html">gpp_python_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain.html">gpp_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_common.html">gpp_python_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyperparameter_optimization_demo.html">gpp_hyperparameter_optimization_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry_test.html">gpp_geometry_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math_test.html">gpp_math_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_cuda_math.html">gpp_cuda_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_model_selection.html">gpp_python_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math.html">gpp_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random_test.html">gpp_random_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimizer_parameters.html">gpp_optimizer_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_demo.html">gpp_expected_improvement_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization.html">gpp_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils_test.html">gpp_test_utils_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra.html">gpp_linear_algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_expected_improvement.html">gpp_python_expected_improvement</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_exception.html">gpp_exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection.html">gpp_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random.html">gpp_random</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance_test.html">gpp_covariance_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_mock_optimization_objective_functions.html">gpp_mock_optimization_objective_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python.html">gpp_python</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_test.html">gpp_model_selection_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyper_and_EI_demo.html">gpp_hyper_and_EI_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_gaussian_process.html">gpp_python_gaussian_process</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_common.html">gpp_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_gpu_test.html">gpp_expected_improvement_gpu_test</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="moe.html">moe package</a> &raquo;</li>
      
          <li><a href="moe.optimal_learning.html">moe.optimal_learning package</a> &raquo;</li>
      
    <li>moe.optimal_learning.python package</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/moe.optimal_learning.python.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="moe-optimal-learning-python-package">
<h1>moe.optimal_learning.python package<a class="headerlink" href="#moe-optimal-learning-python-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="subpackages">
<h2>Subpackages<a class="headerlink" href="#subpackages" title="Permalink to this headline">¶</a></h2>
<div class="toctree-wrapper compound">
<ul>
<li class="toctree-l1"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html">moe.optimal_learning.python.cpp_wrappers package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.covariance">moe.optimal_learning.python.cpp_wrappers.covariance module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.cpp_utils">moe.optimal_learning.python.cpp_wrappers.cpp_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.domain">moe.optimal_learning.python.cpp_wrappers.domain module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.expected_improvement">moe.optimal_learning.python.cpp_wrappers.expected_improvement module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.gaussian_process">moe.optimal_learning.python.cpp_wrappers.gaussian_process module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.log_likelihood">moe.optimal_learning.python.cpp_wrappers.log_likelihood module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers.optimization">moe.optimal_learning.python.cpp_wrappers.optimization module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html">moe.optimal_learning.python.interfaces package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.covariance_interface">moe.optimal_learning.python.interfaces.covariance_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.domain_interface">moe.optimal_learning.python.interfaces.domain_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.expected_improvement_interface">moe.optimal_learning.python.interfaces.expected_improvement_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.gaussian_process_interface">moe.optimal_learning.python.interfaces.gaussian_process_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.log_likelihood_interface">moe.optimal_learning.python.interfaces.log_likelihood_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces.optimization_interface">moe.optimal_learning.python.interfaces.optimization_interface module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe.optimal_learning.python.python_version.html">moe.optimal_learning.python.python_version package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.covariance">moe.optimal_learning.python.python_version.covariance module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.domain">moe.optimal_learning.python.python_version.domain module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.expected_improvement">moe.optimal_learning.python.python_version.expected_improvement module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.gaussian_process">moe.optimal_learning.python.python_version.gaussian_process module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.log_likelihood">moe.optimal_learning.python.python_version.log_likelihood module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.optimization">moe.optimal_learning.python.python_version.optimization module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version.python_utils">moe.optimal_learning.python.python_version.python_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version">Module contents</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div class="section" id="submodules">
<h2>Submodules<a class="headerlink" href="#submodules" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-moe.optimal_learning.python.constant">
<span id="moe-optimal-learning-python-constant-module"></span><h2>moe.optimal_learning.python.constant module<a class="headerlink" href="#module-moe.optimal_learning.python.constant" title="Permalink to this headline">¶</a></h2>
<p>Some default configuration parameters for optimal_learning components.</p>
<dl class="data">
<dt id="moe.optimal_learning.python.constant.CONSTANT_LIAR_METHODS">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">CONSTANT_LIAR_METHODS</tt><em class="property"> = ['constant_liar_min', 'constant_liar_max', 'constant_liar_mean']</em><a class="headerlink" href="#moe.optimal_learning.python.constant.CONSTANT_LIAR_METHODS" title="Permalink to this definition">¶</a></dt>
<dd><p>Pre-defined constant liar &#8220;lie&#8221; methods supported by <a class="reference internal" href="moe.html#module-moe" title="moe"><tt class="xref py py-mod docutils literal"><span class="pre">moe</span></tt></a></p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.COVARIANCE_TYPES">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">COVARIANCE_TYPES</tt><em class="property"> = ['square_exponential']</em><a class="headerlink" href="#moe.optimal_learning.python.constant.COVARIANCE_TYPES" title="Permalink to this definition">¶</a></dt>
<dd><p>Covariance types supported by <a class="reference internal" href="moe.html#module-moe" title="moe"><tt class="xref py py-mod docutils literal"><span class="pre">moe</span></tt></a></p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.DEFAULT_MAX_NUM_THREADS">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">DEFAULT_MAX_NUM_THREADS</tt><em class="property"> = 4</em><a class="headerlink" href="#moe.optimal_learning.python.constant.DEFAULT_MAX_NUM_THREADS" title="Permalink to this definition">¶</a></dt>
<dd><p>Default number of threads to use in computation</p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.DOMAIN_TYPES">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">DOMAIN_TYPES</tt><em class="property"> = ['tensor_product', 'simplex_intersect_tensor_product']</em><a class="headerlink" href="#moe.optimal_learning.python.constant.DOMAIN_TYPES" title="Permalink to this definition">¶</a></dt>
<dd><p>Domain types supported by <a class="reference internal" href="moe.html#module-moe" title="moe"><tt class="xref py py-mod docutils literal"><span class="pre">moe</span></tt></a></p>
</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.constant.DefaultOptimizerInfoTuple">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">DefaultOptimizerInfoTuple</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/constant.html#DefaultOptimizerInfoTuple"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.constant.DefaultOptimizerInfoTuple" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.constant._BaseDefaultOptimizerInfoTuple</span></tt></p>
<p>Container holding default values to use with a <tt class="xref py py-class docutils literal"><span class="pre">moe.views.schemas.OptimizerInfo</span></tt>.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>num_multistarts</strong> &#8211; (<em>int &gt; 0</em>) number of locations from which to start optimization runs</li>
<li><strong>num_random_samples</strong> &#8211; (<em>int &gt;= 0</em>) number of random search points to use if multistart optimization fails</li>
<li><strong>optimizer_parameters</strong> &#8211; (<em>namedtuple</em>) parameters to use with the core optimizer,
i.e., one of <a class="reference internal" href="moe.optimal_learning.python.python_version.html#moe.optimal_learning.python.python_version.optimization.GradientDescentParameters" title="moe.optimal_learning.python.python_version.optimization.GradientDescentParameters"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.python_version.optimization.GradientDescentParameters</span></tt></a>,
<a class="reference internal" href="moe.optimal_learning.python.python_version.html#moe.optimal_learning.python.python_version.optimization.NewtonParameters" title="moe.optimal_learning.python.python_version.optimization.NewtonParameters"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.python_version.optimization.NewtonParameters</span></tt></a>, etc.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.ENDPOINT_TO_DEFAULT_OPTIMIZER_TYPE">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">ENDPOINT_TO_DEFAULT_OPTIMIZER_TYPE</tt><em class="property"> = {'gp_next_points_kriging': 'gradient_descent_optimizer', ('gp_hyper_opt', 'leave_one_out_log_likelihood'): 'gradient_descent_optimizer', ('gp_next_points_epi', 'multi_point_ei'): 'l_bfgs_b_optimizer', 'gp_next_points_constant_liar': 'gradient_descent_optimizer', ('gp_hyper_opt', 'log_marginal_likelihood'): 'newton_optimizer', ('gp_next_points_epi', 'single_point_ei'): 'gradient_descent_optimizer'}</em><a class="headerlink" href="#moe.optimal_learning.python.constant.ENDPOINT_TO_DEFAULT_OPTIMIZER_TYPE" title="Permalink to this definition">¶</a></dt>
<dd><p>dict mapping from tuples describing endpoints and objective functions to optimizer type strings;
i.e., one of <a class="reference internal" href="#moe.optimal_learning.python.constant.OPTIMIZER_TYPES" title="moe.optimal_learning.python.constant.OPTIMIZER_TYPES"><tt class="xref py py-const docutils literal"><span class="pre">moe.optimal_learning.python.constant.OPTIMIZER_TYPES</span></tt></a>.</p>
</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.constant.GaussianProcessParameters">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">GaussianProcessParameters</tt><a class="headerlink" href="#moe.optimal_learning.python.constant.GaussianProcessParameters" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>GaussianProcessParameters(length_scale, signal_variance)</p>
<dl class="attribute">
<dt id="moe.optimal_learning.python.constant.GaussianProcessParameters.length_scale">
<tt class="descname">length_scale</tt><a class="headerlink" href="#moe.optimal_learning.python.constant.GaussianProcessParameters.length_scale" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.constant.GaussianProcessParameters.signal_variance">
<tt class="descname">signal_variance</tt><a class="headerlink" href="#moe.optimal_learning.python.constant.GaussianProcessParameters.signal_variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.LIKELIHOOD_TYPES">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">LIKELIHOOD_TYPES</tt><em class="property"> = ['leave_one_out_log_likelihood', 'log_marginal_likelihood']</em><a class="headerlink" href="#moe.optimal_learning.python.constant.LIKELIHOOD_TYPES" title="Permalink to this definition">¶</a></dt>
<dd><p>Log Likelihood types supported by <a class="reference internal" href="moe.html#module-moe" title="moe"><tt class="xref py py-mod docutils literal"><span class="pre">moe</span></tt></a></p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.MAX_ALLOWED_NUM_THREADS">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">MAX_ALLOWED_NUM_THREADS</tt><em class="property"> = 10000</em><a class="headerlink" href="#moe.optimal_learning.python.constant.MAX_ALLOWED_NUM_THREADS" title="Permalink to this definition">¶</a></dt>
<dd><p>Maximum number of threads that a user can specify
TODO(GH-301): make this a server configurable value or set appropriate openmp env var</p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.OPTIMIZER_TYPES">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">OPTIMIZER_TYPES</tt><em class="property"> = ['null_optimizer', 'newton_optimizer', 'gradient_descent_optimizer', 'l_bfgs_b_optimizer']</em><a class="headerlink" href="#moe.optimal_learning.python.constant.OPTIMIZER_TYPES" title="Permalink to this definition">¶</a></dt>
<dd><p>Optimizer types supported by <a class="reference internal" href="moe.html#module-moe" title="moe"><tt class="xref py py-mod docutils literal"><span class="pre">moe</span></tt></a></p>
</dd></dl>

<dl class="data">
<dt id="moe.optimal_learning.python.constant.OPTIMIZER_TYPE_AND_OBJECTIVE_TO_DEFAULT_PARAMETERS">
<tt class="descclassname">moe.optimal_learning.python.constant.</tt><tt class="descname">OPTIMIZER_TYPE_AND_OBJECTIVE_TO_DEFAULT_PARAMETERS</tt><em class="property"> = {('gradient_descent_optimizer', 'gp_next_points_epi', 'ei_analytic'): _BaseDefaultOptimizerInfoTuple(num_multistarts=600, num_random_samples=50000, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=500, max_num_restarts=4, num_steps_averaged=0, gamma=0.6, pre_mult=1.0, max_relative_change=1.0, tolerance=1e-07)), ('null_optimizer', 'gp_next_points_epi', 'ei_monte_carlo'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=50000, optimizer_parameters=NullParameters()), ('null_optimizer', 'gp_hyper_opt', 'log_marginal_likelihood'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=300000, optimizer_parameters=NullParameters()), ('null_optimizer', 'gp_next_points_epi', 'ei_analytic'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=500000, optimizer_parameters=NullParameters()), ('null_optimizer', 'gp_next_points_kriging'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=500000, optimizer_parameters=NullParameters()), ('null_optimizer', 'gp_next_points_constant_liar'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=500000, optimizer_parameters=NullParameters()), ('null_optimizer', 'gp_hyper_opt', 'leave_one_out_log_likelihood'): _BaseDefaultOptimizerInfoTuple(num_multistarts=1, num_random_samples=300000, optimizer_parameters=NullParameters()), ('newton_optimizer', 'gp_hyper_opt', 'log_marginal_likelihood'): _BaseDefaultOptimizerInfoTuple(num_multistarts=200, num_random_samples=0, optimizer_parameters=_BaseNewtonParameters(max_num_steps=150, gamma=1.2, time_factor=0.0005, max_relative_change=1.0, tolerance=1e-09)), ('gradient_descent_optimizer', 'gp_hyper_opt', 'leave_one_out_log_likelihood'): _BaseDefaultOptimizerInfoTuple(num_multistarts=400, num_random_samples=0, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=600, max_num_restarts=10, num_steps_averaged=0, gamma=0.9, pre_mult=0.25, max_relative_change=0.2, tolerance=1e-05)), ('gradient_descent_optimizer', 'gp_next_points_kriging'): _BaseDefaultOptimizerInfoTuple(num_multistarts=600, num_random_samples=50000, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=500, max_num_restarts=4, num_steps_averaged=0, gamma=0.6, pre_mult=1.0, max_relative_change=1.0, tolerance=1e-07)), ('l_bfgs_b_optimizer', 'gp_next_points_epi', 'ei_analytic'): _BaseDefaultOptimizerInfoTuple(num_multistarts=200, num_random_samples=4000, optimizer_parameters=_BaseLBFGSBParameters(approx_grad=True, max_func_evals=15000, max_metric_correc=10, factr=10000000.0, pgtol=1e-05, epsilon=1e-08)), ('gradient_descent_optimizer', 'gp_hyper_opt', 'log_marginal_likelihood'): _BaseDefaultOptimizerInfoTuple(num_multistarts=400, num_random_samples=0, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=600, max_num_restarts=10, num_steps_averaged=0, gamma=0.9, pre_mult=0.25, max_relative_change=0.2, tolerance=1e-05)), ('gradient_descent_optimizer', 'gp_next_points_constant_liar'): _BaseDefaultOptimizerInfoTuple(num_multistarts=600, num_random_samples=50000, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=500, max_num_restarts=4, num_steps_averaged=0, gamma=0.6, pre_mult=1.0, max_relative_change=1.0, tolerance=1e-07)), ('gradient_descent_optimizer', 'gp_next_points_epi', 'ei_monte_carlo'): _BaseDefaultOptimizerInfoTuple(num_multistarts=200, num_random_samples=4000, optimizer_parameters=_BaseGradientDescentParameters(max_num_steps=500, max_num_restarts=4, num_steps_averaged=100, gamma=0.6, pre_mult=1.0, max_relative_change=1.0, tolerance=1e-05))}</em><a class="headerlink" href="#moe.optimal_learning.python.constant.OPTIMIZER_TYPE_AND_OBJECTIVE_TO_DEFAULT_PARAMETERS" title="Permalink to this definition">¶</a></dt>
<dd><p>dict mapping from tuples of optimizer type, endpoint, etc. to default optimizer parameters. The default parameter
structs are of type <a class="reference internal" href="#moe.optimal_learning.python.constant.DefaultOptimizerInfoTuple" title="moe.optimal_learning.python.constant.DefaultOptimizerInfoTuple"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.constant.DefaultOptimizerInfoTuple</span></tt></a> and the actual default
parameters are defined in <a class="reference internal" href="#module-moe.optimal_learning.python.constant" title="moe.optimal_learning.python.constant"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.constant</span></tt></a>.
Note: (NEWTON_OPTIMIZER, views_constant.GP_HYPER_OPT_ROUTE_NAME, LEAVE_ONE_OUT_LOG_LIKELIHOOD)
does not have an entry because this combination is not yet implemented.
Newton is also not implemented for any of the GP_NEXT_POINTS_* endpoints.</p>
</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python.data_containers">
<span id="moe-optimal-learning-python-data-containers-module"></span><h2>moe.optimal_learning.python.data_containers module<a class="headerlink" href="#module-moe.optimal_learning.python.data_containers" title="Permalink to this headline">¶</a></h2>
<p>Data containers convenient for/used to interact with optimal_learning.python members.</p>
<dl class="class">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.data_containers.</tt><tt class="descname">HistoricalData</tt><big>(</big><em>dim</em>, <em>sample_points=None</em>, <em>validate=False</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">object</span></tt></p>
<p>A data container for storing the historical data from an entire experiment in a layout convenient for this library.</p>
<p>Users will likely find it most convenient to store experiment historical data in &#8220;tuples&#8221; of
(coordinates, value, noise); for example, these could be the columns of a database row, part of an ORM, etc.
The <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt> class (above) provides a convenient representation of this input format, but users are <em>not</em> required
to use it.</p>
<p>But the internals of optimal_learning will generally do computations on all coordinates at once, all values at once,
and/or all noise measurements at once. So this object reads the input data and &#8220;transposes&#8221; the ordering so that
we have a matrix of coordinates and vectors of values and noises. Compared to storing a list of <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt>,
these internals save on redundant data transformations and improve locality.</p>
<p>Note that the points in HistoricalData are <em>not</em> associated to any particular domain. HistoricalData could be (and is)
used for model selection as well as Gaussian Process manipulation, Expected Improvement optimization, etc. In the former,
the point-domain has no meaning (as opposed to the hyperparameter domain). In the latter, users could perform multiple
optimization runs with slightly different domains (e.g., differing levels of exploration) without changing
HistoricalData. Users may also optimize within a subdomain of the points already sampled. Thus, we are not including
domain in HistoricalData so as to place no restriction on how users can use optimal_learning and think about their
experiments.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>_points_sampled</strong> &#8211; (<em>array of float64 with shape (self.num_sampled, self.dim)</em>) already-sampled points</li>
<li><strong>_points_sampled_value</strong> &#8211; (<em>array of float64 with shape (self.num_sampled)</em>) function value measured at each point</li>
<li><strong>_points_sampled_noise_variance</strong> &#8211; (<em>array of float64 with shape (self.num_sampled)</em>) noise variance associated with <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.append_historical_data">
<tt class="descname">append_historical_data</tt><big>(</big><em>points_sampled</em>, <em>points_sampled_value</em>, <em>points_sampled_noise_variance</em>, <em>validate=False</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.append_historical_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.append_historical_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Append lists of points_sampled, their values, and their noise variances to the data members of this class.</p>
<p>This class (see class docstring) stores its data members as numpy arrays; this method provides a way for users
who already have data in this format to append directly instead of creating an intermediate <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt> list.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>points_sampled</strong> (<em>array of float64 with shape (num_sampled, dim)</em>) &#8211; already-sampled points</li>
<li><strong>points_sampled_value</strong> (<em>array of float64 with shape (num_sampled)</em>) &#8211; function value measured at each point</li>
<li><strong>points_sampled_noise_variance</strong> (<em>array of float64 with shape (num_sampled)</em>) &#8211; noise variance associated with <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt></li>
<li><strong>validate</strong> (<em>boolean</em>) &#8211; whether to sanity-check the input sample_points</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.append_sample_points">
<tt class="descname">append_sample_points</tt><big>(</big><em>sample_points</em>, <em>validate=False</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.append_sample_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.append_sample_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Append the contents of <tt class="docutils literal"><span class="pre">sample_points</span></tt> to the data members of this class.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sample_points</strong> (iterable of iterables with the same structure as a list of <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt>) &#8211; the already-sampled points: coordinates, objective function values, and noise variance</li>
<li><strong>validate</strong> (<em>boolean</em>) &#8211; whether to sanity-check the input sample_points</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.dim">
<tt class="descname">dim</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the number of spatial dimensions of a point in <tt class="docutils literal"><span class="pre">self.points_sampled</span></tt>.</p>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.json_payload">
<tt class="descname">json_payload</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.json_payload"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.json_payload" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a json serializeable and MOE REST recognizeable dictionary of the historical data.</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.num_sampled">
<tt class="descname">num_sampled</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.num_sampled"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.num_sampled" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the number of sampled points.</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.points_sampled">
<tt class="descname">points_sampled</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.points_sampled"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.points_sampled" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the coordinates of the points_sampled, array of float64 with shape (self.num_sampled, self.dim).</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.points_sampled_noise_variance">
<tt class="descname">points_sampled_noise_variance</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.points_sampled_noise_variance"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.points_sampled_noise_variance" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the noise variances associated with function values measured at each of <tt class="docutils literal"><span class="pre">self.points_sampled</span></tt>, array of floa664 with shape (self.num_sampled).</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.points_sampled_value">
<tt class="descname">points_sampled_value</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.points_sampled_value"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.points_sampled_value" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the objective function values measured at each of <tt class="docutils literal"><span class="pre">self.points_sampled</span></tt>, array of floa664 with shape (self.num_sampled).</p>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.to_list_of_sample_points">
<tt class="descname">to_list_of_sample_points</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.to_list_of_sample_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.to_list_of_sample_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert this HistoricalData into a list of SamplePoint.</p>
<p>The list of SamplePoint format is more convenient for human consumption/introspection.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">list where i-th SamplePoint has data from the i-th entry of each self.points_sampled* member.</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">list of <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.validate_historical_data">
<em class="property">static </em><tt class="descname">validate_historical_data</tt><big>(</big><em>dim</em>, <em>points_sampled</em>, <em>points_sampled_value</em>, <em>points_sampled_noise_variance</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.validate_historical_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.validate_historical_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Check that the historical data components (dim, coordinates, values, noises) are consistent in dimension and all have finite values.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dim</strong> (<em>int &gt; 0</em>) &#8211; number of (expected) spatial dimensions</li>
<li><strong>points_sampled</strong> (<em>array of float64 with shape (num_sampled, dim)</em>) &#8211; already-sampled points</li>
<li><strong>points_sampled_value</strong> (<em>array of float64 with shape (num_sampled)</em>) &#8211; function value measured at each point</li>
<li><strong>points_sampled_noise_variance</strong> (<em>array of float64 with shape (num_sampled)</em>) &#8211; noise variance associated with <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt></li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">True if inputs are valid</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">boolean</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="staticmethod">
<dt id="moe.optimal_learning.python.data_containers.HistoricalData.validate_sample_points">
<em class="property">static </em><tt class="descname">validate_sample_points</tt><big>(</big><em>dim</em>, <em>sample_points</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#HistoricalData.validate_sample_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.HistoricalData.validate_sample_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Check that sample_points passes basic validity checks: dimension is the same, all values are finite.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>dim</strong> (<em>int &gt; 0</em>) &#8211; number of (expected) spatial dimensions</li>
<li><strong>sample_points</strong> (iterable of iterables with the same structure as a list of <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.SamplePoint</span></tt>) &#8211; the already-sampled points: coordinates, objective function values, and noise variance</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">True if inputs are valid</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">boolean</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.data_containers.SamplePoint">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.data_containers.</tt><tt class="descname">SamplePoint</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#SamplePoint"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.SamplePoint" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.data_containers._BaseSamplePoint</span></tt></p>
<p>A point (coordinates, function value, noise variance) sampled from the objective function we are modeling/optimizing.</p>
<p>This class is a representation of a &#8220;Sample Point,&#8221; which is defined by the three data members listed here.
SamplePoint is a convenient way of communicating data to the rest of the optimal_learning library (via the
HistoricalData container); it also provides a convenient grouping for interactive introspection.</p>
<p>Users are not required to use SamplePoint&#8211;iterables with the same data layout will suffice.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>point</strong> &#8211; (<em>iterable of dim float64</em>) The point sampled (in the domain of the function)</li>
<li><strong>value</strong> &#8211; (<em>float64</em>) The value returned by the function</li>
<li><strong>noise_variance</strong> &#8211; (<em>float64 &gt;= 0.0</em>) The noise/measurement variance (if any) associated with :attr`value`</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.SamplePoint.json_payload">
<tt class="descname">json_payload</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#SamplePoint.json_payload"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.SamplePoint.json_payload" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert the sample_point into a dict to be consumed by json for a REST request.</p>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.data_containers.SamplePoint.validate">
<tt class="descname">validate</tt><big>(</big><em>dim=None</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/data_containers.html#SamplePoint.validate"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.data_containers.SamplePoint.validate" title="Permalink to this definition">¶</a></dt>
<dd><p>Check this SamplePoint passes basic validity checks: dimension is expected, all values are finite.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><p class="first"><strong>dim</strong> (<em>int &gt; 0</em>) &#8211; number of (expected) spatial dimensions; None to skip check</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><strong>ValueError</strong> &#8211; self.point does not have exactly dim entries</li>
<li><strong>ValueError</strong> &#8211; if any member data is non-finite or out of range</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python.geometry_utils">
<span id="moe-optimal-learning-python-geometry-utils-module"></span><h2>moe.optimal_learning.python.geometry_utils module<a class="headerlink" href="#module-moe.optimal_learning.python.geometry_utils" title="Permalink to this headline">¶</a></h2>
<p>Geometry utilities. e.g., ClosedInterval, point-plane geometry, random point generation.</p>
<dl class="class">
<dt id="moe.optimal_learning.python.geometry_utils.ClosedInterval">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.geometry_utils.</tt><tt class="descname">ClosedInterval</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#ClosedInterval"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval" title="moe.optimal_learning.python.geometry_utils.ClosedInterval"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.geometry_utils.ClosedInterval</span></tt></a></p>
<p>Container to represent the mathematical notion of a closed interval, commonly written ms [a,b]me.</p>
<p>The closed interval ms [a,b]me is the set of all numbers ms x in mathbb{R}me such that ms a leq x leq bme.
Note that &#8220;closed&#8221; here indicates the interval <em>includes</em> both endpoints.
An interval with ms a &gt; bme is considered empty.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Variables:</th><td class="field-body"><ul class="first last simple">
<li><strong>min</strong> &#8211; (<em>float64</em>) the &#8220;left&#8221; bound of the domain, <tt class="docutils literal"><span class="pre">a</span></tt></li>
<li><strong>max</strong> &#8211; (<em>float64</em>) the &#8220;right&#8221; bound of the domain, <tt class="docutils literal"><span class="pre">b</span></tt></li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="staticmethod">
<dt id="moe.optimal_learning.python.geometry_utils.ClosedInterval.build_closed_intervals_from_list">
<em class="property">static </em><tt class="descname">build_closed_intervals_from_list</tt><big>(</big><em>bounds_list</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#ClosedInterval.build_closed_intervals_from_list"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval.build_closed_intervals_from_list" title="Permalink to this definition">¶</a></dt>
<dd><p>Construct a list of dim ClosedInterval from an iterable structure of dim iterables with len = 2.</p>
<p>For example, [[1, 2], [3, 4]] becomes [ClosedInterval(min=1, max=2), ClosedInterval(min=3, max=4)].</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>bounds_list</strong> (<em>iterable of iterables, where the second dimension has len = 2</em>) &#8211; bounds to convert</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">bounds_list converted to list of ClosedInterval</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">list of ClosedInterval</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.geometry_utils.ClosedInterval.is_empty">
<tt class="descname">is_empty</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#ClosedInterval.is_empty"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval.is_empty" title="Permalink to this definition">¶</a></dt>
<dd><p>Check whether this ClosedInterval is the emptyset: max &lt; min.</p>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.geometry_utils.ClosedInterval.is_inside">
<tt class="descname">is_inside</tt><big>(</big><em>value</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#ClosedInterval.is_inside"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval.is_inside" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if a value is inside this ClosedInterval.</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.geometry_utils.ClosedInterval.length">
<tt class="descname">length</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#ClosedInterval.length"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval.length" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute the length of this ClosedInterval.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="moe.optimal_learning.python.geometry_utils.generate_grid_points">
<tt class="descclassname">moe.optimal_learning.python.geometry_utils.</tt><tt class="descname">generate_grid_points</tt><big>(</big><em>points_per_dimension</em>, <em>domain_bounds</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#generate_grid_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.generate_grid_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate a uniform grid of points on a tensor product region; exponential runtime.</p>
<p>This can be useful for producing a reasonable set of initial samples when bootstrapping optimal_learning.
Grid sampling (as opposed to a random sampling, e.g., latin hypercube) is not random. It also guarantees
sampling of the domain corners.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">This operation is like an outer-product, so 4 points per dimension in 10 dimensions produces
4^{10} points. This could be built as an iterator instead, but the typical use
case involves function evaluations at every point, so generating the points is
not the limiting factor.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>points_per_dimension</strong> (<em>tuple or scalar</em>) &#8211; (n_1, n_2, ... n_{dim}) number of stencil points per spatial dimension.
If len(points_per_dimension) == 1, then n_i = len(points_per_dimension)</li>
<li><strong>domain_bounds</strong> (<em>iterable of dim ClosedInterval</em>) &#8211; the boundaries of a dim-dimensional tensor-product domain</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">stencil point coordinates</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array of float64 with shape (Pi_i n_i, dim)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="moe.optimal_learning.python.geometry_utils.generate_latin_hypercube_points">
<tt class="descclassname">moe.optimal_learning.python.geometry_utils.</tt><tt class="descname">generate_latin_hypercube_points</tt><big>(</big><em>num_points</em>, <em>domain_bounds</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/geometry_utils.html#generate_latin_hypercube_points"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.geometry_utils.generate_latin_hypercube_points" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a set of random points inside some domain that lie in a latin hypercube.</p>
<p>In 2D, a latin hypercube is a latin square&#8211;a checkerboard&#8211;such that there is exactly one sample in
each row and each column.  This notion is generalized for higher dimensions where each dimensional
&#8216;slice&#8217; has precisely one sample.</p>
<p>See wikipedia: <a class="reference external" href="http://en.wikipedia.org/wiki/Latin_hypercube_sampling">http://en.wikipedia.org/wiki/Latin_hypercube_sampling</a>
for more details on the latin hypercube sampling process.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_points</strong> (<em>int &gt; 0</em>) &#8211; number of random points to generate</li>
<li><strong>domain_bounds</strong> (<em>list of dim ClosedInterval</em>) &#8211; [min, max] boundaries of the hypercube in each dimension</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">uniformly distributed random points inside the specified hypercube</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array of float64 with shape (num_points, dim)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python.linkers">
<span id="moe-optimal-learning-python-linkers-module"></span><h2>moe.optimal_learning.python.linkers module<a class="headerlink" href="#module-moe.optimal_learning.python.linkers" title="Permalink to this headline">¶</a></h2>
<p>Links between the python and cpp_wrapper implementations of domains, covariances and optimizations.</p>
<dl class="class">
<dt id="moe.optimal_learning.python.linkers.CovarianceLinks">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.linkers.</tt><tt class="descname">CovarianceLinks</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.CovarianceLinks" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>CovarianceLinks(python_covariance_class, cpp_covariance_class)</p>
<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.CovarianceLinks.cpp_covariance_class">
<tt class="descname">cpp_covariance_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.CovarianceLinks.cpp_covariance_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.CovarianceLinks.python_covariance_class">
<tt class="descname">python_covariance_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.CovarianceLinks.python_covariance_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.linkers.DomainLinks">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.linkers.</tt><tt class="descname">DomainLinks</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.DomainLinks" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>DomainLinks(python_domain_class, cpp_domain_class)</p>
<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.DomainLinks.cpp_domain_class">
<tt class="descname">cpp_domain_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.DomainLinks.cpp_domain_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.DomainLinks.python_domain_class">
<tt class="descname">python_domain_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.DomainLinks.python_domain_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.linkers.LogLikelihoodMethod">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.linkers.</tt><tt class="descname">LogLikelihoodMethod</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.LogLikelihoodMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>LogLikelihoodMethod(log_likelihood_type, log_likelihood_class)</p>
<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.LogLikelihoodMethod.log_likelihood_class">
<tt class="descname">log_likelihood_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.LogLikelihoodMethod.log_likelihood_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.LogLikelihoodMethod.log_likelihood_type">
<tt class="descname">log_likelihood_type</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.LogLikelihoodMethod.log_likelihood_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.linkers.</tt><tt class="descname">OptimizerMethod</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <tt class="xref py py-class docutils literal"><span class="pre">tuple</span></tt></p>
<p>OptimizerMethod(optimizer_type, python_parameters_class, cpp_parameters_class, python_optimizer_class, cpp_optimizer_class)</p>
<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod.cpp_optimizer_class">
<tt class="descname">cpp_optimizer_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod.cpp_optimizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 4</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod.cpp_parameters_class">
<tt class="descname">cpp_parameters_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod.cpp_parameters_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 2</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod.optimizer_type">
<tt class="descname">optimizer_type</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod.optimizer_type" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 0</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod.python_optimizer_class">
<tt class="descname">python_optimizer_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod.python_optimizer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 3</p>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.linkers.OptimizerMethod.python_parameters_class">
<tt class="descname">python_parameters_class</tt><a class="headerlink" href="#moe.optimal_learning.python.linkers.OptimizerMethod.python_parameters_class" title="Permalink to this definition">¶</a></dt>
<dd><p>Alias for field number 1</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python.repeated_domain">
<span id="moe-optimal-learning-python-repeated-domain-module"></span><h2>moe.optimal_learning.python.repeated_domain module<a class="headerlink" href="#module-moe.optimal_learning.python.repeated_domain" title="Permalink to this headline">¶</a></h2>
<p>RepeatedDomain class for handling manipulating sets of points in a (kernel) domain simultaneously.</p>
<dl class="class">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain">
<em class="property">class </em><tt class="descclassname">moe.optimal_learning.python.repeated_domain.</tt><tt class="descname">RepeatedDomain</tt><big>(</big><em>num_repeats</em>, <em>domain</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference internal" href="moe.optimal_learning.python.interfaces.html#moe.optimal_learning.python.interfaces.domain_interface.DomainInterface" title="moe.optimal_learning.python.interfaces.domain_interface.DomainInterface"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.interfaces.domain_interface.DomainInterface</span></tt></a></p>
<p>A generic domain type for simultaneously manipulating <tt class="docutils literal"><span class="pre">num_repeats</span></tt> points in a &#8220;regular&#8221; domain (the kernel).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Comments in this class are copied from RepeatedDomain in gpp_domain.hpp.</p>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">the kernel domain is <em>not</em> copied. Instead, the kernel functions are called
<tt class="docutils literal"><span class="pre">num_repeats</span></tt> times in a loop. In some cases, data reordering is also necessary
to preserve the output properties (e.g., uniform distribution).</p>
</div>
<p>For some use cases (e.g., q,p-EI optimization with q &gt; 1), we need to simultaneously
manipulate several points within the same domain. To support this use case, we have
the <tt class="docutils literal"><span class="pre">RepeatedDomain</span></tt>, a light-weight wrapper around any DomainInterface subclass
that kernalizes that object&#8217;s functionality.</p>
<p>In general, kernel domain operations need be performed <tt class="docutils literal"><span class="pre">num_repeats</span></tt> times, once
for each point. This class hides the looping logic so that use cases like various
<a class="reference internal" href="moe.optimal_learning.python.interfaces.html#moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface" title="moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface</span></tt></a>
subclasses do not need to be explicitly aware
of whether they are optimizing 1 point or 50 points. Instead, the OptimizableInterface
implementation provides problem_size() and appropriately sized gradient information.
Coupled with RepeatedDomain, Optimizers can remain oblivious.</p>
<p>In simpler terms, say we want to solve 5,0-EI in a parameter-space of dimension 3.
So we would have 5 points moving around in a 3D space. The 3D space, whatever it is,
is the kernel domain. We &#8220;repeat&#8221; the kernel 5 times; in practice this mostly amounts to
simple loops around kernel functions and sometimes data reordering is also needed.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">this operation is more complex than just working in a higher dimensional space.
3 points in a 2D simplex is not the same as 1 point in a 6D simplex; e.g.,
<tt class="docutils literal"><span class="pre">[(0.5,</span> <span class="pre">0.5),</span> <span class="pre">(0.5,</span> <span class="pre">0.5),</span> <span class="pre">(0.5,</span> <span class="pre">0.5)]</span></tt> is valid in the first scenario but not in the second.</p>
</div>
<p>Where the member domain takes <tt class="docutils literal"><span class="pre">kernel_input</span></tt>, this class&#8217;s members take an array with
shape <tt class="docutils literal"><span class="pre">(num_repeats,</span> <span class="pre">)</span> <span class="pre">+</span> <span class="pre">kernel_input.shape</span></tt>. Similarly <tt class="docutils literal"><span class="pre">kernel_output</span></tt> becomes an
array with shape <tt class="docutils literal"><span class="pre">(num_repeats,</span> <span class="pre">)</span> <span class="pre">+</span> <span class="pre">kernel_output.shape</span></tt>.</p>
<p>For example, <tt class="docutils literal"><span class="pre">check_point_inside()</span></tt> calls the kernel domain&#8217;s <tt class="docutils literal"><span class="pre">check_point_inside()</span></tt>
function <tt class="docutils literal"><span class="pre">num_repeats</span></tt> times, returning True only if all <tt class="docutils literal"><span class="pre">num_repeats</span></tt> input
points are inside the kernel domain.</p>
<dl class="method">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.check_point_inside">
<tt class="descname">check_point_inside</tt><big>(</big><em>points</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.check_point_inside"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.check_point_inside" title="Permalink to this definition">¶</a></dt>
<dd><p>Check if a point is inside the domain/on its boundary or outside.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>point</strong> (<em>array of float64 with shape (num_repeats, dim)</em>) &#8211; point to check</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">true if point is inside the repeated domain</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body">bool</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.compute_update_restricted_to_domain">
<tt class="descname">compute_update_restricted_to_domain</tt><big>(</big><em>max_relative_change</em>, <em>current_point</em>, <em>update_vector</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.compute_update_restricted_to_domain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.compute_update_restricted_to_domain" title="Permalink to this definition">¶</a></dt>
<dd><p>Compute a new update so that CheckPointInside(<tt class="docutils literal"><span class="pre">current_point</span></tt> + <tt class="docutils literal"><span class="pre">return_value</span></tt>) is true.</p>
<dl class="docutils">
<dt>Returns a new update vector in <tt class="docutils literal"><span class="pre">return_value</span></tt> so that:</dt>
<dd><tt class="docutils literal"><span class="pre">point_new</span> <span class="pre">=</span> <span class="pre">point</span> <span class="pre">+</span> <span class="pre">return_value</span></tt></dd>
</dl>
<p>has coordinates such that <tt class="docutils literal"><span class="pre">CheckPointInside(point_new)</span></tt> returns true. We select <tt class="docutils literal"><span class="pre">point_new</span></tt>
by projecting <tt class="docutils literal"><span class="pre">point</span> <span class="pre">+</span> <span class="pre">update_vector</span></tt> to the nearest point on the domain.</p>
<p><tt class="docutils literal"><span class="pre">return_value</span></tt> is a function of <tt class="docutils literal"><span class="pre">update_vector</span></tt>.
<tt class="docutils literal"><span class="pre">return_value</span></tt> is just a copy of <tt class="docutils literal"><span class="pre">update_vector</span></tt> if <tt class="docutils literal"><span class="pre">current_point</span></tt> is already inside the domain.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">We modify update_vector (instead of returning point_new) so that further update
limiting/testing may be performed.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>max_relative_change</strong> (<em>float64 in (0, 1]</em>) &#8211; max change allowed per update (as a relative fraction of current distance to boundary)</li>
<li><strong>current_point</strong> (<em>array of float64 with shape (num_repeats, dim)</em>) &#8211; starting point</li>
<li><strong>update_vector</strong> (<em>array of float64 with shape (num_repeats, dim)</em>) &#8211; proposed update</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">new update so that the final point remains inside the domain</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array of float64 with shape (num_repeats, dim)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="attribute">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.dim">
<tt class="descname">dim</tt><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.dim"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.dim" title="Permalink to this definition">¶</a></dt>
<dd><p>Return the number of spatial dimensions of the kernel domain.</p>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.generate_random_point_in_domain">
<tt class="descname">generate_random_point_in_domain</tt><big>(</big><em>random_source=None</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.generate_random_point_in_domain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.generate_random_point_in_domain" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate <tt class="docutils literal"><span class="pre">point</span></tt> uniformly at random such that <tt class="docutils literal"><span class="pre">self.check_point_inside(point)</span></tt> is True.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">if you need multiple points, use generate_uniform_random_points_in_domain instead;
depending on implementation, it may ield better distributions over many points. For example,
tensor product type domains use latin hypercube sampling instead of repeated random draws
which guarantees that no non-uniform clusters may arise (in subspaces) versus this method
which treats all draws independently.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">point in repeated domain</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body">array of float64 with shape (num_repeats, dim)</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.generate_uniform_random_points_in_domain">
<tt class="descname">generate_uniform_random_points_in_domain</tt><big>(</big><em>num_points</em>, <em>random_source=None</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.generate_uniform_random_points_in_domain"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.generate_uniform_random_points_in_domain" title="Permalink to this definition">¶</a></dt>
<dd><p>Generate AT MOST <tt class="docutils literal"><span class="pre">num_points</span></tt> uniformly distributed points from the domain.</p>
<p>Unlike many of this class&#8217;s other member functions, <tt class="docutils literal"><span class="pre">generate_uniform_random_points_in_domain()</span></tt>
is not as simple as calling the kernel&#8217;s member function <tt class="docutils literal"><span class="pre">num_repeats</span></tt> times. To
obtain the same distribution, we have to additionally &#8220;transpose&#8221; (see implementation
for details).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">The number of points returned may be LESS THAN <tt class="docutils literal"><span class="pre">num_points</span></tt>!</p>
</div>
<p>Implementations may use rejection sampling. In such cases, generating the requested
number of points may be unreasonably slow, so implementers are allowed to generate
fewer than <tt class="docutils literal"><span class="pre">num_points</span></tt> results.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>num_points</strong> (<em>integer &gt;= 0</em>) &#8211; max number of points to generate</li>
<li><strong>random_source</strong> (<em>callable yielding uniform random numbers in [0,1]</em>) &#8211; </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first">uniform random sampling of points from the domain; may be fewer than <tt class="docutils literal"><span class="pre">num_points</span></tt>!</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">array of float64 with shape (num_points_generated, num_repeats, dim)</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="moe.optimal_learning.python.repeated_domain.RepeatedDomain.get_bounding_box">
<tt class="descname">get_bounding_box</tt><big>(</big><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/repeated_domain.html#RepeatedDomain.get_bounding_box"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain.get_bounding_box" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a list of ClosedIntervals representing a bounding box for this domain.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python.timing">
<span id="moe-optimal-learning-python-timing-module"></span><h2>moe.optimal_learning.python.timing module<a class="headerlink" href="#module-moe.optimal_learning.python.timing" title="Permalink to this headline">¶</a></h2>
<p>Simple context manager for logging timing information.</p>
<p>TODO(GH-299): Make this part of a more complete monitoring setup, flesh out timing tools.
TODO(GH-299): Add a decorator for timing functions.</p>
<dl class="function">
<dt id="moe.optimal_learning.python.timing.timing_context">
<tt class="descclassname">moe.optimal_learning.python.timing.</tt><tt class="descname">timing_context</tt><big>(</big><em>*args</em>, <em>**kwds</em><big>)</big><a class="reference internal" href="_modules/moe/optimal_learning/python/timing.html#timing_context"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#moe.optimal_learning.python.timing.timing_context" title="Permalink to this definition">¶</a></dt>
<dd><p>Context manager that logs the runtime of the body of the with-statement.</p>
<p>Uses time.clock() for measurement; not appropriate for fast-running code.
Consider the <tt class="docutils literal"><span class="pre">timeit</span></tt> library for such situations.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>name</strong> (<em>str</em>) &#8211; name to log with this timing information</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-moe.optimal_learning.python">
<span id="module-contents"></span><h2>Module contents<a class="headerlink" href="#module-moe.optimal_learning.python" title="Permalink to this headline">¶</a></h2>
<p>The python component of the optimal_learning package, containing wrappers around C++ implementations of features and Python implementations of some of those features.</p>
<p><strong>Files in this package</strong></p>
<ul class="simple">
<li><a class="reference internal" href="#module-moe.optimal_learning.python.constant" title="moe.optimal_learning.python.constant"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.constant</span></tt></a>: some default configuration values for <tt class="docutils literal"><span class="pre">optimal_learning</span></tt> components</li>
<li><a class="reference internal" href="#module-moe.optimal_learning.python.data_containers" title="moe.optimal_learning.python.data_containers"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.data_containers</span></tt></a>: <a class="reference internal" href="#moe.optimal_learning.python.data_containers.SamplePoint" title="moe.optimal_learning.python.data_containers.SamplePoint"><tt class="xref py py-class docutils literal"><span class="pre">SamplePoint</span></tt></a>
and <a class="reference internal" href="#moe.optimal_learning.python.data_containers.HistoricalData" title="moe.optimal_learning.python.data_containers.HistoricalData"><tt class="xref py py-class docutils literal"><span class="pre">HistoricalData</span></tt></a> containers for passing data to the <tt class="docutils literal"><span class="pre">optimal_learning</span></tt> library</li>
<li><a class="reference internal" href="#module-moe.optimal_learning.python.geometry_utils" title="moe.optimal_learning.python.geometry_utils"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.geometry_utils</span></tt></a>: geometry utilities;
e.g., <a class="reference internal" href="#moe.optimal_learning.python.geometry_utils.ClosedInterval" title="moe.optimal_learning.python.geometry_utils.ClosedInterval"><tt class="xref py py-class docutils literal"><span class="pre">ClosedInterval</span></tt></a>, random point generation</li>
<li><a class="reference internal" href="#module-moe.optimal_learning.python.linkers" title="moe.optimal_learning.python.linkers"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.linkers</span></tt></a>: linkers connecting equivalent <tt class="docutils literal"><span class="pre">cpp_wrappers</span></tt> and <tt class="docutils literal"><span class="pre">python_version</span></tt> versions of <tt class="docutils literal"><span class="pre">optimal_learning</span></tt> components.</li>
<li><a class="reference internal" href="#module-moe.optimal_learning.python.repeated_domain" title="moe.optimal_learning.python.repeated_domain"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.repeated_domain</span></tt></a>: <a class="reference internal" href="#moe.optimal_learning.python.repeated_domain.RepeatedDomain" title="moe.optimal_learning.python.repeated_domain.RepeatedDomain"><tt class="xref py py-class docutils literal"><span class="pre">RepeatedDomain</span></tt></a>
object for manipulating sets of points simultaneously within the same domain</li>
</ul>
<p><strong>Major sub-packages</strong></p>
<p><strong>interfaces</strong>
<a class="reference internal" href="moe.optimal_learning.python.interfaces.html#module-moe.optimal_learning.python.interfaces" title="moe.optimal_learning.python.interfaces"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.interfaces</span></tt></a></p>
<p>A set of abstract base classes (ABCs) defining an interface for interacting with <tt class="docutils literal"><span class="pre">optimal_learning</span></tt>. These consist of composable
functions and classes to build models, perform model selection, and design new experiments.</p>
<p><strong>cpp_wrappers</strong>
<a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#module-moe.optimal_learning.python.cpp_wrappers" title="moe.optimal_learning.python.cpp_wrappers"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.cpp_wrappers</span></tt></a></p>
<p>An implementation of the ABCs in interfaces using wrappers around (fast) C++ calls. These routines are meant for &#8220;production&#8221; runs
where high performance is a concern.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">the higher level C++ interfaces are generally <em>not</em> composable with objects not in the cpp_wrappers package. So it
would be possible to implement
<a class="reference internal" href="moe.optimal_learning.python.interfaces.html#moe.optimal_learning.python.interfaces.expected_improvement_interface.ExpectedImprovementInterface" title="moe.optimal_learning.python.interfaces.expected_improvement_interface.ExpectedImprovementInterface"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.interfaces.expected_improvement_interface.ExpectedImprovementInterface</span></tt></a>
in Python and connect it to
<a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#moe.optimal_learning.python.cpp_wrappers.gaussian_process.GaussianProcess" title="moe.optimal_learning.python.cpp_wrappers.gaussian_process.GaussianProcess"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.cpp_wrappers.gaussian_process.GaussianProcess</span></tt></a>,
BUT it is not currently possible to connect
<a class="reference internal" href="moe.optimal_learning.python.cpp_wrappers.html#moe.optimal_learning.python.cpp_wrappers.expected_improvement.ExpectedImprovement" title="moe.optimal_learning.python.cpp_wrappers.expected_improvement.ExpectedImprovement"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.cpp_wrappers.expected_improvement.ExpectedImprovement</span></tt></a> to
<a class="reference internal" href="moe.optimal_learning.python.python_version.html#moe.optimal_learning.python.python_version.gaussian_process.GaussianProcess" title="moe.optimal_learning.python.python_version.gaussian_process.GaussianProcess"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.python_version.gaussian_process.GaussianProcess</span></tt></a>.</p>
</div>
<p><strong>python_version</strong>
<a class="reference internal" href="moe.optimal_learning.python.python_version.html#module-moe.optimal_learning.python.python_version" title="moe.optimal_learning.python.python_version"><tt class="xref py py-mod docutils literal"><span class="pre">moe.optimal_learning.python.python_version</span></tt></a></p>
<p>An implementation of the ABCs in interfaces using Python (with numpy/scipy). These routines are more for educational and
experimental purposes. Python is generally simpler than C++ so the hope is that this package is more accessible to new
users hoping to learn about optimal_learning. Additionally, development time in Python is shorter, so it could be convenient
to test new ideas here before fully implementing them in C++. For example, developers could test a new
<a class="reference internal" href="moe.optimal_learning.python.interfaces.html#moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface" title="moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface"><tt class="xref py py-class docutils literal"><span class="pre">moe.optimal_learning.python.interfaces.optimization_interface.OptimizerInterface</span></tt></a>
implementation in Python while connecting it to C++ evaluation of objective functions.</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="moe.optimal_learning.python.cpp_wrappers.html" class="btn btn-neutral float-right" title="moe.optimal_learning.python.cpp_wrappers package"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="moe.optimal_learning.cpp.html" class="btn btn-neutral" title="moe.optimal_learning.cpp package"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2012-2014 Yelp. MOE is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.2.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>