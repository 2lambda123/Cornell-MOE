

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gpp_test_utils &mdash; MOE 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.1.0 documentation" href="index.html"/>
        <link rel="up" title="C++ Files" href="cpp_tree.html"/>
        <link rel="next" title="gpp_logging" href="gpp_logging.html"/>
        <link rel="prev" title="gpp_linear_algebra-inl" href="gpp_linear_algebra-inl.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="why_moe.html">Why Do We Need MOE?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#other-methods">Other Methods</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-in-docker">Install in docker</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-from-source">Install from source</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#osx-tips">OSX Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#building-boost">Building Boost</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#linux-tips">Linux Tips</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#cmake-tips">CMake Tips</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe_math.html">How does MOE work?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#build-a-gaussian-process-gp-with-the-historical-data">Build a Gaussian Process (GP) with the historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#optimize-the-hyperparameters-of-the-gaussian-process">Optimize the hyperparameters of the Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#find-the-point-s-of-highest-expected-improvement-ei">Find the point(s) of highest Expected Improvement (EI)</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_math.html#return-the-point-s-to-sample-then-repeat">Return the point(s) to sample, then repeat</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="demo_tutorial.html">Demo Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="demo_tutorial.html#the-interactive-demo">The Interactive Demo</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="pretty_endpoints.html">Pretty Endpoints</a></li>
<li class="toctree-l1"><a class="reference internal" href="objective_functions.html">Objective Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#what-is-an-objective-function">What is an objective function?</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#properties-of-an-objective-function">Properties of an objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#id1"><span class="math">\(\Phi\)</span> Objective Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#example-of-objective-functions">Example of Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples.html#minimizing-an-arbitrary-function">Minimizing an arbitrary function</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#gaussian-process-regression-given-historical-data">Gaussian Process regression given historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#hyperparameter-optimization-of-a-gaussian-process">Hyperparameter optimization of a Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#all-above-examples-combined">All above examples combined</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#setting-thresholds-for-advertising-units">Setting thresholds for advertising units</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#making-a-pull-request">Making a pull request</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style">Style</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">Frequently Asked Questions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="faq.html#what-license-is-moe-released-under">What license is MOE released under?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#when-should-i-use-moe">When should I use MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#what-is-the-time-complexity-of-moe">What is the time complexity of MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-do-i-cite-moe">How do I cite MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#why-does-moe-take-so-long-to-return-the-next-points-to-sample-for-some-inputs">Why does MOE take so long to return the next points to sample for some inputs?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-do-i-bootstrap-moe-what-initial-data-does-it-need">How do I bootstrap MOE? What initial data does it need?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-many-function-evaluations-do-i-need-before-moe-is-done">How many function evaluations do I need before MOE is &#8220;done&#8221;?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#how-many-function-evaluations-do-i-perform-before-i-update-the-hyperparameters-of-the-gp">How many function evaluations do I perform before I update the hyperparameters of the GP?</a></li>
<li class="toctree-l2"><a class="reference internal" href="faq.html#will-you-accept-my-pull-request">Will you accept my pull request?</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="moe.html">moe package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe">Module contents</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="moe_examples.html">moe_examples package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.combined_example">moe_examples.combined_example module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.hyper_opt_of_gp_from_historical_data">moe_examples.hyper_opt_of_gp_from_historical_data module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.mean_and_var_of_gp_from_historic_data">moe_examples.mean_and_var_of_gp_from_historic_data module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples.next_point_via_simple_endpoint">moe_examples.next_point_via_simple_endpoint module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe_examples.html#module-moe_examples">Module contents</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="cpp_tree.html">C++ Files</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_test.html">gpp_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain_test.html">gpp_domain_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra_test.html">gpp_linear_algebra_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry.html">gpp_geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_gaussian_process.html">gpp_python_gaussian_process</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra-inl.html">gpp_linear_algebra-inl</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">gpp_test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_logging.html">gpp_logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance.html">gpp_covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_test.html">gpp_python_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain.html">gpp_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_common.html">gpp_python_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyperparameter_optimization_demo.html">gpp_hyperparameter_optimization_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils_test.html">gpp_test_utils_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math_test.html">gpp_math_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_expected_improvement.html">gpp_python_expected_improvement</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_model_selection.html">gpp_python_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math.html">gpp_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random_test.html">gpp_random_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimizer_parameters.html">gpp_optimizer_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_demo.html">gpp_expected_improvement_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random.html">gpp_random</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry_test.html">gpp_geometry_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra.html">gpp_linear_algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization_test.html">gpp_heuristic_expected_improvement_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_exception.html">gpp_exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection.html">gpp_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization.html">gpp_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance_test.html">gpp_covariance_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_mock_optimization_objective_functions.html">gpp_mock_optimization_objective_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python.html">gpp_python</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_test.html">gpp_model_selection_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyper_and_EI_demo.html">gpp_hyper_and_EI_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization.html">gpp_heuristic_expected_improvement_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_common.html">gpp_common</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="cpp_tree.html">C++ Files</a> &raquo;</li>
      
    <li>gpp_test_utils</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/gpp_test_utils.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="gpp-test-utils">
<h1>gpp_test_utils<a class="headerlink" href="#gpp-test-utils" title="Permalink to this headline">¶</a></h1>
<p><strong>Contents:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><a class="reference internal" href="#gpp-test-utils-hpp">gpp_test_utils.hpp</a></li>
<li><a class="reference internal" href="#gpp-test-utils-cpp">gpp_test_utils.cpp</a></li>
</ol>
</div></blockquote>
<div class="section" id="gpp-test-utils-hpp">
<h2>gpp_test_utils.hpp<a class="headerlink" href="#gpp-test-utils-hpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><p><p>This file declares functions and classes that are useful for unit testing.  This includes some relative/absolute precision
checks and a few mathematical utilities.</p>
<p>The &#8220;big stuff&#8221; in this file is a class that defines the interface for a pingable function:
PingableMatrixInputVectorOutputInterface
Then there is a PingDerivative() function that can conduct ping testing on any implementer of this interface.</p>
<p>There&#8217;s also a mock environment class that sets up quantities commonly needed by tests of GP functionality.
Similarly, there is a mock data class that builds up &#8220;history&#8221; (points_sampled, points_sampled_value) by
constructing a GP with random hyperparameters on a random domain.</p>
</p>
<p><p>Test for gpp_test_utils.hpp: utilities that are useful in writing other tests, like pinging
error checking, etc.</p>
 </p>
</p>
<p><p id="project0namespaceboost"><em>namespace</em> <strong>boost</strong></p>
<blockquote>
<div><p></p>
<p></p>
<p><p id="project0classboost_1_1uniform__real"><em>class</em> <strong>uniform_real</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</p>
</div></blockquote>
</p>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<p><p id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface"><em>class</em> <strong>PingableMatrixInputVectorOutputInterface</strong></p>
<blockquote>
<div><p></p>
<p><p>Class to enable numerical and analytic differentiation of functions of the form:
<tt class="docutils literal"><span class="pre">f_{k}</span> <span class="pre">=</span> <span class="pre">f(X_{d,i})</span></tt>
with derivatives taken wrt each member of <tt class="docutils literal"><span class="pre">X_{d,i}</span></tt>,
<tt class="docutils literal"><span class="pre">gradf_{k,d,i}</span> <span class="pre">=</span> <span class="pre">\frac{\partial</span> <span class="pre">f_k}{\partial</span> <span class="pre">X_{d,i}}</span></tt>
In the nomenclature used in the class:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">d</span></tt> indexes over num_rows (set in GetInputSizes())</li>
<li><tt class="docutils literal"><span class="pre">i</span></tt> indexes over num_cols (set in GetInputSizes())</li>
<li><tt class="docutils literal"><span class="pre">k</span></tt> indexes over GetOutputSize()</li>
</ul>
<p>Typically <tt class="docutils literal"><span class="pre">d</span></tt> is the spatial_dimension of the problem.  So if <tt class="docutils literal"><span class="pre">i</span></tt> ranges over <tt class="docutils literal"><span class="pre">1</span> <span class="pre">..</span> <span class="pre">num_points</span></tt>,
then <tt class="docutils literal"><span class="pre">X_{d,i}</span></tt> is a matrix of num_points points each with dimension spatial_dim.
And <tt class="docutils literal"><span class="pre">k</span></tt> refers to num_outputs.</p>
<p><tt class="docutils literal"><span class="pre">X_{d,i}</span></tt> can of course be any arbitrary matrix; <tt class="docutils literal"><span class="pre">d,</span> <span class="pre">i</span></tt> need not refer to spatial dimension and num_points.  But
that is currently the most common use case.</p>
<p>This class enables easy pinging of a multitude of <tt class="docutils literal"><span class="pre">f,</span> <span class="pre">X</span></tt> combinations.  Since it abstracts away indexing, it does not limit
how implementations store/compute <tt class="docutils literal"><span class="pre">f()</span></tt> and its gradient.</p>
<p>Generally, usage goes as follows:</p>
<ul class="simple">
<li>Use GetInputSizes(), GetOutputSize(), and possibly GetGradientsSize() to inspect the dimensions of the problem</li>
<li>EvaluateAndStoreAnalyticGradient(): compute and internally store the gradient evaluated at a given input*</li>
<li>GetAnalyticGradient: returns the value of the analytic gradient for a given output (<tt class="docutils literal"><span class="pre">k</span></tt>), wrt a given point <tt class="docutils literal"><span class="pre">(d,i)</span></tt></li>
<li>EvaluateFunction: returns all outputs of the function for a given input</li>
</ul>
<p>* It is not necessary to fully evaluate the gradient here.  Instead, the input point can be stored and evaluation can
happen on-the-fly in GetAnalyticGradient() if desired.</p>
<p>So to ping a derivative, you can:
<tt class="docutils literal"><span class="pre">f_p</span> <span class="pre">=</span> <span class="pre">EvaluateFunction(X</span> <span class="pre">+</span> <span class="pre">h)</span></tt>, <tt class="docutils literal"><span class="pre">f_m</span> <span class="pre">=</span> <span class="pre">EvaluateFunction(X</span> <span class="pre">-</span> <span class="pre">h)</span></tt>
Compare:
<tt class="docutils literal"><span class="pre">(f_p</span> <span class="pre">-</span> <span class="pre">f_m)/(2h)</span></tt>
to
GetAnalyticGradient
See PingDerivative() docs for more details.</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1aae2e0fa3e2ab70de89abac30458f8b52"></span><div class="line-block">
<div class="line">void <strong>GetInputSizes</strong>(int * num_rows, int * num_cols)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Number of rows and columns of the input, <tt class="docutils literal"><span class="pre">X_{d,i}</span></tt>, to <tt class="docutils literal"><span class="pre">f()</span></tt>.</p>
<p>For example, the input might be a <tt class="docutils literal"><span class="pre">N_d</span> <span class="pre">X</span> <span class="pre">N_i</span></tt> matrix, <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>, where <tt class="docutils literal"><span class="pre">N_d</span></tt> = spatial dimension (rows)
and <tt class="docutils literal"><span class="pre">N_i</span></tt> = number of points (columns).</p>
<dl class="docutils">
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">num_rows[1]:</th><td class="field-body">the number of rows of the input matrix <tt class="docutils literal"><span class="pre">X</span></tt></td>
</tr>
<tr class="field-even field"><th class="field-name">num_cols[1]:</th><td class="field-body">the number of columns of the input matrix <tt class="docutils literal"><span class="pre">X</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1a618a890a7f5e9059b88499963277bc96"></span><div class="line-block">
<div class="line">int <strong>GetOutputSize</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Number of outputs of the function <tt class="docutils literal"><span class="pre">f_k</span> <span class="pre">=</span> <span class="pre">f(X_{d,i})</span></tt>; i.e., <tt class="docutils literal"><span class="pre">length(f_k)</span></tt></p>
<dl class="docutils">
<dt><strong>Returns</strong>:</dt>
<dd>The number of entries in <tt class="docutils literal"><span class="pre">f_k</span></tt> aka number of outputs of <tt class="docutils literal"><span class="pre">f()</span></tt></dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1afe065587368c70b5de0da86792f72f3c"></span><div class="line-block">
<div class="line">int <strong>GetGradientsSize</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Number of entries in the gradient of the output wrt each entry of the input.</p>
<p>This should generally not be used unless you require direct access to the analytic gradient.</p>
<dl class="docutils">
<dt><strong>Returns</strong>:</dt>
<dd>MUST be <tt class="docutils literal"><span class="pre">num_rows*num_cols*GetOutputSize()</span></tt> invalid memory read/writes may occur</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1a238a21daa3611c30580534a90bf75534"></span><div class="line-block">
<div class="line">void <strong>EvaluateAndStoreAnalyticGradient</strong>(double const *restrict input_matrix, double *restrict gradients)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Setup so that GetAnalyticGradient(row, column, output) will be able to return
<tt class="docutils literal"><span class="pre">gradf[row][column][output]</span></tt> evaluated at <tt class="docutils literal"><span class="pre">X</span></tt>, &#8220;input_matrix.&#8221;</p>
<p>Typically this will entail computing and storing the analytic gradient.  But the only thing that needs
to be saved is the contents of input_matrix for later access.</p>
<p>MUST BE CALLED before using GetAnalyticGradient!</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">input_matrix[num_rows][num_cols]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the input, <tt class="docutils literal"><span class="pre">X_{d,i}</span></tt></td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gradients[num_rows][num_cols][num_outputs]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">filled with the gradient evaluated at
input_matrix.  Ignored if nullptr.  IMPLEMENTATION NOT REQUIRED.</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1a3d99731d1cb8b1d0d1d4bb85338dabba"></span><div class="line-block">
<div class="line">double <strong>GetAnalyticGradient</strong>(int row_index, int column_index, int output_index)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>The gradients are indexed by: <tt class="docutils literal"><span class="pre">dA[input_row][input_column][output_index]</span></tt>, where row, column index
the input matrix and output_index indexes the output.</p>
<p>Returns the gradient computed/stored by EvaluateAndStoreAnalyticGradient().</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">row_index:</th><td class="field-body">row_index (<tt class="docutils literal"><span class="pre">d</span></tt>) of the input to be differentiated with respect to</td>
</tr>
<tr class="field-even field"><th class="field-name">column_index:</th><td class="field-body">column_index (<tt class="docutils literal"><span class="pre">i</span></tt>) of the input to be differentiated with respect to</td>
</tr>
<tr class="field-odd field"><th class="field-name">output_index:</th><td class="field-body">(flat) index into the output</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>The <tt class="docutils literal"><span class="pre">[row_index][column_index][output_index]'th</span></tt> entry of the analytic gradient evaluated at input_matrix (where
input matrix was specified in EvaluateAndStoreAnalyticGradient()).</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1af5990c6775d5cfca73692a544f146e3a"></span><div class="line-block">
<div class="line">void <strong>EvaluateFunction</strong>(double const *restrict input_matrix, double *restrict function_values)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Evalutes <tt class="docutils literal"><span class="pre">f_k</span> <span class="pre">=</span> <span class="pre">f(X_{d,i})</span></tt>.  <tt class="docutils literal"><span class="pre">X_{d,i}</span></tt> is the &#8220;input_matrix&#8221; and <tt class="docutils literal"><span class="pre">f_k</span></tt> is in &#8220;function_values.&#8221;</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">input_matrix[num_rows][num_cols]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the matrix of inputs</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">function_values[num_outputs]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">vector of outputs of <tt class="docutils literal"><span class="pre">f()</span></tt></td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1a9e1d28ce8619c73a680989bc2756ed40"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface"><em>PingableMatrixInputVectorOutputInterface</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Protected Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1a663c67662a7a6a971aaf0123efa865e2"></span><div class="line-block">
<div class="line"> <strong>PingableMatrixInputVectorOutputInterface</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface_1adeae44f38006ae39e4f4afbc9544b909"></span><div class="line-block">
<div class="line"> <strong>~PingableMatrixInputVectorOutputInterface</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_mock_expected_improvement_environment"><em>class</em> <strong>MockExpectedImprovementEnvironment</strong></p>
<blockquote>
<div><p></p>
<p><p>Class to conveniently hold and generate random data that are commonly needed for testing functions in gpp_math.cpp.  In
particular, this mock is used for testing GP mean, GP variance, and expected improvement (and their gradients).</p>
<p>This class holds arrays: <tt class="docutils literal"><span class="pre">points_sampled</span></tt>, <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt>, <tt class="docutils literal"><span class="pre">points_to_sample</span></tt>, and <tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>
which are sized according to the parameters specified in Initialize(), and filled with random numbers.</p>
<p>TODO(GH-125): we currently generate the point sets by repeated calls to rand().  This is generally
unwise since the distribution of points is not particularly random.  Additionally, our current covariance
functions are all stationary, so we would rather generate a random base point <tt class="docutils literal"><span class="pre">x</span></tt>, and then a random
(direction, radius) pair so that <tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x</span> <span class="pre">+</span> <span class="pre">direction*radius</span></tt>. We better cover the different behavioral
regimes of our code in this case, since it&#8217;s the radius value that actually correlates to results.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1abc0846411620ad518bf34ae22d1bc181"></span>typedef UniformRandomGenerator::EngineType <strong>EngineType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ae46330f988942c0e7ad421d95fda54e3"></span><div class="line-block">
<div class="line"> <strong>MockExpectedImprovementEnvironment</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Construct a MockExpectedImprovementEnvironment and set invalid values for all size parameters
(so that Initialize must be called to do anything useful) and pre-allocate some space.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a52bd3bfc093508ad680d4849b58afceb"></span><div class="line-block">
<div class="line">void <strong>Initialize</strong>(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>(Re-)initializes the data data in this function: this includes space allocation and random number generation.</p>
<p>If any of the size parameters are changed from their current values, space will be realloc&#8217;d.
Then it re-draws another set of uniform random points (in [-5, 5]) for the member arrays
points_sampled, points_sampled_value, points_to_sample, and points_being_sampled.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the spatial dimension of a point (i.e., number of independent params in experiment)</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of points to be sampled in future experiments</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">num_being_sampled:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">number of points being sampled concurrently</td>
</tr>
<tr class="field-even field"><th class="field-name">num_sampled:</th><td class="field-body">number of already-sampled points</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a22e85f71db1244a0fdd9ec44c4d646a4"></span><div class="line-block">
<div class="line">void <strong>Initialize</strong>(int dim_in, int num_to_sample_in, int num_being_sampled_in, int num_sampled_in, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a145abcc2dee50e16ae146abfac91a791"></span><div class="line-block">
<div class="line">double * <strong>points_sampled</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a34fdbc12b6b6c45f3933fa05cca2283e"></span><div class="line-block">
<div class="line">double * <strong>points_sampled_value</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a48ea3a4642da4b0a4bf49f5fb6f5d2ca"></span><div class="line-block">
<div class="line">double * <strong>points_to_sample</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ab22a886abb65a514106fa0ed8f47f7e1"></span><div class="line-block">
<div class="line">double * <strong>points_being_sampled</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ae813b6752bf6039696170cc27a9a399a"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_mock_expected_improvement_environment"><em>MockExpectedImprovementEnvironment</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a58ade07993ea2973e10ba383c6bf2e00"></span>int <strong>dim</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of points_sampled) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a63a0ffcfccc159dc4bf8dac65232d1de"></span>int <strong>num_sampled</strong></p>
<blockquote>
<div><p>number of points in points_sampled (history) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ac7e71fce85c9fd2a459262d90c81bf7e"></span>int <strong>num_to_sample</strong></p>
<blockquote>
<div><p>number of points to be sampled in future experiments (i.e., the q in q,p-EI) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a6bda18ee183ed724da1d5df33955d122"></span>int <strong>num_being_sampled</strong></p>
<blockquote>
<div><p>number of points currently being sampled (i.e., the p in q,p-EI) </p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Static Attributes</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ad0e638620f0873e2fdefdddc20ba59b1"></span>constexpr EngineType::result_type <strong>kDefaultSeed</strong></p>
<blockquote>
<div><p>default seed for repeatability in testing </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a691f587aea22c681ee51cf5982840084"></span>constexpr double <strong>range_min</strong></p>
<blockquote>
<div><p>minimum coordinate value </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1afcbc2126d71e9ff718740356e7bd31b0"></span>constexpr double <strong>range_max</strong></p>
<blockquote>
<div><p>maximum coordinate value </p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a6a7876be37cd27893441d28ae9f89dda"></span>std::vector&lt; double &gt; <strong>points_sampled_</strong></p>
<blockquote>
<div><p>coordinates of already-sampled points, <tt class="docutils literal"><span class="pre">X</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1ad3f89cc5c64d18b67530211b099e6fb8"></span>std::vector&lt; double &gt; <strong>points_sampled_value_</strong></p>
<blockquote>
<div><p>function values at points_sampled, <tt class="docutils literal"><span class="pre">y</span></tt> </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a7ede8d188d94e60e2fd4bf642707354c"></span>std::vector&lt; double &gt; <strong>points_to_sample_</strong></p>
<blockquote>
<div><p>points to be sampled in experiments (i.e., the q in q,p-EI) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a4192430a7f825b9b72e3b2b3132540de"></span>std::vector&lt; double &gt; <strong>points_being_sampled_</strong></p>
<blockquote>
<div><p>points being sampled in concurrent experiments (i.e., the p in q,p-EI) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a7e6d95a95716357126af88fd53f2f477"></span><a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a> <strong>uniform_generator_</strong></p>
<blockquote>
<div><p>uniform random number generator for generating coordinates </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_mock_expected_improvement_environment_1a1168e3a4dfad600aa1cfbacc2023573c"></span><a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; <strong>uniform_double_</strong></p>
<blockquote>
<div><p>distribution over <tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> that coordinate values lie in </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data"><em>class</em> <strong>MockGaussianProcessPriorData</strong></p>
<blockquote>
<div><p></p>
<p><p>Struct to generate data randomly from a GaussianProcess. This object contains the generated GaussianProcess
as well as the inputs needed to generate it (e.g., hyperparameters, domain, etc).</p>
<p>This struct is intended for convenience (so that test writers do not need to repeat these lines in every test
that builds its input data from a GP). It has a fire-and-forget constructor that builds all fields
randomly, but it also exposes all internal state/functions used by that ctor so that power users
can customize their test scenarios further.</p>
<p>Implementation: this object uses std::unique_ptr to hide complex object definitions in the corresponding cpp file</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a62a6ce6a1efb7739abfef2aa9f97d468"></span><div class="line-block">
<div class="line"> <strong>MockGaussianProcessPriorData</strong>(const  <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  &amp; covariance, const std::vector&lt; double &gt; &amp; noise_variance_in, int dim_in, int num_sampled_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Construct an empty MockGaussianProcessPriorData. Member int, double, and vectors are
initialized appropriately. covariance_ptr is cloned from covariance.
BUT domain_ptr and gaussian_process_ptr ARE NOT initialized. Use this class&#8217;s member
functions to properly initialize these more complex data.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">covariance:</th><td class="field-body">the CovarianceInterface object encoding assumptions about the GP&#8217;s behavior on our data</td>
</tr>
<tr class="field-even field"><th class="field-name">noise_variance:</th><td class="field-body">the <tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt> (noise variance) associated w/observation, i-th entry will be associated with the i-th point generated by the GP</td>
</tr>
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the spatial dimension of a point (i.e., number of independent params in experiment)</td>
</tr>
<tr class="field-even field"><th class="field-name">num_sampled:</th><td class="field-body">number of already-sampled points (that we want the GP to hold)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a0cd419fe647014bae72e7c4c26ec8617"></span><div class="line-block">
<div class="line"> <strong>MockGaussianProcessPriorData</strong>(const  <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  &amp; covariance, const std::vector&lt; double &gt; &amp; noise_variance_in, int dim_in, int num_sampled_in, const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_domain_lower, const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_domain_upper, const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_hyperparameters, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Completely constructs a MockGaussianProcessPriorData, initializing all fields.
Builds a GP based on a randomly generated domain and hyperparameters.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">covariance:</th><td class="field-body">the CovarianceInterface object encoding assumptions about the GP&#8217;s behavior on our data</td>
</tr>
<tr class="field-even field"><th class="field-name">noise_variance:</th><td class="field-body">the <tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt> (noise variance) associated w/observation, i-th entry will be associated with the i-th point generated by the GP</td>
</tr>
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the spatial dimension of a point (i.e., number of independent params in experiment)</td>
</tr>
<tr class="field-even field"><th class="field-name">num_sampled:</th><td class="field-body">number of already-sampled points (that we want the GP to hold)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_domain_lower:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw domain lower bounds</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_double_domain_upper:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw domain upper bounds</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_hyperparameters:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw hyperparameters</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a5c85b4f61053e3bb5a66566a3a07cac5"></span><div class="line-block">
<div class="line"> <strong>~MockGaussianProcessPriorData</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Prevent inline destructor: the dtor of std::unique_ptr&lt;T&gt; needs access to T&#8217;s dtor (b/c
unique_ptr&#8217;s dtor basically calls delete on T*). But we want to forward-declare all of our
T objects, so the dtor must be defined in the cpp file where those defintions are visible.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a7bef8667f1cafab1d3d336522a254a39"></span><div class="line-block">
<div class="line">void <strong>InitializeHyperparameters</strong>(const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_hyperparameters, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Sets hyperparameters of covariance with random draws from the specified interval.
Modifies: hyperparameters, covariance_ptr</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_hyperparameters:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw hyperparameters</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a13d28b3559285cc46fe6081f9b281e0f"></span><div class="line-block">
<div class="line">void <strong>InitializeDomain</strong>(const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_domain_lower, const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_domain_upper, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Sets the domain from which the GP&#8217;s historical data will be generated. For each dimension,
we draw <tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> bounds (domain_bounds); then we construct a domain object.
Modifies: domain_bounds, domain_ptr</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_domain_lower:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw domain lower bounds</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_double_domain_upper:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body"><tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> range from which to draw domain upper bounds</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a994b275757d1cde9be7fd083f09fefbc"></span><div class="line-block">
<div class="line">void <strong>InitializeGaussianProcess</strong>(<a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Builds a GaussianProcess with num_sampled points (drawn randomly from the domain) whose
values are drawn randomly from the GP (sampled one at a time and added to the prior).
Users MUST call InitializeHyperparameters() and InitializeDomain() (or otherwise initialize
hyperparameters and domain_ptr) before calling this function.
Modifies: covariance_ptr, best_so_far, gaussian_procss_ptr</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to random draws</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a8a2099dce947d2cad25a8139c88bc7a2"></span>int <strong>dim</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., entries per point of points_sampled) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1aa72bb012f18fb702d4847c4c7326de8a"></span>int <strong>num_sampled</strong></p>
<blockquote>
<div><p>number of points in points_sampled (history) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a454dc42f50804cadb5f29cb22cc99e62"></span>std::vector&lt;  <a class="reference internal" href="gpp_geometry.html#project0structoptimal__learning_1_1_closed_interval"><em>ClosedInterval</em></a>  &gt; <strong>domain_bounds</strong></p>
<blockquote>
<div><p>set of <tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt> bounds for the coordinates of each dimension </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a8910746cf6e0bf5361525500bba9dbef"></span>std::unique_ptr&lt; DomainType &gt; <strong>domain_ptr</strong></p>
<blockquote>
<div><p>the domain the GP will live in </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a0886273e48708e2858de019c916ec203"></span>std::unique_ptr&lt;  <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  &gt; <strong>covariance_ptr</strong></p>
<blockquote>
<div><p>covariance class (for computing covariance and its gradients); encodes assumptions about GP&#8217;s behavior on data </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a78f29cd4c76c1d65a214eee1cef13e9a"></span>std::vector&lt; double &gt; <strong>hyperparameters</strong></p>
<blockquote>
<div><p>hyperparameters of the covariance function </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a2cc90ab69b1f0833e54a01f7ea108e87"></span>std::vector&lt; double &gt; <strong>noise_variance</strong></p>
<blockquote>
<div><p><tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt>, the noise variance </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1ad58c6df728049f5543079ef6fd5f0264"></span>double <strong>best_so_far</strong></p>
<blockquote>
<div><p>lowest function value seen so far </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_mock_gaussian_process_prior_data_1a6637ed98416882b49c2c7b976004ca7a"></span>std::unique_ptr&lt;  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &gt; <strong>gaussian_process_ptr</strong></p>
<blockquote>
<div><p>the GP constructed by this object, using points sampled from domain and the stored covariance </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div></blockquote>
</p>
</div>
<div class="section" id="gpp-test-utils-cpp">
<h2>gpp_test_utils.cpp<a class="headerlink" href="#gpp-test-utils-cpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><p>Implementations of utitilies useful for unit testing.</p>
 </p>
<em>Defines</em><blockquote>
<div><p><span class="target" id="project0gpp__test__utils_8cpp_1a108c0140270b524f13500dfc29e694e6"></span><strong>OL_PING_TEST_DEBUG_PRINTF</strong>(...)</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Functions</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1a8df0aa011ffe9ea21befe72ff9cbce64"></span><div class="line-block">
<div class="line">bool <strong>CheckIntEquals</strong>(int64_t value, int64_t truth)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Checks if <tt class="docutils literal"><span class="pre">|value</span> <span class="pre">-</span> <span class="pre">truth|</span> <span class="pre">==</span> <span class="pre">0</span></tt></p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">value:</th><td class="field-body">number to be tested</td>
</tr>
<tr class="field-even field"><th class="field-name">truth:</th><td class="field-body">the exact/desired result</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>true if value, truth are equal.</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a6d85b76c7a0fbcfc071d2d29fee21b8c"></span><div class="line-block">
<div class="line">double <strong>ResidualNorm</strong>(double const *restrict A, double const *restrict x, double const *restrict b, int size)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p><tt class="docutils literal"><span class="pre">\|b</span> <span class="pre">-</span> <span class="pre">A*x\|_2</span></tt>
The quantity <tt class="docutils literal"><span class="pre">b</span> <span class="pre">-</span> <span class="pre">A*x</span></tt> is called the &#8220;residual.&#8221;  This is meaningful when <tt class="docutils literal"><span class="pre">x</span></tt> is
the solution of the linear system <tt class="docutils literal"><span class="pre">A*x</span> <span class="pre">=</span> <span class="pre">b</span></tt>.  Then having a small residual norm
is a <em>NECESSARY</em> but <em>NOT SUFFICIENT</em> indicator of accuracy in <tt class="docutils literal"><span class="pre">x</span></tt>; that is,
these quantities need not be small simultaneously.  In particular, we know:</p>
<p><tt class="docutils literal"><span class="pre">\|\delta</span> <span class="pre">x\|</span> <span class="pre">/</span> <span class="pre">\|x\|</span> <span class="pre">\le</span> <span class="pre">cond(A)</span> <span class="pre">*</span> <span class="pre">\|r\|</span> <span class="pre">/</span> <span class="pre">(\|A\|</span> <span class="pre">*</span> <span class="pre">\|x\|)</span></tt>
where <tt class="docutils literal"><span class="pre">x</span></tt> is the approximate solution and delta x is the error.  So
<tt class="docutils literal"><span class="pre">\delta</span> <span class="pre">x</span> <span class="pre">=</span> <span class="pre">0</span> <span class="pre">&lt;=&gt;</span> <span class="pre">r</span> <span class="pre">=</span> <span class="pre">0</span></tt> BUT if not identically 0, ||r|| can be much larger.</p>
<p>However, a numerically (backward) stable algorithm will compute solutions
with small relative residual norms <em>regardless</em> of conditioning.  Hence
coupled with knowledge of the particular algorithm for solving <tt class="docutils literal"><span class="pre">A*x</span> <span class="pre">=</span> <span class="pre">b</span></tt>,
residual norm is a valuable measure of correctness.</p>
<p>Suppose <tt class="docutils literal"><span class="pre">x</span></tt> (computed solution) satisfies: <tt class="docutils literal"><span class="pre">(A</span> <span class="pre">+</span> <span class="pre">\delta</span> <span class="pre">A)*x</span> <span class="pre">=</span> <span class="pre">b</span></tt>.  Then:
<tt class="docutils literal"><span class="pre">\|r\|</span> <span class="pre">/</span> <span class="pre">(\|A\|</span> <span class="pre">*</span> <span class="pre">\|x\|)</span> <span class="pre">\le</span> <span class="pre">\|\delta</span> <span class="pre">A\|</span> <span class="pre">/</span> <span class="pre">\|A\|</span></tt>
So large <tt class="docutils literal"><span class="pre">\|r\|</span></tt> indicates a large backward error, implying the linear solver
is not backward stable (and hence should not be used).</p>
</p>
<p><p>Computes <tt class="docutils literal"><span class="pre">||b</span> <span class="pre">-</span> <span class="pre">A*x||_2</span></tt>
The quantity <tt class="docutils literal"><span class="pre">b</span> <span class="pre">-</span> <span class="pre">A*x</span></tt> is called the &#8220;residual.&#8221;  This is meaningful when x is
the solution of the linear system <tt class="docutils literal"><span class="pre">A*x</span> <span class="pre">=</span> <span class="pre">b</span></tt>.</p>
<p>Coupled with knowledge of the underlying algorithm, having a small residual
norm is a useful measure of method correctness.  See implementation documentation
for more details.</p>
<p>This norm is what is minimzied in least squares problems.  However here we
are not working with least squares solutions and require that <tt class="docutils literal"><span class="pre">A</span></tt> is square.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">A[size][size]:</th><td class="field-body">the linear system</td>
</tr>
<tr class="field-even field"><th class="field-name">x[size}:</th><td class="field-body">the solution vector</td>
</tr>
<tr class="field-odd field"><th class="field-name">b[size]:</th><td class="field-body">the RHS vector</td>
</tr>
<tr class="field-even field"><th class="field-name">size:</th><td class="field-body">the dimension of the problem</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>the 2-norm of b-A*x</dd>
</dl>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1aa994e861df161318e2bb293315196c36"></span><div class="line-block">
<div class="line">bool <strong>CheckDoubleWithin</strong>(double value, double truth, double tolerance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Checks if <tt class="docutils literal"><span class="pre">|value</span> <span class="pre">-</span> <span class="pre">truth|</span> <span class="pre">&lt;=</span> <span class="pre">tolerance</span></tt> (error)</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">value:</th><td class="field-body">number to be tested</td>
</tr>
<tr class="field-even field"><th class="field-name">truth:</th><td class="field-body">the exact/desired result</td>
</tr>
<tr class="field-odd field"><th class="field-name">tolerance:</th><td class="field-body">permissible difference</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>true if value, truth differ by no more than tolerance.</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a17833a2fec9cb99b6096b523428d7242"></span><div class="line-block">
<div class="line">bool <strong>CheckDoubleWithinRelative</strong>(double value, double truth, double tolerance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Checks if <tt class="docutils literal"><span class="pre">|value</span> <span class="pre">-</span> <span class="pre">truth|</span> <span class="pre">/</span> <span class="pre">|truth|</span> <span class="pre">&lt;=</span> <span class="pre">tolerance</span></tt> (relative error)</p>
<p>If truth = 0.0, CheckDoubleWithin() is performed.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">value:</th><td class="field-body">number to be tested</td>
</tr>
<tr class="field-even field"><th class="field-name">truth:</th><td class="field-body">the exact/desired result</td>
</tr>
<tr class="field-odd field"><th class="field-name">tolerance:</th><td class="field-body">permissible relative difference</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>true if value, truth differ relatively by no more than tolerance.</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a8bbccaa783a662f9b1f9dc27afd86e86"></span><div class="line-block">
<div class="line">bool <strong>CheckMatrixNormWithin</strong>(double const *restrict matrix1, double const *restrict matrix2, int size_m, int size_n, double tolerance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Uses the Frobenius Norm for convenience; matrix 2-norms are expensive to compute.</p>
<p><p>Checks that <tt class="docutils literal"><span class="pre">||A</span> <span class="pre">-</span> <span class="pre">B||_F</span> <span class="pre">&lt;=</span> <span class="pre">tolerance</span></tt></p>
<p>Note: the user may want to scale this norm by sqrt(size) because ||I||_F = sqrt(size),
and we may desire that the norm of the idenity be 1.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">matrix1[size_m][size_n]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">matrix A</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">matrix2[size_m][size_n]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">matrix B</td>
</tr>
<tr class="field-odd field"><th class="field-name">size_m:</th><td class="field-body">rows of A, B</td>
</tr>
<tr class="field-even field"><th class="field-name">size_n:</th><td class="field-body">columns of A, B</td>
</tr>
<tr class="field-odd field"><th class="field-name">tolerance:</th><td class="field-body">largest permissible norm of the difference A - B</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>true if A - B are &#8220;close&#8221;</dd>
</dl>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a4a0bd16bb7e2fb31905a436560f95438"></span><div class="line-block">
<div class="line">int <strong>CheckPointsAreDistinct</strong>(double const *restrict point_list, int num_points, int dim, double tolerance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Check whether the distance between every pair of points is larger than tolerance.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">point_list[dim][num_points]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">list of points to check</td>
</tr>
<tr class="field-even field"><th class="field-name">dim:</th><td class="field-body">number of coordinates in each point</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_points:</th><td class="field-body">number of points</td>
</tr>
<tr class="field-even field"><th class="field-name">tolerance:</th><td class="field-body">the minimum allowed distance between points</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>the number of pairs of points whose inter-point distance is less than tolerance</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a027ab5d71d7f3c9c892f012e84dd396f"></span><div class="line-block">
<div class="line">int <strong>PingDerivative</strong>(const  <a class="reference internal" href="#project0classoptimal__learning_1_1_pingable_matrix_input_vector_output_interface"><em>PingableMatrixInputVectorOutputInterface</em></a>  &amp; function_and_derivative_evaluator, double const *restrict points, double epsilon, double rate_tolerance_fine, double rate_tolerance_relaxed, double input_output_ratio)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>Pings the gradient <tt class="docutils literal"><span class="pre">\nabla</span> <span class="pre">f</span></tt> of a function <tt class="docutils literal"><span class="pre">f</span></tt>, using second order finite differences:
<tt class="docutils literal"><span class="pre">grad_f_approximate</span> <span class="pre">=</span> <span class="pre">(</span> <span class="pre">f(x</span> <span class="pre">+</span> <span class="pre">h)</span> <span class="pre">-</span> <span class="pre">f(x</span> <span class="pre">-</span> <span class="pre">h)</span> <span class="pre">)/</span> <span class="pre">(2</span> <span class="pre">*</span> <span class="pre">h)</span></tt>
This converges as <tt class="docutils literal"><span class="pre">O(h^2)</span></tt> (Taylor series expansion) for twice-differentiable functions.
Thus for <tt class="docutils literal"><span class="pre">h_1</span> <span class="pre">!=</span> <span class="pre">h_2</span></tt>, we can compute two <tt class="docutils literal"><span class="pre">grad_f_approximate</span></tt> results and thus two errors, <tt class="docutils literal"><span class="pre">e_1,</span> <span class="pre">e_2</span></tt>.
Then (in exact precision), we would obtain:
<tt class="docutils literal"><span class="pre">log(e_1/e_2)</span> <span class="pre">/</span> <span class="pre">log(h_1/h_2)</span> <span class="pre">&gt;=</span> <span class="pre">2</span></tt>.
(&gt; sign because superconvergence can happen.)
Proof: By taylor-expanding <tt class="docutils literal"><span class="pre">grad_f_approximate</span></tt>, we see that:</p>
<blockquote>
<div><tt class="docutils literal"><span class="pre">`e</span> <span class="pre">=</span> <span class="pre">grad_f_anlytic</span> <span class="pre">-</span> <span class="pre">grad_f_approximate</span> <span class="pre">=</span> <span class="pre">O(h^2)</span> <span class="pre">=</span> <span class="pre">c*h^2</span> <span class="pre">+</span> <span class="pre">H.O.T</span></tt> (Higher Order Terms).</div></blockquote>
<dl class="docutils">
<dt>Assuming <tt class="docutils literal"><span class="pre">H.O.T</span> <span class="pre">\approx</span> <span class="pre">0</span></tt>, then</dt>
<dd><tt class="docutils literal"><span class="pre">e_1/e_2</span> <span class="pre">\approx</span> <span class="pre">c*h_1^2</span> <span class="pre">/</span> <span class="pre">(c*h_2^2)</span> <span class="pre">=</span> <span class="pre">h_1^2</span> <span class="pre">/</span> <span class="pre">h_2^2</span></tt>,</dd>
<dt>and</dt>
<dd><tt class="docutils literal"><span class="pre">log(e_1/e_2)</span> <span class="pre">\approx</span> <span class="pre">log(h_1^2</span> <span class="pre">/</span> <span class="pre">h_2^2)</span> <span class="pre">=</span> <span class="pre">2</span> <span class="pre">*</span> <span class="pre">log(h_1/h_2)</span></tt>.</dd>
<dt>Hence</dt>
<dd><tt class="docutils literal"><span class="pre">rate</span> <span class="pre">=</span> <span class="pre">log(e_1/e_2)</span> <span class="pre">/</span> <span class="pre">log(h_1/h_2)</span> <span class="pre">=</span> <span class="pre">2</span></tt>, <em>in exact precision.</em></dd>
</dl>
<p>If <tt class="docutils literal"><span class="pre">c</span> <span class="pre">=</span> <span class="pre">0</span></tt> (or is very small), then H.O.T. matters: we could have <tt class="docutils literal"><span class="pre">e</span> <span class="pre">=</span> <span class="pre">0*h^2</span> <span class="pre">+</span> <span class="pre">c_1*h^3</span> <span class="pre">+</span> <span class="pre">H.O.T</span></tt>.
And we will compute rates larger than 2 (superconvergence). This is not true in general but it can happen.</p>
<p>This function verifies that the expected convergence rate is obtained with acceptable accuracy in the
presence of floating point errors.</p>
<p>This function knows how to deal with vector-valued <tt class="docutils literal"><span class="pre">f()</span></tt> which accepts a matrix
of inputs, <tt class="docutils literal"><span class="pre">X</span></tt>.  Analytic gradients and finite difference approximations
are computed for each output of <tt class="docutils literal"><span class="pre">f()</span></tt> with respect to each entry in the input, <tt class="docutils literal"><span class="pre">X</span></tt>.  In particular, this
function can handle <tt class="docutils literal"><span class="pre">f()</span></tt>:
<tt class="docutils literal"><span class="pre">f_k</span> <span class="pre">=</span> <span class="pre">f(X_{d,i})</span></tt>
computing gradients:
<tt class="docutils literal"><span class="pre">gradf_{d,i,k}</span> <span class="pre">=</span> <span class="pre">\frac{\partial</span> <span class="pre">f_k}{\partial</span> <span class="pre">X_{i,d}}</span></tt>.
So <tt class="docutils literal"><span class="pre">d,</span> <span class="pre">i</span></tt> index the inputs and <tt class="docutils literal"><span class="pre">k</span></tt> indexes the outputs.</p>
<p>The basic structure is as follows:</p>
<div class="highlight-python"><div class="highlight"><pre>for i
  for d
    # Compute function values and analytic/finite difference gradients for the current
    # input. Do this for each h value.
    # Step 1
    for each h = h_1, h_2
      X_p = X; X_p[d][i] += h
      X_m = X; X_m[d][i] -= h

      f_p = f(X_p); # k values in f
      f_m = f(X_m); # k values in f

      # Loop over outputs and compute difference between finite difference and analytic results.
      for k
        grad_f_analytic   = get_grad_f[d][i][k](X) // d,i,k entry of gradient evaluated at X
        grad_f_finitediff = (f_p[k] - f_m[k])/(2*h)
        error[k] = grad_f_analytic - grad_f_finitediff
      endfor
    endfor

    # Step 2
    for k
      check error, convergence
    endfor
  endfor
endfor
</pre></div>
</div>
<p>Hence all checks for a specific output (<tt class="docutils literal"><span class="pre">k</span></tt>) wrt to a specific point (<tt class="docutils literal"><span class="pre">d,i</span></tt>) happen together.
All dimensions of a given point are grouped together.
Keep this in mind when writing PingableMatrixInputVectorOutputInterface subclasses as it can make reading debugging output easier.</p>
<p>See PingableMatrixInputVectorOutputInterface (and its subclasses) for more information/examples on how <tt class="docutils literal"><span class="pre">f</span></tt> and <tt class="docutils literal"><span class="pre">\nabla</span> <span class="pre">f</span></tt>
can be structured.</p>
<p>Note that in some cases, this function will decide to skip some tests or run them
under more relaxed tolerances.  There are many reasons for this:</p>
<ol class="arabic simple">
<li>When the exact gradient is near 0, finite differencing is simply trying
to compute <tt class="docutils literal"><span class="pre">(x1</span> <span class="pre">-</span> <span class="pre">x2)</span> <span class="pre">=</span> <span class="pre">0</span></tt>, which has an infinite condition number.  If our
method is correct/accurate, we will be able to reasonably closely approximate
0, but we cannot expect convergence.</li>
<li>Backward stability type error analysis deals with normed bounds; for example,
see the discussion in ResidualNorm().  These normed estimates bound the error
on the LARGEST entries. The error in the smaller entries can be much larger.</li>
</ol>
<p>However, even when errors could be large, we&#8217;re trying to compute 0, etc., we do
not want to completely ignore these scenarios since that could cause us to accept
completely bogus outputs.  Instead we try to compensate.</p>
<p>Implementer&#8217;s Note:
This is an &#8220;expert tool.&#8221; I originally implemented it to automate my workflow when implementing and testing
&#8220;math as code&#8221; for gradients. It is a common testing technique from the world of derivative-based optimization
and nonlinear solvers.</p>
<p>Generally, this function is to check at a large number of (random) points in the hopes that most of the points
avoid ill-conditioning issues. Random points are chosen make the implementer&#8217;s life easier.</p>
<p>The typical workflow to implement <tt class="docutils literal"><span class="pre">f(x)</span></tt> and <tt class="docutils literal"><span class="pre">df(x)/dx</span></tt> might look like:</p>
<ol class="arabic simple">
<li>Code <tt class="docutils literal"><span class="pre">f(x)</span></tt></li>
<li>Verify <tt class="docutils literal"><span class="pre">f(x)</span></tt></li>
<li>Analytically compute df/dx (on paper, with a computer algebra system, etc.)</li>
<li>Check <tt class="docutils literal"><span class="pre">df/dx</span></tt><ol class="loweralpha">
<li>at some hand-evaluated points</li>
<li>Ping testing (this function)</li>
</ol>
</li>
</ol>
<p>If errors arise, this function will output some information to provide further context on what input/output
combination failed and how. At the head of this file, define OL_PING_TEST_DEBUG_PRINT to turn on super verbose
printing, which will provide substantially more detail. This is generally useful to see if failing points are just
barely on the wrong side of tolerance or if there is a larger problem afoot.</p>
<p>As it turns out, testing derivative code is no easy undertaking. It is often not a well-conditioned problem, and
the conditioning is difficult to predict, being a function of the input point, function values, and gradient.
The exact value is knowable from the analytic gradient, but here we are trying to ascertain whether the
implementation of the analytic gradient is correct!</p>
<p>Since we do nothing to guarantee good conditioning (random inputs), sometimes our luck is bad. Thus, this function
includes a number of heuristics to detect conditions in which numerical error is too high to make an informed decision
on whether we are looking at noise or a genuine error.</p>
<p>These heuristics involve a number of &#8220;magic numbers&#8221; defined in the code. These depend heavily
on the complexity of the gradient being tested (as well as the points at which gradients are being computed).
Here, we are roughly assuming that the entries of &#8220;points&#8221; are in [1.0e-3, 1.0e1].
The magic numbers here are not perfect, but they appear to work in the current use cases. I have left them
hard-coded for now in the interest of making ping test easier to set up. So, apologies in advance for the
magic numbers.</p>
<p>We chose the bypasses to heavily favor eliminating false positives. Some true positives are <em>also lost</em>.
This is why OL_PING_TEST_DEBUG_PRINTF is important when debugging. If a true positive comes up, then
there are probably many more, which can be viewed by parsing the more detailed output.</p>
<p>WARNING: This function is NOT a catch-all. With the heuristics in place, you could adversarially construct an
implementation that is wrong but passes this test.</p>
<p>WARNING: Additionally, this function cannot detect errors that are not in the gradient. If you intended
to implement <tt class="docutils literal"><span class="pre">f</span> <span class="pre">=</span> <span class="pre">sin(x)</span></tt>, <tt class="docutils literal"><span class="pre">f'</span> <span class="pre">=</span> <span class="pre">cos(x)</span></tt>, but instead coded <tt class="docutils literal"><span class="pre">g</span> <span class="pre">=</span> <span class="pre">sin(x)</span> <span class="pre">+</span> <span class="pre">1</span></tt>, this function will not find it.
This cannot detect small amounts of noise in the gradient either (e.g., <tt class="docutils literal"><span class="pre">f'</span> <span class="pre">=</span> <span class="pre">cos(x)</span> <span class="pre">+</span> <span class="pre">1.0e-15</span></tt>). There is no
way to tell whether that is noise due to numerical error or noise due to incorrectness.</p>
<p>TODO(GH-162): thresholds are an imperfect tool for this task. Loss of precision is not a binary event; you are not
certain at <tt class="docutils literal"><span class="pre">2^{-52}</span></tt> but uncertain at <tt class="docutils literal"><span class="pre">2^{-51}</span></tt>. It might be better to estimate the range over which we go from
meaningful loss of precision to complete noise and have a linear ramp for the tolerance over that space. Maybe
it should be done in log-space?</p>
</p>
<p><p>Checks the correctness of analytic gradient calculations using finite differences.</p>
<p>Since the exact level of error is virtually impossible to compute precisely, we
use finite differences at two different h values (see implementation docs for
details) and check the convergence rate.</p>
<p>Includes logic to skip tests or run at relaxed tolerances when poor
conditioning or loss precision is detected.</p>
<p>In general, this function is meant to be used to test analytic gradient computations on a large
number of random points. The logic to skip tests or relax tolerances is designed to decrease the
false positive rate to 0. Some true positives are lost in the process. So we obtain &#8220;reasonable
certainty&#8221; by testing a large number of points.</p>
<p>If you are implementing/testing new gradients code, please read the function comments for this as well.
This is an &#8220;expert tool&#8221; and not necessarily the most user-friendly at that.</p>
<p>This function produces the most useful debugging output when in PingableMatrixInputVectorOutputInterface,
<tt class="docutils literal"><span class="pre">num_rows</span></tt> = spatial dimension (<tt class="docutils literal"><span class="pre">d</span></tt>)
<tt class="docutils literal"><span class="pre">num_cols</span></tt> = num_points (<tt class="docutils literal"><span class="pre">i</span></tt>)
GetOutputSize() = num_outputs (<tt class="docutils literal"><span class="pre">k</span></tt>)
for functions <tt class="docutils literal"><span class="pre">f_k</span> <span class="pre">=</span> <span class="pre">f(X_{d,i})</span></tt></p>
<p>WARNING: this function generates ~10 lines of output (to stdout) PER FAILURE. If your implementation
is incorrect, expect a large number of lines printed to stdout.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">function_and_derivative_evaluator:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">an object that inherits from
PingableMatrixInputVectorOutputInterface. This object must define all
virtual functions in that interface.</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">points[num_cols][num_rows]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">num_cols points, each with dimension
num_rows. Assumed that the coordinate-wise magnitues are &#8220;around 1.0&#8221;: say [1.0e-3, 1.0e1].</td>
</tr>
<tr class="field-odd field"><th class="field-name">epsilon[2]:</th><td class="field-body"><tt class="docutils literal"><span class="pre">array[h1,</span> <span class="pre">h2]</span></tt> of step sizes to use for finite differencing.
These should not be too small/too large; 5.0e-3, 1.0e-3 are suggested
starting values.
Note that the more ill-conditioned computing f_k is, the larger the tolerances
will need to be.  For example, &#8220;complex&#8221; functions (e.g., many math ops) may
be ill-conditioned.</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">rate_tolerance_fine:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">desired amount of deviation from the exact rate</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">rate_tolerance_relaxed:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum allowable abmount of deviation from the exact rate</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">input_output_ratio:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">for <tt class="docutils literal"><span class="pre">||analytic_gradient||/||input||</span> <span class="pre">&lt;</span> <span class="pre">input_output_ratio</span></tt>, ping testing is not performed
Suggest values around 1.0e-15 to 1.0e-18 (around machine preciscion).</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>The number of gradient entries that failed pinging.  Expected to be 0.</dd>
</dl>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1aa98be88ad6451f04352cdd79258cee68"></span><div class="line-block">
<div class="line">void <strong>ExpandDomainBounds</strong>(double scale_factor, std::vector&lt;  <a class="reference internal" href="gpp_geometry.html#project0structoptimal__learning_1_1_closed_interval"><em>ClosedInterval</em></a>  &gt; * domain_bounds)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Expand each ClosedInterval by scale_factor (outward from their midpoints).</p>
<p>This is handy for testing the constrained EI optimizers. That is, we want a domain which
contains the true optima (b/c it is easier to know that things worked if the gradient is 0)
but not a domain that is so large that constraint satisfaction is irrelevant. Hence we
scale the side-lengths by some &#8220;reasonable&#8221; amount.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">scale_factor:</th><td class="field-body">relative amount by which to expand each ClosedInterval</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">domain_bounds[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">input list of valid ClosedInterval</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">domain_bounds[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">input ClosedIntervals expanded by scale_factor</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1adcf5a3c01f12386d963b1d8486f9ead4"></span><div class="line-block">
<div class="line">void <strong>FillRandomCovarianceHyperparameters</strong>(const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_hyperparameter, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, std::vector&lt; double &gt; * hyperparameters, <a class="reference internal" href="gpp_covariance.html#project0classoptimal__learning_1_1_covariance_interface"><em>CovarianceInterface</em></a>  * covariance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Generates random hyperparameters and sets a covariance object&#8217;s hyperparameters to the new values.</p>
<p>Let <tt class="docutils literal"><span class="pre">n_hyper</span> <span class="pre">=</span> <span class="pre">covariance-&gt;GetNumberOfHyperparameters()</span></tt>;
<strong>Parameters</strong>:</p>
<blockquote>
<div><table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_hyperparameter:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">an uniform range, [min, max], from which to draw the hyperparameters</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">hyperparameters[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">vector initialized to hold n_hyper items</td>
</tr>
<tr class="field-even field"><th class="field-name">covariance[1]:</th><td class="field-body">appropriately initialized covariance object that can accept n_hyper hyperparameters</td>
</tr>
</tbody>
</table>
</div></blockquote>
<dl class="docutils">
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to hyperparameters-&gt;size() random draws</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">hyperparameters[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">vector set to the newly generated hyperparameters</td>
</tr>
<tr class="field-odd field"><th class="field-name">covariance[1]:</th><td class="field-body">covariance object with its hyperparameters set to &#8220;hyperparameters&#8221;</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1af9947e8fe0f02ae4dbd2f607e4d6986b"></span><div class="line-block">
<div class="line">void <strong>FillRandomDomainBounds</strong>(const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_lower_bound, const  <a class="reference internal" href="#project0classboost_1_1uniform__real"><em>boost::uniform_real</em></a> &lt; double &gt; &amp; uniform_double_upper_bound, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, std::vector&lt;  <a class="reference internal" href="gpp_geometry.html#project0structoptimal__learning_1_1_closed_interval"><em>ClosedInterval</em></a>  &gt; * domain_bounds)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Generates a random list of <tt class="docutils literal"><span class="pre">domain_bounds-&gt;size()</span></tt> <tt class="docutils literal"><span class="pre">[min_i,</span> <span class="pre">max_i]</span></tt> pairs such that:
<tt class="docutils literal"><span class="pre">min_i</span> <span class="pre">\in</span> <span class="pre">[uniform_double_lower_bound.a(),</span> <span class="pre">uniform_double_lower_bound.b()]</span></tt>
<tt class="docutils literal"><span class="pre">max_i</span> <span class="pre">\in</span> <span class="pre">[uniform_double_upper_bound.a(),</span> <span class="pre">uniform_double_upper_bound.b()]</span></tt></p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_double_lower_bound:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">an uniform range, <tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt>, from which to draw the domain lower bounds, <tt class="docutils literal"><span class="pre">min_i</span></tt></td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_double_upper_bound:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">an uniform range, <tt class="docutils literal"><span class="pre">[min,</span> <span class="pre">max]</span></tt>, from which to draw the domain upper bounds, <tt class="docutils literal"><span class="pre">max_i</span></tt></td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">domain_bounds[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">vector of ClosedInterval objects to initialize</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">UniformRandomGenerator object will have its state changed due to <tt class="docutils literal"><span class="pre">2*domain_bounds-&gt;size()</span></tt> random draws</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">domain_bounds[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">vector of ClosedInterval objects with their min, max members initialized as described</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1af5f74598a1558bb9d5810c73fe35f316"></span><div class="line-block">
<div class="line">void <strong>FillRandomGaussianProcess</strong>(double const *restrict points_to_sample, double const *restrict noise_variance, int dim, int num_to_sample, double *restrict points_to_sample_value, <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  * gaussian_process)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Utility to draw num_to_sample points from a GaussianProcess and add those values to the prior.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample[dim][num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">points at which to draw/sample from the GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">noise_variance[num_to_sample]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the <tt class="docutils literal"><span class="pre">\sigma_n^2</span></tt> (noise variance) associated w/the new observations, <tt class="docutils literal"><span class="pre">points_sampled_value</span></tt></td>
</tr>
<tr class="field-odd field"><th class="field-name">dim:</th><td class="field-body">the spatial dimension of a point (i.e., number of independent params in experiment)</td>
</tr>
<tr class="field-even field"><th class="field-name">num_to_sample:</th><td class="field-body">number of points add to the GP</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a properly initialized GaussianProcess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">points_to_sample_value[num_to_sample]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">values of the newly sampled points</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">gaussian_process[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the input GP with num_sampled additional points added to it; its internal state is modified to match
the new data; and its internal PRNG state is modified</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpp_logging.html" class="btn btn-neutral float-right" title="gpp_logging"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpp_linear_algebra-inl.html" class="btn btn-neutral" title="gpp_linear_algebra-inl"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2012-2014 Yelp. MOE is licensed under the Apache License, Version 2.0: http://www.apache.org/licenses/LICENSE-2.0.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>