

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gpp_heuristic_expected_improvement_optimization &mdash; MOE 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.1.0 documentation" href="index.html"/>
        <link rel="up" title="C++ Files" href="cpp_tree.html"/>
        <link rel="next" title="gpp_linear_algebra-inl" href="gpp_linear_algebra-inl.html"/>
        <link rel="prev" title="gpp_geometry" href="gpp_geometry.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="moe.html">moe package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe">Module contents</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="cpp_tree.html">C++ Files</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_test.html">gpp_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain_test.html">gpp_domain_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_exception.html">gpp_exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry.html">gpp_geometry</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">gpp_heuristic_expected_improvement_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra-inl.html">gpp_linear_algebra-inl</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils.html">gpp_test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_logging.html">gpp_logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance.html">gpp_covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_test.html">gpp_python_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain.html">gpp_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_common.html">gpp_python_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyperparameter_optimization_demo.html">gpp_hyperparameter_optimization_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils_test.html">gpp_test_utils_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math_test.html">gpp_math_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_expected_improvement.html">gpp_python_expected_improvement</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_model_selection.html">gpp_python_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math.html">gpp_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random_test.html">gpp_random_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_demo.html">gpp_expected_improvement_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random.html">gpp_random</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry_test.html">gpp_geometry_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra.html">gpp_linear_algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization_test.html">gpp_heuristic_expected_improvement_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra_test.html">gpp_linear_algebra_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_core.html">gpp_core</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization.html">gpp_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance_test.html">gpp_covariance_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_gaussian_process.html">gpp_python_gaussian_process</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_mock_optimization_objective_functions.html">gpp_mock_optimization_objective_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python.html">gpp_python</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization.html">gpp_model_selection_and_hyperparameter_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyper_and_EI_demo.html">gpp_hyper_and_EI_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_parameters.html">gpp_optimization_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_common.html">gpp_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization_test.html">gpp_model_selection_and_hyperparameter_optimization_test</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="cpp_tree.html">C++ Files</a> &raquo;</li>
      
    <li>gpp_heuristic_expected_improvement_optimization</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/gpp_heuristic_expected_improvement_optimization.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="gpp-heuristic-expected-improvement-optimization">
<h1>gpp_heuristic_expected_improvement_optimization<a class="headerlink" href="#gpp-heuristic-expected-improvement-optimization" title="Permalink to this headline">¶</a></h1>
<p><strong>Contents:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><a class="reference internal" href="#gpp-heuristic-expected-improvement-optimization-hpp">gpp_heuristic_expected_improvement_optimization.hpp</a></li>
<li><a class="reference internal" href="#gpp-heuristic-expected-improvement-optimization-cpp">gpp_heuristic_expected_improvement_optimization.cpp</a></li>
</ol>
</div></blockquote>
<div class="section" id="gpp-heuristic-expected-improvement-optimization-hpp">
<h2>gpp_heuristic_expected_improvement_optimization.hpp<a class="headerlink" href="#gpp-heuristic-expected-improvement-optimization-hpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><ol class="arabic simple">
<li>FILE OVERVIEW</li>
<li>CODE DESIGN/LAYOUT OVERVIEW:<ol class="loweralpha">
<li>class FunctionValue</li>
<li>class ObjectiveEstimationPolicyInterface<ol class="lowerroman">
<li>ConstantLiarEstimationPolicy</li>
<li>KrigingBelieverEstimationPolicy</li>
</ol>
</li>
<li>function ComputeHeuristicPointsToSample()</li>
</ol>
</li>
</ol>
<p><strong>1 FILE OVERVIEW</strong></p>
<p>Readers should review the header docs for gpp_math.hpp first to understand Gaussian Processes and Expected
Improvement; readers should additionally check gpp_math.cpp file docs for further details.</p>
<p>This file declares classes and functions supporting &#8220;heuristic&#8221; Expected Improvement optimization to solve the
q,0-EI problem. In gpp_math.hpp, methods like ComputeOptimalPointsToSample() solve the q,p-EI problem
using only intrinsic properties of a GaussianProcess. Here, we make additional assumptions about the underlying
objective function&#8217;s behavior, trading accuracy for increased speed.</p>
<p>Together, the ConstantLiar and KrigingBeliever estimation policies with ComputeHeuristicPointsToSample()
implement the heuristics discussed in Ginsbourger 2008. Now we&#8217;ll provide a brief overview of these components;
see the class and function docs for more details.</p>
<p><strong>2 CODE DESIGN/LAYOUT OVERVIEW</strong></p>
<p>Currently, we have:</p>
<p><strong>2a FunctionValue</strong></p>
<p>A simple container class for holding the pair (function_value, noise_variance), representing a measured or
estimated objective function value and the associated noise variance.</p>
<p><strong>2b ObjectiveEstimationPolicyInterface</strong></p>
<p>A simple interface for computing objective function estimates. This supports a single function, ComputeEstimate(),
that estimates the objective function evaluated at a point. It additionally has access to the GaussianProcess
and an iteration counter.</p>
<p><strong>2b, i ConstantLiarEstimationPolicy</strong></p>
<p>The simplest estimation policy, &#8220;Constant Liar&#8221; always returns the same objective function estimate, no matter what.</p>
<p><strong>2b, ii KrigingBelieverEstimationPolicy</strong></p>
<p>Kriging Believer uses some information from the GaussianProcess to produce its estimates. In the basic form
(as used in Ginsbourger 2008), Kriging returns the GP Mean at the evaluation point. We also allow shifting
by some scaling of the GP std deviation.</p>
<p><strong>2c Finally, we discuss performing heuristic EI optimization via ComputeHeuristicPointsToSample()</strong></p>
<p>As with the EI optimizers in gpp_math, this function is templated on domain. This function is responsible for
actually performing the heuristic optimization. It uses ComputeOptimalPointsToSampleWithRandomStarts()
(from gpp_math.hpp) to do this. This function estimates the solution to q-EI using a sequence of q solves
of the 1-EI problem&#8211;cheaper, but potentially/probably inaccurate.</p>
<p>Lastly, note that this template function is explicitly instantiated. There are limited domain choices, and this
lets us hide all the implementation details in the .cpp file (e.g., see the long list of forward declarations).</p>
 </p>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Enums</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1a660d50b78a0fa26cadcaf532b2fae1db"></span><strong>EstimationPolicyTypes enum</strong></p>
<blockquote>
<div><p></p>
<p><p>Enumerating estimation policies for convenience. Useful for dispatching tests.</p>
 </p>
<p><em>Values:</em></p>
<ul class="breatheenumvalues">
<li><tt class="first docutils literal"><span class="pre">kConstantLiar</span></tt><tt class="docutils literal"> <span class="pre">=</span> <span class="pre">=</span> <span class="pre">0</span></tt> - <p><a class="reference internal" href="#project0classoptimal__learning_1_1_constant_liar_estimation_policy"><em>ConstantLiarEstimationPolicy</em></a>. </p>
</li>
<li><tt class="first docutils literal"><span class="pre">kKrigingBeliever</span></tt><tt class="docutils literal"> <span class="pre">=</span> <span class="pre">=</span> <span class="pre">1</span></tt> - <p><a class="reference internal" href="#project0classoptimal__learning_1_1_kriging_believer_estimation_policy"><em>KrigingBelieverEstimationPolicy</em></a>. </p>
</li>
</ul>
</div></blockquote>
</div></blockquote>
<p><p id="project0structoptimal__learning_1_1_function_value"><em>class</em> <strong>FunctionValue</strong></p>
<blockquote>
<div><p></p>
<p><p>Container (POD) to represent the notion of a measured function value with some [Gaussian] uncertainty.
That is, <tt class="docutils literal"><span class="pre">N(\mu,</span> <span class="pre">\sigma)</span></tt>, where <tt class="docutils literal"><span class="pre">\mu</span> <span class="pre">=</span> <span class="pre">function_value</span></tt> and <tt class="docutils literal"><span class="pre">\sigma</span> <span class="pre">=</span> <span class="pre">\sqrt{noise_variance}</span></tt>.</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_function_value_1aa6be1ebbc0743e5c8ab9aa318aeb3ffa"></span><div class="line-block">
<div class="line"> <strong>FunctionValue</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Explicitly defaulted default constructor.
Defining a custom ctor (below) disables the default ctor, so we explicitly default it.
This is needed to maintain POD-ness.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_function_value_1ab949f73d9cf3bf463b63571dedc719fe"></span><div class="line-block">
<div class="line"> <strong>FunctionValue</strong>(double function_value_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a FunctionValue object with the specified function_value and 0 noise.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">function_value_in:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the function_value to hold</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_function_value_1a0dbdf049ce58188e21bfff914efff706"></span><div class="line-block">
<div class="line"> <strong>FunctionValue</strong>(double function_value_in, double noise_variance_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a FunctionValue object with the specified function_value noise_variance.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">function_value_in:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the function_value to hold</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">noise_variance_in:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the noise_variance to hold</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_function_value_1ac96ebf0b8ed440b707f5733344597799"></span>double <strong>function_value</strong></p>
<blockquote>
<div><p>the measured function value being represented </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_function_value_1aa19a5a8405c23dd04aab703c5d9317d6"></span>double <strong>noise_variance</strong></p>
<blockquote>
<div><p>the uncertainty (variance) in the measurement of the function value </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_objective_estimation_policy_interface"><em>class</em> <strong>ObjectiveEstimationPolicyInterface</strong></p>
<blockquote>
<div><p></p>
<p><p>At the moment, ComputeEstimatedSetOfPointsToSample() is the sole consumer of this class. Some of the documentation
here will discuss ObjectiveEstimationPolicyInterface specifically in that light.</p>
<p>He also points out that larger lie values will lead methods like ComputeEstimatedSetOfPointsToSample() to
be more explorative and vice versa.</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_objective_estimation_policy_interface_1af88c15849f5cf4a5994be0925c86d3d8"></span><div class="line-block">
<div class="line"> <strong>~ObjectiveEstimationPolicyInterface</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_objective_estimation_policy_interface_1a5d435bd7802bcdf742c78659d1f217ce"></span><div class="line-block">
<div class="line"><a class="reference internal" href="#project0structoptimal__learning_1_1_function_value"><em>FunctionValue</em></a> <strong>ComputeEstimate</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, double const *restrict point, int iteration)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>The estimate is often computed repeatedly (e.g., see ComputeEstimatedSetOfPointsToSample()); we include
the number of previous calls in the input &#8220;iteration.&#8221; This may be useful if users want to implement an
analogue of &#8220;Constant Liar&#8221; using a fixed distribution of lies, make random draws, etc.</p>
<p>Let dim = gaussian_process.dim()</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the
underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name">point[dim]:</th><td class="field-body">the point at which to compute the estimate</td>
</tr>
<tr class="field-odd field"><th class="field-name">iteration:</th><td class="field-body">the number of previous calls to ComputeEstimate()</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_constant_liar_estimation_policy"><em>class</em> <strong>ConstantLiarEstimationPolicy</strong></p>
<blockquote>
<div><p></p>
<p><p>The &#8220;Constant Liar&#8221; objective function estimation policy is the simplest: it always returns the same value
(Ginsbourger 2008). We call this the &#8220;lie. This object also allows users to associate a noise variance to
the lie value.</p>
<p>In Ginsbourger&#8217;s work, the most common lie values have been the min and max of all previously observed objective
function values; i.e., min, max of <tt class="docutils literal"><span class="pre">GP.points_sampled_value</span></tt>. The mean has also been considered.</p>
<p>He also points out that larger lie values (e.g., max of prior measurements) will lead methods like
ComputeEstimatedSetOfPointsToSample() to be more explorative and vice versa.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into constant_liar_expected_improvement_optimization() in cpp_wrappers/expected_improvement.py.</p>
</div>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1a1d1d21f59e657739cc20e6abd4c2c3fc"></span><div class="line-block">
<div class="line"> <strong>ConstantLiarEstimationPolicy</strong>(double lie_value)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a ConstantLiarEstimationPolicy object with the specified lie_value and 0 noise.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">lie_value:</th><td class="field-body">the &#8220;constant lie&#8221; that this estimator should return</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1a5b9b0209a8fbdeb0617e91c254a7f012"></span><div class="line-block">
<div class="line"> <strong>ConstantLiarEstimationPolicy</strong>(double lie_value, double lie_noise_variance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a ConstantLiarEstimationPolicy object with the specified lie_value and lie_noise_variance.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">lie_value:</th><td class="field-body">the &#8220;constant lie&#8221; that this estimator should return</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">lie_noise_variance:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the <tt class="docutils literal"><span class="pre">noise_variance</span></tt> to associate to the <tt class="docutils literal"><span class="pre">lie_value</span></tt> (MUST be <tt class="docutils literal"><span class="pre">&gt;=</span> <span class="pre">0.0</span></tt>)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1ac36acbb3b8200ccbd55f62650aec47a1"></span><div class="line-block">
<div class="line"><a class="reference internal" href="#project0structoptimal__learning_1_1_function_value"><em>FunctionValue</em></a> <strong>ComputeEstimate</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; OL_UNUSED, double const *restrict  OL_UNUSED, int  OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1a55ccfe0bbe63c49c2122cdc0073e9a4f"></span><div class="line-block">
<div class="line"> <strong>ConstantLiarEstimationPolicy</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1afa201cc251ffc56e8676efbf38728b8d"></span>double <strong>lie_value_</strong></p>
<blockquote>
<div><p>the constant function value estimate this object should return </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_constant_liar_estimation_policy_1ac39a5334f688ae9b2f7f04dc67871ab1"></span>double <strong>lie_noise_variance_</strong></p>
<blockquote>
<div><p>the noise variance to associate to the lie_value </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy"><em>class</em> <strong>KrigingBelieverEstimationPolicy</strong></p>
<blockquote>
<div><p></p>
<p><p>The &#8220;Kriging Believer&#8221; objective function estimation policy uses the Gaussian Process (i.e., the prior)
to produce objective function estimates. The simplest method is to trust the GP completely:
<tt class="docutils literal"><span class="pre">estimate</span> <span class="pre">=</span> <span class="pre">GP.mean(point)</span></tt>
This follows the usage in Ginsbourger 2008. Users may also want the estimate to depend on the GP variance
at the evaluation point, so that the estimate reflects how confident the GP is in the prediction. Users may
also specify std_devation_ceof:
<tt class="docutils literal"><span class="pre">estimate</span> <span class="pre">=</span> <span class="pre">GP.mean(point)</span> <span class="pre">+</span> <span class="pre">std_deviation_coef</span> <span class="pre">*</span> <span class="pre">GP.variance(point)</span></tt>
Note that the coefficient is signed, and analogously to ConstantLiar, larger positive values are more
explorative and larger negative values are more exploitive.</p>
<p>This object also allows users to associate a noise variance to the lie value.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into kriging_believer_expected_improvement_optimization() in cpp_wrappers/expected_improvement.py.</p>
</div>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1aba524ab1f18a4ad99368e36492c3bb54"></span><div class="line-block">
<div class="line"> <strong>KrigingBelieverEstimationPolicy</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a KrigingBelieverEstimationPolicy object whose estimates will only depend on the mean with 0 noise.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1a4f99ac6f2c0e6a39bc9af8066b8bed8f"></span><div class="line-block">
<div class="line"> <strong>KrigingBelieverEstimationPolicy</strong>(double std_deviation_coef)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a KrigingBelieverEstimationPolicy object with the specified std_deviation_coef and 0 noise.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">std_deviation_coef:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the relative amount of bias (in units of GP std deviation) to introduce into the GP mean</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1ac3b756e60b4e383408f432f79e734b96"></span><div class="line-block">
<div class="line"> <strong>KrigingBelieverEstimationPolicy</strong>(double std_deviation_coef, double kriging_noise_variance)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Constructs a KrigingBelieverEstimationPolicy object with the specified std_deviation_coef and 0 noise.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">std_deviation_coef:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">the relative amount of bias (in units of GP std deviation) to introduce into the GP mean</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">kriging_noise_variance:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the <tt class="docutils literal"><span class="pre">noise_variance</span></tt> to associate to each function value estimate (MUST be <tt class="docutils literal"><span class="pre">&gt;=</span> <span class="pre">0.0</span></tt>)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1ac2a86a0aef28fc9a4632a7e8360bd5f5"></span><div class="line-block">
<div class="line"><a class="reference internal" href="#project0structoptimal__learning_1_1_function_value"><em>FunctionValue</em></a> <strong>ComputeEstimate</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, double const *restrict point, int  OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Depending on the use-case, performance could improve if the GaussianProcess were stored as a class member
alongside a matching PointsToSampleState. That said, doing so introduces new issues in maintaining consistency.
It is not a performance concern right now (this function is called infrequently).</p>
</div>
 </p>
</div></blockquote>
</div></blockquote>
<em>Private Members</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1a42b74662e7a9ffdf8438f9892e8e47dd"></span>double <strong>std_deviation_coef_</strong></p>
<blockquote>
<div><p>the relative amount of bias (in units of GP std deviation) to introduce into the GP mean </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_kriging_believer_estimation_policy_1a428c365c9f97c5dcace97bf122dce629"></span>double <strong>kriging_noise_variance_</strong></p>
<blockquote>
<div><p>the noise variance to associate to each KrigingBeliever estimate </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div></blockquote>
</p>
</div>
<div class="section" id="gpp-heuristic-expected-improvement-optimization-cpp">
<h2>gpp_heuristic_expected_improvement_optimization.cpp<a class="headerlink" href="#gpp-heuristic-expected-improvement-optimization-cpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><p>This file contains defintions of expensive ObjectiveEstimationPolicyInterface::ComputeEstimate() functions as well
as the function ComputeHeuristicPointsToSample() which uses these policies to heuristically optimize the
q-EI problem. The idea behind the latter is to make explicit guesses about the behavior of the underlying objective
function (that the GaussianProcess is modeling), which is cheap, instead of using the GP&#8217;s more powerful notion
of the distribution of possible objective function behaviors. That is, instead of taking expectations on the
distribution of objective function behaviors, we simply pick one (through the EstimationPolicy).</p>
<p>Readers should review the header docs for gpp_math.hpp/cpp first to understand Gaussian Processes and Expected
Improvement.</p>
 </p>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Functions</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1accd40b922cedec9d2f8a5882b5c1d7ff"></span><div class="line-block">
<div class="line">template &lt; typename DomainType &gt;</div>
<div class="line">void <strong>ComputeHeuristicPointsToSample</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const DomainType &amp; domain, const  <a class="reference internal" href="#project0classoptimal__learning_1_1_objective_estimation_policy_interface"><em>ObjectiveEstimationPolicyInterface</em></a>  &amp; estimation_policy, double best_so_far, int max_num_threads, bool lhc_search_only, int num_lhc_samples, int num_to_sample, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p><p>This implements a generic tool for heuristically solving the q-EI problem using methods like &#8220;Constant Liar&#8221; or
&#8220;Kriging Believer&#8221; described in Ginsbourger 2008. In a loop, we solve 1-EI (with resulting optima &#8220;point&#8221;), then ask
a heuristic EstimationPolicy to guess the objective function value at &#8220;point&#8221; (in lieu of sampling the real objective by
say, running an [expensive] experiment).</p>
<p>As such, this method is really a fairly loose wrapper around ComputeOptimalPointsToSampleWithRandomStarts() configured
to optimize 1-EI.</p>
<p>Solving q-EI optimally is expensive since this requires monte-carlo evaluation of EI and its gradient. This method
is much cheaper: 1-EI allows analytic computation of EI and its gradient and is fairly easily optimized. So this
heuristic optimizer is cheaper but potentially highly inaccurate, providing no guarantees on the quality of the
best_points_to_sample output.</p>
</p>
<p><p>This function computes a heuristic approximation to the result of ComputeOptimalPointsToSample() with 0 ongoing
experiments (<tt class="docutils literal"><span class="pre">points_being_sampled</span></tt>). Consider this as an alternative when ComputeOptimalPointsToSample() is too expensive.</p>
<p>It heuristically solves the q,0-EI optimization problem. As a reminder, that problem is finding the set of q points
that maximizes the Expected Improvement (saved in the output, best_points_to_sample). Solving for q points simultaneously
usually requires monte-carlo iteration and is expensive. The heuristic here solves q-EI as a sequence of 1-EI problems.
We solve 1-EI, and then we <em>ASSUME</em> an objective function value at the resulting optima. This process is repeated q times.
It is perhaps more clear in pseudocode:</p>
<div class="highlight-python"><div class="highlight"><pre>points_being_sampled = {}  // This stays empty! We are only working with 1,0-EI solves
for i = 0:num_to_sample-1 {
  // First, solve the 1,0-EI problem\*
  new_point = ComputeOptimalPointsToSampleWithRandomStarts(gaussian_process, points_being_sampled, other_parameters)
  // *Estimate* the objective function value at new_point
  new_function_value = ESTIMATED_OBJECTIVE_FUNCTION_VALUE(new_point, other_args)
  new_function_value_noise = ESTIMATED_NOISE_VARIANCE(new_point, other_args)
  // Write the estimated objective values to the GP as *truth*
  gaussian_process.AddPoint(new_point, new_function_value, new_function_value_noise)
  optimal_points_to_sample.append(new_point)
}
</pre></div>
</div>
<p>*Recall: each call to ComputeOptimalPointsToSampleWithRandomStarts() (gpp_math.hpp) kicks off a round of MGD optimization of 1-EI.</p>
<p>Note that ideally the estimated objective function value (and noise) would be measured from the real-world (e.g.,
by running an experiment). Then this algorithm would be optimal. However, the estimate probably is not accurately
representating of the true objective.</p>
<p>The estimation is handled through the &#8220;estimation_policy&#8221; input. Passing a ConstantLiarEstimationPolicy or
KrigingBelieverEstimationPolicy object to this function will produce the &#8220;Constant Liar&#8221; and &#8220;Kriging Believer&#8221;
heuristics described in Ginsbourger 2008. The interface for estimation_policy is generic so users may specify
other estimators as well.</p>
<p>Contrast this approach with ComputeOptimalPointsToSample() (gpp_math.hpp) which solves all outputs of the q,0-EI
problem simultaneously instead of one point at a time. That method is more accurate (b/c it
does not attempt to estimate the behavior of the underlying objective function) but much more expensive (because it
requires monte-carlo iteration).</p>
<p>If <tt class="docutils literal"><span class="pre">num_to_sample</span> <span class="pre">=</span> <span class="pre">1</span></tt>, this is exactly the same as ComputeOptimalPointsToSample(); i.e.,
both methods solve the 1-EI optimization problem the same way.</p>
<p>Currently, during optimization, we recommend that the coordinates of the initial guesses not differ from the
coordinates of the optima by more than about 1 order of magnitude. This is a very (VERY!) rough guideline for
sizing the domain and num_multistarts; i.e., be wary of sets of initial guesses that cover the space too sparsely.</p>
<p>Solution is guaranteed to lie within the region specified by &#8220;domain&#8221;; note that this may not be a
local optima (i.e., the gradient may be substantially nonzero).</p>
<dl class="docutils">
<dt>WARNING: this function fails if any step fails to find improvement! In that case, the best_points output should not be</dt>
<dd>read and found_flag will be false.</dd>
</dl>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">These comments were copied into _heuristic_expected_improvement_optimization() in cpp_wrappers/expected_improvement.py.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">gaussian_process:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">GaussianProcess object (holds points_sampled, values, noise_variance, derived quantities) that describes the
underlying GP</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">optimization_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">GradientDescentParameters object that describes the parameters controlling 1-EI optimization (e.g., number
of iterations, tolerances, learning rate) in each &#8220;outer&#8221; iteration</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">estimation_policy:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the policy to use to produce (heuristic) objective function estimates during q,0-EI optimization</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_so_far:</th><td class="field-body">value of the best sample so far (must be min(points_sampled_value))</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">lhc_search_only:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">whether to ONLY use latin hypercube search (and skip gradient descent EI opt)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_lhc_samples:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of samples to draw if/when doing latin hypercube search</td>
</tr>
<tr class="field-odd field"><th class="field-name">num_to_sample:</th><td class="field-body">how many simultaneous experiments you would like to run (i.e., the q in q,0-EI)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">uniform_generator[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a UniformRandomGenerator object providing the random engine for uniform random numbers</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">found_flag[1]:</th><td class="field-body">true if best_points_to_sample corresponds to a nonzero EI if sampled simultaneously</td>
</tr>
</tbody>
</table>
<p class="last">:uniform_generator[1]:UniformRandomGenerator object will have its state changed due to random draws
:best_points_to_sample[num_to_sample*dim]: point yielding the best EI according to constant liar</p>
</dd>
</dl>
 </p>
</p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a1c9f21837c9affb80abdf0e3b2e3de45"></span><div class="line-block">
<div class="line">template void <strong>ComputeHeuristicPointsToSample</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const  <a class="reference internal" href="gpp_domain.html#project0classoptimal__learning_1_1_tensor_product_domain"><em>TensorProductDomain</em></a>  &amp; domain, const  <a class="reference internal" href="#project0classoptimal__learning_1_1_objective_estimation_policy_interface"><em>ObjectiveEstimationPolicyInterface</em></a>  &amp; estimation_policy, double best_so_far, int max_num_threads, bool lhc_search_only, int num_lhc_samples, int num_to_sample, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a0d0a90c03c706da5fe140d0f9d12691e"></span><div class="line-block">
<div class="line">template void <strong>ComputeHeuristicPointsToSample</strong>(const  <a class="reference internal" href="gpp_math.html#project0classoptimal__learning_1_1_gaussian_process"><em>GaussianProcess</em></a>  &amp; gaussian_process, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; optimization_parameters, const  <a class="reference internal" href="gpp_domain.html#project0classoptimal__learning_1_1_simplex_intersect_tensor_product_domain"><em>SimplexIntersectTensorProductDomain</em></a>  &amp; domain, const  <a class="reference internal" href="#project0classoptimal__learning_1_1_objective_estimation_policy_interface"><em>ObjectiveEstimationPolicyInterface</em></a>  &amp; estimation_policy, double best_so_far, int max_num_threads, bool lhc_search_only, int num_lhc_samples, int num_to_sample, bool *restrict found_flag, <a class="reference internal" href="gpp_random.html#project0structoptimal__learning_1_1_uniform_random_generator"><em>UniformRandomGenerator</em></a>  * uniform_generator, double *restrict best_points_to_sample)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpp_linear_algebra-inl.html" class="btn btn-neutral float-right" title="gpp_linear_algebra-inl"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpp_geometry.html" class="btn btn-neutral" title="gpp_geometry"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Yelp and Cornell Collaboration.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>