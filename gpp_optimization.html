

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>gpp_optimization &mdash; MOE 0.1.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="_static/favicon.ico"/>
  

  
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  
    <link rel="stylesheet" href="_static/breathe.css" type="text/css" />
  
    <link rel="top" title="MOE 0.1.0 documentation" href="index.html"/>
        <link rel="up" title="C++ Files" href="cpp_tree.html"/>
        <link rel="next" title="gpp_covariance_test" href="gpp_covariance_test.html"/>
        <link rel="prev" title="gpp_core" href="gpp_core.html"/> 

  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/modernizr/2.6.2/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-nav-search">
        <a href="index.html" class="fa fa-home"> MOE</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
        
        
            <ul>
<li class="toctree-l1"><a class="reference internal" href="why_moe.html">Why Do We Need MOE?</a><ul>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#what-is-moe">What is MOE?</a></li>
<li class="toctree-l2"><a class="reference internal" href="why_moe.html#why-is-this-hard">Why is this hard?</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="install.html">Install</a><ul>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-in-docker">Install in docker:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#install-from-source">Install from source:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#osx-tips-10-8-for-10-9-see-separate-instructions-below">OSX Tips (&lt;=10.8. For 10.9, see separate instructions below):</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#linux-tips">Linux Tips:</a></li>
<li class="toctree-l2"><a class="reference internal" href="install.html#cmake-tips">CMake Tips:</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="objective_functions.html">Objective Functions</a><ul>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#what-is-an-objective-function">What is an objective function?</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#properties-of-an-objective-function">Properties of an objective function</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#parameters">Parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#id1"><span class="math">\(\Phi\)</span> Objective Functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="objective_functions.html#example-of-objective-functions">Example of Objective Functions</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a><ul>
<li class="toctree-l2"><a class="reference internal" href="examples.html#minimizing-an-arbitrary-function">Minimizing an arbitrary function</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#gaussian-process-regression-given-historical-data">Gaussian Process regression given historical data</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#hyperparameter-optimization-of-a-gaussian-process">Hyperparameter optimization of a Gaussian Process</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#all-above-examples-combined">All above examples combined</a></li>
<li class="toctree-l2"><a class="reference internal" href="examples.html#setting-thresholds-for-advertising-units">Setting thresholds for advertising units</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="contributing.html">Contributing</a><ul>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#making-a-pull-request">Making a pull request</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#documentation">Documentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#testing">Testing</a></li>
<li class="toctree-l2"><a class="reference internal" href="contributing.html#style">Style</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="moe.html">moe package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="moe.html#subpackages">Subpackages</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#submodules">Submodules</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe.resources">moe.resources module</a></li>
<li class="toctree-l2"><a class="reference internal" href="moe.html#module-moe">Module contents</a></li>
</ul>
</li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="cpp_tree.html">C++ Files</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_test.html">gpp_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain_test.html">gpp_domain_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_exception.html">gpp_exception</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry.html">gpp_geometry</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization.html">gpp_heuristic_expected_improvement_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra-inl.html">gpp_linear_algebra-inl</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils.html">gpp_test_utils</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_logging.html">gpp_logging</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance.html">gpp_covariance</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_test.html">gpp_python_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_domain.html">gpp_domain</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_common.html">gpp_python_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyperparameter_optimization_demo.html">gpp_hyperparameter_optimization_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_test_utils_test.html">gpp_test_utils_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math_test.html">gpp_math_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_expected_improvement.html">gpp_python_expected_improvement</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_model_selection.html">gpp_python_model_selection</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_math.html">gpp_math</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random_test.html">gpp_random_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_expected_improvement_demo.html">gpp_expected_improvement_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_random.html">gpp_random</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_geometry_test.html">gpp_geometry_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra.html">gpp_linear_algebra</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_heuristic_expected_improvement_optimization_test.html">gpp_heuristic_expected_improvement_optimization_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_linear_algebra_test.html">gpp_linear_algebra_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_core.html">gpp_core</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="">gpp_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_covariance_test.html">gpp_covariance_test</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python_gaussian_process.html">gpp_python_gaussian_process</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_mock_optimization_objective_functions.html">gpp_mock_optimization_objective_functions</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_python.html">gpp_python</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization.html">gpp_model_selection_and_hyperparameter_optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_hyper_and_EI_demo.html">gpp_hyper_and_EI_demo</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_optimization_parameters.html">gpp_optimization_parameters</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_common.html">gpp_common</a></li>
<li class="toctree-l2"><a class="reference internal" href="gpp_model_selection_and_hyperparameter_optimization_test.html">gpp_model_selection_and_hyperparameter_optimization_test</a></li>
</ul>
</li>
</ul>

        
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="index.html">MOE</a>
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="index.html">Docs</a> &raquo;</li>
      
          <li><a href="cpp_tree.html">C++ Files</a> &raquo;</li>
      
    <li>gpp_optimization</li>
      <li class="wy-breadcrumbs-aside">
        
          <a href="_sources/gpp_optimization.txt" rel="nofollow"> View page source</a>
        
      </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            
  <div class="section" id="gpp-optimization">
<h1>gpp_optimization<a class="headerlink" href="#gpp-optimization" title="Permalink to this headline">¶</a></h1>
<p><strong>Contents:</strong></p>
<blockquote>
<div><ol class="arabic simple">
<li><a class="reference internal" href="#gpp-optimization-hpp">gpp_optimization.hpp</a></li>
</ol>
</div></blockquote>
<div class="section" id="gpp-optimization-hpp">
<h2>gpp_optimization.hpp<a class="headerlink" href="#gpp-optimization-hpp" title="Permalink to this headline">¶</a></h2>
<p></p>
<p><p>Table of Contents:</p>
<ol class="arabic simple">
<li>FILE OVERVIEW</li>
<li>OPTIMIZATION OF OBJECTIVE FUNCTIONS<ol class="loweralpha">
<li>GRADIENT DESCENT<ol class="lowerroman">
<li>OVERVIEW</li>
<li>IMPLEMENTATION DETAILS</li>
</ol>
</li>
<li>NEWTON&#8217;S METHOD<ol class="lowerroman">
<li>OVERVIEW</li>
<li>IMPLEMENTATION DETAILS</li>
</ol>
</li>
<li>MULTISTART OPTIMIZATION</li>
</ol>
</li>
<li>CODE HIERARCHY / CALL-TREE<ol class="loweralpha">
<li>REQUIREMENTS OF TEMPLATE (CLASS) PARAMETERS</li>
<li>CODE HIERARCHY / CALL-TREE FOR THIS FILE<ol class="lowerroman">
<li>OPTIMIZER CLASS TEMPLATE</li>
<li>OPTIMIZER CLASSES IN THIS FILE</li>
<li>MULTISTART OPTIMIZATION</li>
</ol>
</li>
</ol>
</li>
</ol>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments in this header are copied in the module docstring in python/python_version/optimization.py
and in the module docstring in python/cpp_wrappers/optimization.py.</p>
</div>
<p>Read the &#8220;OVERVIEW&#8221; sections for header-style comments that describe the file contents at a high level.
Read the &#8220;IMPLEMENTATION&#8221; comments for cpp-style comments that talk more about the specifics.  Both types
are included together here since this file contains template class declarations and template function definitions.
For further implementation details, see comment blocks before each individual class/function.</p>
<p><strong>1. FILE OVERVIEW</strong></p>
<p>First, the functions in this file are all MAXIMIZERS.  We also use the term &#8220;optima,&#8221; and unless we specifically
state otherwise, &#8220;optima&#8221; and &#8220;optimization&#8221; refer to &#8220;maxima&#8221; and &#8220;maximization,&#8221; respectively.  (Note that
minimizing <tt class="docutils literal"><span class="pre">g(x)</span></tt> is equivalent to maximizing <tt class="docutils literal"><span class="pre">f(x)</span> <span class="pre">=</span> <span class="pre">-1</span> <span class="pre">*</span> <span class="pre">g(x)</span></tt>.)</p>
<p>This file contains templates for some common optimization techniques: gradient descent (GD) and Newton&#8217;s method.
We provide constrained implementations (constraint via heuristics like restricting updates to 50% of the distance
to the nearest wall) of these optimizers.  For unconstrained, just set the domain to be huge: <tt class="docutils literal"><span class="pre">[-DBL_MAX,</span> <span class="pre">DBL_MAX]</span></tt>.</p>
<p>We provide *Optimizer template classes (e.g., NewtonOptimizer) as main endpoints for doing local optimization
(i.e., run the optimization method from a single initial guess).  We also provide a MultistartOptimizer class
for global optimization (i.e., start optimizers from each of a set of initial guesses).  These are all discussed
further below.</p>
<p>All of the optimizers in this file are templated.  The template parameter is an Evaluator type which must have
a type alias, Evaluator::StateType.  So we work with (Evaluator, State) &#8220;tuples.&#8221;  The Evaluator encompasses data
and functions needed to evaluate the objective function, its gradient, and/or its hessian.  The State covers
the mutable state of the evaluation process.  So an Evaluator for <tt class="docutils literal"><span class="pre">f(x)</span> <span class="pre">=</span> <span class="pre">x^T</span> <span class="pre">*</span> <span class="pre">A</span> <span class="pre">*</span> <span class="pre">x</span> <span class="pre">/</span> <span class="pre">||x||_2</span></tt> might contain the code for
matrix-vector and vector-vector multiplication and the data for the matrix <tt class="docutils literal"><span class="pre">A</span></tt>.  f&#8217;s State would contain just the
vector <tt class="docutils literal"><span class="pre">x</span></tt>.  Then <tt class="docutils literal"><span class="pre">f::ComputeObjectiveFunction(state)</span></tt> would have everything it needs to compute <tt class="docutils literal"><span class="pre">f(x)</span></tt>.</p>
<p>In this way, we can make the local and global optimizers completely agonistic to the function they are optimizing.</p>
<p>This Evaluator/State &#8220;idiom&#8221; is decsribed more thoroughly in item 5) in the header comments for gpp_common.hpp.  See
section 3a) of this header (below) for details on precisely what functions/interface a (Evaluator, State) tuple are
required to provide in order to be used with the optimizers in this file.</p>
<p><strong>2. OPTIMIZATION OF OBJECTIVE FUNCTIONS</strong></p>
<p><strong>2a. GRADIENT DESCENT (GD)</strong></p>
<p><strong>2a, i. OVERVIEW</strong></p>
<p>We use first derivative information to walk the path of steepest ascent, hopefully toward a (local) maxima of the
chosen log likelihood measure.  This is implemented in: GradientDescentOptimization().
This method ensures that the result lies within a specified domain.</p>
<p>We additionally restart gradient-descent in practice; i.e., we repeatedly take the output of a GD run and start a
new GD run from that point.  This lives in: GradientDescentOptimizer::Optimize().</p>
<p>Even with restarts, gradient descent (GD) cannot start &#8220;too far&#8221; from the solution and still
successfully find it.  Thus users should typically start it from multiple initial guesses and take the best one
(see gpp_math and gpp_model_selection for examples).  The MultistartOptimizer template class in this file
provides generic multistart functionality.</p>
<p>Gradient descent is implemented in: GradientDescentOptimizer::Optimize() (which calls GradientDescentOptimization())</p>
<p><strong>2a, ii. IMPLEMENTATION DETAILS</strong></p>
<p>GD&#8217;s update is: <tt class="docutils literal"><span class="pre">\theta_{i+1}</span> <span class="pre">=</span> <span class="pre">\theta_i</span> <span class="pre">+</span> <span class="pre">\gamma</span> <span class="pre">*</span> <span class="pre">\nabla</span> <span class="pre">f(\theta_i)</span></tt>
where <tt class="docutils literal"><span class="pre">\gamma</span></tt> controls the step-size and is chosen heuristically, often varying by problem.</p>
<p>The previous update leads to unconstrained optimization.  To ensure that our results always stay within the
specified domain, we additionally limit updates if they would move us outside the domain.  For example,
we could imagine only moving half the distance to the nearest boundary.</p>
<p>With gradient descent (GD), it is hard to know what step sizes to take.  Unfortunately, far enough away from an
optima, the objective could increase (but very slowly).  If gradient descent takes too large of a step in a
bad direction, it can easily &#8220;get lost.&#8221;  At the same time, taking very small steps leads to slow performance.
To help, we take the standard approach of scaling down step size with iteration number. We also allow the user
to specify a maximum relative change to limit the aggressiveness of GD steps.  Finally, we wrap GD in a restart
loop, where we fire off another GD run from the current location unless convergence was reached.</p>
<p><strong>2b. NEWTON&#8217;S METHOD</strong></p>
<p><strong>2b, i. OVERVIEW</strong></p>
<p>Newton&#8217;s Method (for optimization) uses second derivative information in addition to the first derivatives used by
gradient descent (GD). In higher dimensions, first derivatives =&gt; gradients and second derivatives =&gt; Hessian matrix.
At each iteration, gradient descent computes the derivative and blindly takes a step (of some
heuristically determined size) in that direction.  Care must be taken in the step size choice to balance robustness
and speed while ensuring that convergence is possible.  By using second derivative (the Hessian matrix in higher
dimensions), which is interpretable as information about local curvature, Newton makes better* choices about
step size and direction to ensure rapid** convergence.</p>
<p>*, ** See &#8220;IMPLEMENTATION DETAILS&#8221; comments section for details.</p>
<p>Recall that Newton indiscriminately finds solutions where <tt class="docutils literal"><span class="pre">f'(x)</span> <span class="pre">=</span> <span class="pre">0</span></tt>; the eigenvalues of the Hessian classify these
<tt class="docutils literal"><span class="pre">x</span></tt> as optima, saddle points, or indeterminate. We multistart Newton (e.g., gpp_model_selection_and_hyperparameter_optimization)
but just take the best objective value without classifying solutions.
The MultistartOptimizer template class in this file provides generic multistart functionality.</p>
<p>Newton is implemented here: NewtonOptimizer::Optimize() (which calls NewtonOptimization())</p>
<p><strong>2b, ii. IMPLEMENTATION DETAILS</strong></p>
<p>Let&#8217;s address the footnotes from the previous section (Section 2b, i paragraph 1):</p>
<p>* Within its region of attraction, Newton&#8217;s steps are optimal (when we have only second derivative information).  Outside
of this region, Newton can make very poor decisions and diverge.  In general, Newton is more sensitive to its initial
conditions than gradient descent, but it has the potential to be much, much faster.</p>
<p>** By quadratic convergence, we mean that once Newton is near enough to the solution, the log of the error will roughly
halve each iteration.  Numerically, we would see the &#8220;number of digits&#8221; double each iteration.  Again, this only happens
once Newton is &#8220;close enough.&#8221;</p>
<p>Newton&#8217;s Method is a root-finding technique at its base.  To find a root of <tt class="docutils literal"><span class="pre">g(x)</span></tt>, Newton requires an
initial guess, <tt class="docutils literal"><span class="pre">x_0</span></tt>, and the ability to compute <tt class="docutils literal"><span class="pre">g(x)</span></tt> and <tt class="docutils literal"><span class="pre">g'(x)</span></tt>.  Then the idea is that you compute
root of the line tangent to <tt class="docutils literal"><span class="pre">g(x_0)</span></tt>; call this <tt class="docutils literal"><span class="pre">x_1</span></tt>.  And repeat.  But the core idea is to make repeated
linear approximations to <tt class="docutils literal"><span class="pre">g(x)</span></tt> and proceed in a fixed-point like fashion.</p>
<p>As an optimization method, we are looking for roots of the gradient, <tt class="docutils literal"><span class="pre">f'(x_{opt})</span> <span class="pre">=</span> <span class="pre">0</span></tt>.  So we require an initial guess
<tt class="docutils literal"><span class="pre">x_0</span></tt> and the ability to evaluate <tt class="docutils literal"><span class="pre">f'(x)</span></tt> and <tt class="docutils literal"><span class="pre">f''(x)</span></tt> (in higher dimensions, the gradient and Hessian of f).  Thus Newton
makes repeated linear approximations to <tt class="docutils literal"><span class="pre">f'(x)</span></tt> or equivalently, it locally approximates <tt class="docutils literal"><span class="pre">f(x)</span></tt> with a <em>quadratic</em> function,
continuing iteration from the optima of that quadratic.
In particular, Newton would solve the optimization problem of a quadratic program in one iteration.</p>
<p>Mathematically, the update formulas for gradient descent (GD) and Newton are:
GD:     <tt class="docutils literal"><span class="pre">\theta_{i+1}</span> <span class="pre">=</span> <span class="pre">\theta_i</span> <span class="pre">+</span>&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">\gamma</span>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <span class="pre">*</span> <span class="pre">\nabla</span> <span class="pre">f(\theta_i)</span></tt>
Newton: <tt class="docutils literal"><span class="pre">\theta_{i+1}</span> <span class="pre">=</span> <span class="pre">\theta_i</span> <span class="pre">-</span> <span class="pre">H_f^-1(\theta_i)</span> <span class="pre">*</span> <span class="pre">\nabla</span> <span class="pre">f(\theta_i)</span></tt>
Note: the sign of the udpate is flipped because H is <em>negative</em> definite near a maxima.
These update schemes are similar.  In GD, <tt class="docutils literal"><span class="pre">\gamma</span></tt> is chosen heuristically.  There are many ways to proceed but only
so much that can be done with just gradient information; moreover the standard algorithm always proceeds in the direction
of the gradient.  Newton takes a much more general appraoch.  Instead of a scalar <tt class="docutils literal"><span class="pre">\gamma</span></tt>, the Newton update applies
<tt class="docutils literal"><span class="pre">H^-1</span></tt> to the gradient, changing both the direction and magnitude of the step.</p>
<p>Unfortunately, Newton indiscriminately finds solutions where <tt class="docutils literal"><span class="pre">f'(x)</span> <span class="pre">=</span> <span class="pre">0</span></tt>.  This is not necesarily an optima!  In one dimension,
we can have <tt class="docutils literal"><span class="pre">f'(x)</span> <span class="pre">=</span> <span class="pre">0</span></tt> and <tt class="docutils literal"><span class="pre">f''(x)</span> <span class="pre">=</span> <span class="pre">0</span></tt>, in which case the solution need not be an optima (e.g., <tt class="docutils literal"><span class="pre">y</span> <span class="pre">=</span> <span class="pre">x^3</span></tt> at <tt class="docutils literal"><span class="pre">x</span> <span class="pre">=</span> <span class="pre">0</span></tt>).
In higher dimensions, a saddle point can also result (e.g., <tt class="docutils literal"><span class="pre">z</span> <span class="pre">=</span> <span class="pre">x^2</span> <span class="pre">-</span> <span class="pre">y^2</span></tt> at <tt class="docutils literal"><span class="pre">x,y</span> <span class="pre">=</span> <span class="pre">0</span></tt>).  More generally, we have an
optima if the Hessian is strictly negative or positive definite; a saddle if the Hessian has both positive and negative
eigenvalues, and an indeterminate case if the Hessian is singular.</p>
<p><strong>2c. MULTISTART OPTIMIZATION</strong></p>
<p>Above, we mentioned that gradient descent (GD), Newton, etc. have a difficult time converging if they are started &#8220;too far&#8221;
from an optima.  Even if convergence occurs, it will typically be very slow unless the problem is simple.  Worse,
in a problem with multiple optima, the methods may converge to the wrong one!</p>
<p>Multistarting the optimizers is one way of mitigating* this issue.  Multistart involves starting a run of the
specified optimizer (e.g., Newton) from each of a set of initial guesses.  Then the best result is reported as
the result of the whole procedure.  By trying a large number of initial guesses, we potentially reduce the need
for good guesses; i.e., hopefully at least one guess will be &#8220;near enough&#8221; to the global optimum.  This
functionality is provided in MultistartOptimizer::MultistartOptimize(...).</p>
<p>* As noted below in the MultistartOptimizer::MultistartOptimize() function docs, mitigate is intentional here.
Multistarting is NOT GUARANTEED to find global optima.  But it can increase the chances of success.</p>
<p>Currently we let the user specify the initial guesses.  In practice, this typically means a random sampling of points.
We do not (yet) make any effort to say sample more heavily from regions where &#8220;more stuff is happening&#8221; or any
other heuristics.</p>
<p>TODO(GH-165): Improve multistart heuristics.</p>
<p>Finally, MultistartOptimizer::MultistartOptimize() is also used to provide &#8216;dumb&#8217; search functionality (optimization
by just evaluating the objective at numerous points).  For sufficiently complex problems, gradient descent, Newton, etc.
can have exceptionally poor convergence characteristics or run too slowly.  In cases where these more advanced techniques
fail, we commonly fall back to &#8216;dumb&#8217; search.</p>
<p><strong>3. CODE HIERARCHY / CALL-TREE</strong></p>
<p><strong>3a. REQUIREMENTS OF TEMPLATE (CLASS) PARAMETERS</strong></p>
<p>As mentioned in the overview, the functions and classes in this file are all templated on (Evaluator, State) tuples.
They also template on Domain types.  In particular, the optimization functions and classes have the following form:
template &lt;typename ObjectiveFunctionEvaluator, typename Domain&gt; optimization_function(...);
template &lt;typename ObjectiveFunctionEvaluator, typename Domain&gt; OptimizationClass { ... };
State (as indicated in gpp_common.hpp) is obtained through: typename ObjectiveFunctionEvaluator::State.</p>
<p>Domain objects are explained in gpp_domain.hpp; see there for examples as well.  This file directly requires
a Domain object to supply:
void LimitUpdate(double max_relative_change, double const * restrict current_point, double * restrict update_vector);
and with debugging on,
bool CheckPointInside(double const * restrict point);</p>
<p>Now let&#8217;s talk about (Evaluate, State) template parameters and the optimzation classes in this file.  In ADDITION
to the requirements/guidelines laid out in gpp_common.hpp, an (Evaluate, State) tuple MUST provide the following
interface to be used with the optimizers in this file:</p>
<p>Evaluator:</p>
<div class="highlight-python"><div class="highlight"><pre>// these functions all evaluate f() and its derivatives at state.GetCurrentPoint()
// derivatives are computed against the space whose dimension is state.GetProblemSize()
double ComputeObjectiveFunction(State * state);  // compute f(current_point)
void ComputeGradObjectiveFunction(State * state, double * grad_objective);  // compute f&#39;(current_point)
void ComputeHessianObjectiveFunction(State * state, double * hessian_objective);  // compute f&#39;&#39;(current_point)
</pre></div>
</div>
<p>State:</p>
<div class="highlight-python"><div class="highlight"><pre>int GetProblemSize();  // how many dimensions to optimize
void GetCurrentPoint(double * point);  // get current point at which Evalutor is computing results
void UpdateCurrentPoint(double const * point);  // set current point at which Evalutor is computing results
</pre></div>
</div>
<p>gpp_math.hpp and gpp_model_selection_and_hyperparameter_optimization.hpp have (Evaluator, State) examples that implement
the above interface:</p>
<ul class="simple">
<li>gpp_math.hpp:<ul>
<li>(ExpectedImprovement, ExpectedImprovementState)</li>
<li>(OnePotentialSampleExpectedImprovement, OnePotentialSampleExpectedImprovementState)</li>
</ul>
</li>
<li>gpp_model_selection_and_hyperparameter_optimization.hpp:<ul>
<li>(LogMarginalLikelihoodEvaluator, LogMarginalLikelihoodState)</li>
<li>(LeaveOneOutLogLikelihoodEvaluator, LeaveOneOutLogLikelihoodState)</li>
</ul>
</li>
</ul>
<p>gpp_mock_optimization_objective_functions.hpp lays out a pure abstract Evaluator class (with State) that is optimizable;
examine tests that #include that file for more examples.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">not all (Evaluator, State) tuples (e.g., GaussianProcess) make sense for optimization; these classes
do not provide the above interface.</p>
</div>
<p><strong>3b. CODE HIERARCHY / CALL-TREE FOR THIS FILE</strong></p>
<p>First, we will describe the interface for Optimizer classes.  Then we will go over individual classes.
Finally we will touch on the MultistartOptimizer template class.</p>
<p><strong>3b, i. OPTIMIZER CLASS TEMPLATE</strong></p>
<p>As mentioned above, this file provides various Optimizer classes, e.g., NewtonOptimizer.  Here we&#8217;ll go over high
level details and then go through each specific example.</p>
<p>TODO(GH-174): Include objective, param struct, domain, etc. as Optimizer class members (copies or references).</p>
<p>In general, the Optimizer classes are the primary endpoint for local optimization; i.e., you have a good initial guess
and you are confident that the optima is nearby.  In current use cases, this is uncommon.  But when it is true, multistart
will waste a lot of compute time to arrive at the same result.
For global optimization problems, use the MultistartOptimizer::MultistartOptimize() method in conjunction with an
Optimizer object. This will probably remain the more common use case.</p>
<p>But these notes discuss both to provide context for future extensions of both the local and global optimization techniques.</p>
<p>In both cases, it is recommended to wrap Optimizer::Optimize() calls and/or MultistartOptimizer::MultistartOptimize()
calls with code that sets up state, initial guesses, etc. for your specific problem.
See usage in gpp_math and gpp_model_selection.</p>
<p>At present, the an Optimizer class promises:</p>
<div class="highlight-python"><div class="highlight"><pre>template &lt;typename ObjectiveFunctionEvaluator, typename DomainType&gt;
class Optimizer final {
  using ParameterStruct = OptimizerParameters;  // e.g., NewtonParameters
  Optimizer() = default;  // no state so default ctor ok

  int Optimize(const ObjectiveFunctionEvaluator&amp; objective_evaluator, const ParameterStruct&amp; parameters, const DomainType&amp; domain, typename ObjectiveFunctionEvaluator::StateType * objective_state) const noexcept OL_NONNULL_POINTERS;
}
</pre></div>
</div>
<p>The Optimize() member function performs the desired optimization of the objective function specified through
the (Evaluator, State) pair.  Most importantly, the initial guess will be read through
objective_state::GetCurrentPoint().  Upon return, objective_state::GetCurrentPoint() will provide the final
result, the  <tt class="docutils literal"><span class="pre">x</span></tt> for <tt class="docutils literal"><span class="pre">\argmax_x</span> <span class="pre">f(x)</span></tt>, as determined by the optimization process.
Note that since this is generally constrained optimization, in general <tt class="docutils literal"><span class="pre">f'(x)</span> <span class="pre">!=</span> <span class="pre">0</span></tt> (if the optima lies on a boundary).
See specific Optimizer class docs (and ::Optimize() function docs) for details.</p>
<p>Now, we outline specific optimization classes provided in thils file and the multistarting function that uses them:</p>
<p><strong>3b, ii. OPTIMIZER CLASSES IN THIS FILE</strong></p>
<p>class NullOptimizer&lt;ObjectiveFunctionEvaluator, Domain&gt;:
NullOptimizer&lt;...&gt;::Optimize(...) (do nothing)</p>
<blockquote>
<div><ul class="simple">
<li>This optimizer does nothing; it provides an &#8220;identity&#8221; optimizer where Output := Input.</li>
<li>Its purpose is to allow MultistartOptimizer&lt;...&gt; to be used for &#8216;dumb&#8217; searches.</li>
</ul>
</div></blockquote>
<p>class GradientDescentOptimizer&lt;ObjectiveFunctionEvaluator, Domain&gt;:
GradientDescentOptimizer&lt;...&gt;::Optimize(...) (restarted part of gradient descent)</p>
<blockquote>
<div><ul class="simple">
<li>Iteratively restarts GD from its previous ending point, unless convergence conditions are met</li>
<li>This calls:
GradientDescentOptimization&lt;ObjectiveFunctionEvaluator, Domain&gt;()  (gradient descent)<ul>
<li>Performs gradient descent to optimize specified objective function</li>
<li>Ensures (heuristically by modifying steps) that solutions remain in the specified domain</li>
<li>Calls out to ObjectiveFunctionEvaluator::ComputeObjectiveFunction() and ComputeGradObjectiveFunction()</li>
</ul>
</li>
</ul>
</div></blockquote>
<p>class NewtonOptimizer&lt;ObjectiveFunctionEvaluator, Domain&gt;:
NewtonOptimizer&lt;...&gt;::Optimize() (Newton&#8217;s method with refinement step)</p>
<blockquote>
<div><blockquote>
<div><ul class="simple">
<li>First calls NewtonOptimization() to optimize.  Robustness heuristics are active to help make convergence easier.</li>
<li>Then calls NewtonOptimization() again with robustness heuristics off (heuristics should have converged fully
or gotten us close) to ensure that convergence occurred.  (Sometimes the heuristics converge to nonsense;
this second step will catch that.)</li>
<li>This function calls the following twice:
NewtonOptimization&lt;ObjectiveFunctionEvaluator, Domain&gt;() (Newton&#8217;s method for optimization)<ul>
<li>Performs Newton iteration to optimize the templated objective function</li>
<li>Ensures (heuristically by modifying steps) that solutions remain in the specified domain</li>
<li>Calls out to ObjectiveFunctionEvaluator::ComputeObjectiveFunction(), ComputeGradObjectiveFunction(),
and ComputeHessianObjectiveFunction()</li>
<li>Inner loop also calls ComputePLUFactorization() and PLUMatrixVectorSolve() from gpp_linear_algebra</li>
</ul>
</li>
</ul>
</div></blockquote>
<p><strong>3b, iii. MULTISTART OPTIMIZATION</strong>
class MultistartOptimizer&lt;Optimizer&lt;ObjectiveFunctionEvaluator, Domain&gt; &gt;:
MultistartOptimizer&lt;...&gt;::MultistartOptimize() (multistarts any Optimizer from section 3b, ii.)</p>
<blockquote>
<div><ul class="simple">
<li>Calls Optimizer::Optimize() once for each point in the provided list of initial guesses</li>
<li>Multithreaded using OpenMP for performance</li>
<li>Reports the best result overall (and optionally each individual result)</li>
<li>Proxy for finding the global maximum since it is difficult/impossible to guarantee an optimum is global
in general. See function comments (below) and header comments (above, 2c) for details.</li>
</ul>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">uses OptimizationIOContainer class (see declaration below for details) for inputting/outputting
information about currently best-known objective values/points and the optimization result.</p>
</div>
</div></blockquote>
</div></blockquote>
 </p>
<p><p id="project0namespaceoptimal__learning"><em>namespace</em> <strong>optimal_learning</strong></p>
<blockquote>
<div><p></p>
<p></p>
<em>Functions</em><blockquote>
<div><p><span class="target" id="project0namespaceoptimal__learning_1acffe49185559b8bbcdbee2853fbbbece"></span><div class="line-block">
<div class="line">template &lt; typename ObjectiveFunctionEvaluator, typename DomainType &gt;</div>
<div class="line"><a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1a43aad231fa6009d48201a76fd7dfb6dc"><em>OL_NONNULL_POINTERS</em></a>  void <strong>GradientDescentOptimization</strong>(const ObjectiveFunctionEvaluator &amp; objective_evaluator, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a>  &amp; gd_parameters, const DomainType &amp; domain, typename ObjectiveFunctionEvaluator::StateType * objective_state, double *restrict next_point)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Implements gradient-descrent to to find a locally optimal (maximal here) value of the specified objective function.
Additional high-level discussion is provided in section 2a) in the header docs of this file.</p>
<p>Basic gradient descent (GD) to optimize objective function <tt class="docutils literal"><span class="pre">f(x)</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre>input: initial_guess

next_point = initial_guess
i = 0;
while (not converged) {
  direction = derivative of f(x) at next_point
  step_scale = compute step_size scaling: pre_mult * (i+1)^(-gamma)

  next_point += step_scale * direction
  ++i
}
</pre></div>
</div>
<p>So it marches along the direction of largest gradient (so the steepest descent) for some distance.  The distance
is a combination of the size of the gradient and the step_scale factor.  Here, we use an exponentially decreasing
scale to request progressively smaller step sizes: <tt class="docutils literal"><span class="pre">(i+1)^(-gamma)</span></tt>, where <tt class="docutils literal"><span class="pre">i</span></tt> is the iteration number</p>
<p>We do not allow the step to take next_point out of the domain; if this happens, the update is limited.
Thus the solution is guaranteed to lie within the region specified by &#8220;domain&#8221;; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<p>We may also limit very large updates (controlled via max_relative_change).  Decreasing this value
makes gradient descent (GD) more stable but also slower.  For very sensitive problems like hyperparameter
optimization, max_relative_change = 0.02 is suggested; for less sensitive problems
(e.g., EI, especially analytic), you can use 1.0 (or near).</p>
<p>The constraint implementation (no stepping outside the domain) and the large update limiting are not &#8220;pure&#8221; gradient
descent approaches.  They are all heuristics meant to improve Newton&#8217;s robustness.  The constraint implementation
in particular may lead to non-convergence and it also may not find constrained optima that lie exactly on a boundary.  This would
require a more general handling where we search in an <tt class="docutils literal"><span class="pre">d-1</span></tt> dimensional subspace (i.e., only on the boundary).</p>
<p>Note that we are using an absolute tolerance here, based on the size of the most recent step.
The suggested value is 1.0e-7, although this may need to be loosened for problems with &#8216;difficult&#8217; optima (e.g., the shape
is not locally very peaked).  Setting too high of a tolerance can cause wrong answers&#8211;e.g., we stop at a point
that is not an optima but simply an region with small gradient.  Setting the tolerance too low may make convergence impossible;
GD could get stuck (bouncing between the same few points) or numerical effects could make it impossible to satisfy tolerance.</p>
<p>Finally, GD terminates if updates are very small.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">in general, you should not call/instantiate this function directly.  Instead, create a GradientDescentOptimizer object
and call its ::Optimize() function.</p>
</div>
<p>problem_size refers to objective_state-&gt;GetProblemSize(), the number of dimensions in a &#8220;point&#8221; aka the number of
variables being optimized.  (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.)</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">these comments are are copied to GradientDescentOptimizer.optimize() in python/python_version/optimization.py.</p>
</div>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_evaluator:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">reference to object that can compute the objective function and its gradient</td>
</tr>
<tr class="field-even field"><th class="field-name">gd_parameters:</th><td class="field-body">GradientDescentParameters object that describes the parameters controlling gradient descent optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a properly configured state object for the ObjectiveFunctionEvaluator template parameter
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a state object whose temporary data members may have been modified
objective_state.GetCurrentPoint() will return the point yielding the best objective function value
according to gradient descent</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">next_point[problem_size]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">point yielding the best objective function value according to gradient descent, SAME as
objective_state.GetCurrentPoint() (see previous item)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p>TODO(GH-186): The next_point output is redundant with objective_state.GetCurrentPoint(). Remove it.</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0namespaceoptimal__learning_1a4c118ee2a894733f0aacd04a68e34da7"></span><div class="line-block">
<div class="line">template &lt; typename ObjectiveFunctionEvaluator, typename DomainType &gt;</div>
<div class="line"><a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1a43aad231fa6009d48201a76fd7dfb6dc"><em>OL_NONNULL_POINTERS</em></a>   <a class="reference internal" href="gpp_common.html#project0gpp__common_8hpp_1abda439d29ae03a473c1167e47159ae90"><em>OL_WARN_UNUSED_RESULT</em></a>  int <strong>NewtonOptimization</strong>(const ObjectiveFunctionEvaluator &amp; objective_evaluator, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_newton_parameters"><em>NewtonParameters</em></a>  &amp; newton_parameters, const DomainType &amp; domain, typename ObjectiveFunctionEvaluator::StateType * objective_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Uses Newton&#8217;s Method to optimize the value of an objective function, f (e.g., log marginal likelihood).  Newton&#8217;s method is
a root-finding technique, so for optimization, we are searching for points where gradient = 0.
<a class="reference external" href="http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization">http://en.wikipedia.org/wiki/Newton%27s_method_in_optimization</a>
has some basic details.  Additional high-level discussion is provided in section 2b) in the header docs of this file.</p>
<p>Each newton step is given by:
<tt class="docutils literal"><span class="pre">\theta_{n+1}</span> <span class="pre">=</span> <span class="pre">\theta_n</span> <span class="pre">-</span> <span class="pre">\bar{H}_f^-1(\theta_n)</span> <span class="pre">*</span> <span class="pre">\nabla</span> <span class="pre">f(\theta_n)</span></tt>,
where <tt class="docutils literal"><span class="pre">\bar{H}</span> <span class="pre">=</span> <span class="pre">H_f(\theta_n)</span> <span class="pre">-</span> <span class="pre">1/time_factor</span> <span class="pre">*</span> <span class="pre">I</span></tt>, and
<tt class="docutils literal"><span class="pre">\nabla</span> <span class="pre">f</span></tt> and <tt class="docutils literal"><span class="pre">H</span></tt> are the gradient and Hessian of the objective function, respectively,
<tt class="docutils literal"><span class="pre">time_factor</span> <span class="pre">=</span> <span class="pre">time_factor_0</span> <span class="pre">*</span> <span class="pre">gamma^n</span></tt>, and <tt class="docutils literal"><span class="pre">I</span></tt> is the identity matrix.</p>
<p>This method terminates early if a possible solution is found&#8211;<tt class="docutils literal"><span class="pre">||\nabla</span> <span class="pre">f||</span></tt> is sufficiently small.</p>
<p>Choosing a small <tt class="docutils literal"><span class="pre">gamma</span></tt> (e.g., <tt class="docutils literal"><span class="pre">1.0</span> <span class="pre">&lt;</span> <span class="pre">gamma</span> <span class="pre">&lt;=</span> <span class="pre">1.01</span></tt>) and <tt class="docutils literal"><span class="pre">time_factor</span></tt> (e.g., <tt class="docutils literal"><span class="pre">0</span> <span class="pre">&lt;</span> <span class="pre">time_factor</span> <span class="pre">&lt;=</span> <span class="pre">1.0e-3</span></tt>)
leads to more consistent/stable convergence at the cost of slower performance (and in fact
for gamma or time_factor too small, gradient descent is preferred).  Conversely, choosing more
aggressive values may lead to very fast convergence at the cost of more cases failing to
converge.</p>
<p><tt class="docutils literal"><span class="pre">gamma</span> <span class="pre">=</span> <span class="pre">1.01,</span> <span class="pre">time_factor</span> <span class="pre">=</span> <span class="pre">1.0e-3</span></tt> should lead to good robustness at reasonable speed.  This should be a fairly safe default.
<tt class="docutils literal"><span class="pre">gamma</span> <span class="pre">=</span> <span class="pre">1.05,</span> <span class="pre">time_factor</span> <span class="pre">=</span> <span class="pre">1.0e-1</span></tt> will be several times faster but not as robust.</p>
<p>Notice that we modify the Hessian in an attempt to improve the region of &#8220;attraction&#8221;
for Newton.  To do this, we add diagonal dominance to the Hessian: <tt class="docutils literal"><span class="pre">\bar{H}</span> <span class="pre">=</span> <span class="pre">H</span> <span class="pre">-</span> <span class="pre">1/time_factor</span> <span class="pre">*</span> <span class="pre">I</span></tt>.
In the classic/standard version of Newton&#8217;s Method, <tt class="docutils literal"><span class="pre">time_factor</span> <span class="pre">=</span> <span class="pre">\infty</span></tt> so <tt class="docutils literal"><span class="pre">\bar{H}</span></tt> is just the Hessian.
Note: we subtract because we are maximizing the objective and <tt class="docutils literal"><span class="pre">H</span></tt> is strictly negative definite at a maxima.</p>
<p>When <tt class="docutils literal"><span class="pre">\theta_i</span></tt> is far away from <tt class="docutils literal"><span class="pre">\theta_{opt}</span></tt>, having a very small time_factor causes the Newton
update to behave like the Gradient Descent update.  This is because <tt class="docutils literal"><span class="pre">H</span> <span class="pre">-</span> <span class="pre">LARGE_VALUE</span> <span class="pre">*</span> <span class="pre">I</span></tt> makes
<tt class="docutils literal"><span class="pre">H</span></tt> &#8220;look like&#8221; a scaled identity matrix (with a relatively tiny amount of noise added).  Thus
using very large time_factor for many iterations is inefficient: the udpates are gradient-descent-like,
but the cost is 5-7x more (in this case, not in general).  But hoping that
gradient descent-like steps guide us toward into a convergence region for Newton, time_factor
is increased (by a constant factor) each iteration.  As <tt class="docutils literal"><span class="pre">time_Factor</span> <span class="pre">-&gt;</span> <span class="pre">\infinity</span></tt>, <tt class="docutils literal"><span class="pre">\bar{H}</span> <span class="pre">-&gt;</span> <span class="pre">H</span></tt>;
hence we recover Newton&#8217;s fast convergence properties.</p>
<p>Currently, during optimization, we recommend that the coordinates of the initial guesses not differ from the
coordinates of the optima by more than about 1 order of magnitude. This means a grid will probably be necessary
to give a good enough initial guess.</p>
<p>Guaranteed execute AT MOST max_num_restarts newton steps.</p>
<p>We also allow an under/over-relaxation factor, allowing the update to be scaled up/down.</p>
<p>Thus the basic structure for optimizing f(theta) is:</p>
<div class="highlight-python"><div class="highlight"><pre>\theta_i = initial guess
for i = 0:max_iterations {
  compute gradient of f: \nabla f(\theta_i)
  if (||gradient f|| &lt; tolerance) exit
  compute hessian of f: H(\theta_i)

  time_factor = initial * growth_factor^i
  modify hessian: \bar{H} = H - 1/time_factor * I (I is identity matrix)

  compute update: update = \bar{H}^-1 * \nabla f  (performed without forming inverse)

  relax update: update *= relaxation_factor

  apply update: \theta_i = \theta_i - update
}
</pre></div>
</div>
<p>We do not allow the step to take next_point out of the domain; if this happens, the update is limited.  For now this limiting
is done in a naive way; for example we may restrict the step size to 50% of the distance to the nearest boundary.</p>
<p>We may also limit very large updates (controlled via max_relative_change).  For very sensitive problems like hyperparameter
optimization, max_relative_change = 0.02 is suggested; for less sensitive problems (e.g., EI, especially analytic),
you can use 1.0.</p>
<p>The constraint implementation (no stepping outside the domain), the large update limiting, and the hessian modification
are not &#8220;pure&#8221; Newton approaches.  They are all heuristics meant to improve Newton&#8217;s robustness.  The constraint implementation
in particular may lead to non-convergence and it also may not find constrained optima that lie exactly on a boundary.  This would
require a more general handling where we search in an d-1 dimensional subspace (i.e., only on the boundary).</p>
<p>Finally, note that we are using an absolute tolerance here, based on magnitude of the gradient, not step distance!  (Contrast
this with tolerance in gradient descent.)
The suggested value is 1.0e-13, although this may need to be
loosened for ill-conditioned problems.  Setting too high of a tolerance can cause wrong answers&#8211;e.g., we stop at a point
that is not an optima but simply an region with small gradient.  Setting the tolerance too low will make convergence impossible
due to loss of accuracy through numerical effects.</p>
<p>TODO(GH-161): Investigate/add stagnation detection to newton, so it stops when going too many steps without improving the result.</p>
<p>TODO(GH-133): (GH-134) Improve Newton&#8217;s performance/robustness.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">this method does not check the eigenvalues of H at this solution to verify that it is an optima and not a saddle
or an indeterminate result.
TODO(GH-121): Add optima classification to Newton.</p>
</div>
<p>Solution is guaranteed to lie within the region specified by &#8220;domain&#8221;; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">in general, you should not call/instantiate this function directly.  Instead, create a NewtonOptimizer object
and call its ::Optimize() function.</p>
</div>
<p>problem_size refers to objective_state-&gt;GetProblemSize(), the number of dimensions in a &#8220;point&#8221; aka the number of
variables being optimized.  (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.)</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_evaluator:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">reference to object that can compute the objective function and its gradient</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">newton_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">NewtonParameters object that describes the parameters newton optimization
(e.g., number of iterations, tolerances, additional diagonal dominance)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a properly configured state object for the ObjectiveFunctionEvaluator template parameter
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a state object whose temporary data members may have been modified
objective_state.GetCurrentPoint() will return the point yielding the best objective function value
according to newton</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>number of errors</dd>
</dl>
 </p>
</div></blockquote>
</div></blockquote>
<p><p id="project0structoptimal__learning_1_1_optimization_i_o_container"><em>class</em> <strong>OptimizationIOContainer</strong></p>
<blockquote>
<div><p></p>
<p><p>This object holds the input/output fields for optimizers (maximization).  On input, this can be used to specify the current
best known point (i.e., the optimizer will indicate no new optima found if it cannot beat this value).
Upon completion, this struct should be read to determine the result of optimization.</p>
<p>Since this object is used to communicate some inputs/outputs to the various optimization functions, any function
using this object MUST obey its contract.</p>
<p>The contract:
On input, the optimizer will read best_objective_value_so_far.
IF optimization results in a LARGER objective value, then:</p>
<blockquote>
<div><ol class="arabic simple">
<li>best_objective_value_so_far will be set to that new larger value</li>
<li>best_point will be set to the point producing this new larger objective value</li>
<li>found_flag will be SET to true</li>
</ol>
</div></blockquote>
<p>ELSE:</p>
<blockquote>
<div><ol class="arabic simple">
<li>best_objective_value_so_far will be unmodified</li>
<li>best_point will be unmodified</li>
<li>found_flag will be SET to false</li>
</ol>
</div></blockquote>
<p>The idea is for the user to be able to indicate what an improvement is.  For example, to optimize log likelihood as a
function of hyperparameters, we could do:
best_point = <tt class="docutils literal"><span class="pre">argmax_{x</span> <span class="pre">\in</span> <span class="pre">initial_guesses}</span> <span class="pre">f(x)</span></tt>
best_objective_value = <tt class="docutils literal"><span class="pre">f(best_point)</span></tt>
And then call multistart gradient descent (MGD).  Now, MGD will only change the best point/value if it converges to a better
solution.  If convergence fails or MGD settles on a <em>worse</em> local maxima, found_flag will be SET to false, and the other
fields of IOContainer will be <em>unmodified</em>.  If it finds a better solution, then found_flag will be SET to true and
the other fields will report the new solution.</p>
 </p>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1aa422a9f9544c977325546d46bd5ff883"></span><div class="line-block">
<div class="line"> <strong>OptimizationIOContainer</strong>(int problem_size_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Build an empty OptimizationIOContainer.  best_objective_value and best_point are initialized to zero; THIS MAY
BE AN INVALID STATE.  See class docs for details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">problem_size:</th><td class="field-body">number of dimensions in the optimization problem (e.g., size of best_point)</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1a5163c056fc667cfad92098d71c32e69c"></span><div class="line-block">
<div class="line"> <strong>OptimizationIOContainer</strong>(int problem_size_in, double best_objective_value, double const *restrict best_point_in)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Build and fully initialize a OptimizationIOContainer.  See class docs for details.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">problem_size:</th><td class="field-body">number of dimensions in the optimization problem (e.g., size of best_point)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">best_objective_value:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">the best objective function value seen so far</td>
</tr>
<tr class="field-odd field"><th class="field-name">best_point:</th><td class="field-body">the point to associate with best_objective_value</td>
</tr>
</tbody>
</table>
</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1aa1917fdf59d9e9e999277adb836a04f1"></span><div class="line-block">
<div class="line"> <strong>OptimizationIOContainer</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_optimization_i_o_container"><em>OptimizationIOContainer</em></a>  &amp;&amp; OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1a89e992046a295e03e468d9e337042837"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_DEFAULT_AND_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0structoptimal__learning_1_1_optimization_i_o_container"><em>OptimizationIOContainer</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Members</em><blockquote>
<div><p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1a7c655df775b996490aa126db11ca77d1"></span>const int <strong>problem_size</strong></p>
<blockquote>
<div><p>spatial dimension (e.g., dimensions of a point in points_sampled, num_hyperparameters) </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1a2a77d6c1e747cf874626f2721dcac165"></span>double <strong>best_objective_value_so_far</strong></p>
<blockquote>
<div><p>the best objective function value seen </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1a4d97c311ea60e84e1c578569f8fa3a0b"></span>std::vector&lt; double &gt; <strong>best_point</strong></p>
<blockquote>
<div><p>the point producing <tt class="docutils literal"><span class="pre">best_objective_value_so_far</span></tt> after successful optimizzation (<tt class="docutils literal"><span class="pre">found_flag</span> <span class="pre">=</span> <span class="pre">true</span></tt>); otherwise it contains the original, unmodified values from when the function was called </p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0structoptimal__learning_1_1_optimization_i_o_container_1ae9cd7a94c530c34d4a9faf9d5fb9eb51"></span>bool <strong>found_flag</strong></p>
<blockquote>
<div><p>true if the optimizer found improvement </p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_null_optimizer"><em>class</em> <strong>NullOptimizer</strong></p>
<blockquote>
<div><p></p>
<p><p>The &#8220;null&#8221; or identity optimizer: it does nothing, giving the same output its inputs
This is useful to allow the multistart optimizer template to be reused for &#8216;dumb&#8217; searches and
nontrivial optimization.  In the former, we just need to evaluate the objective at each initial guess,
so there is no optimization to be done at each point (hence null optimizer).  In the latter,
we kick off an optimization run (e.g., gradient descent, newton) at each initial guess.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1a785b3c37a1b2cd8d2c8bbd08f96435a6"></span>typedef ObjectiveFunctionEvaluator_ <strong>ObjectiveFunctionEvaluator</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1a5fcf5d9ecdc73c735015d3d1bcd094a2"></span>typedef DomainType_ <strong>DomainType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1ae28c9502f94afd88e2087bb544ff0ac2"></span>typedef <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_null_parameters"><em>NullParameters</em></a> <strong>ParameterStruct</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1a0a44fda7b7fd06fc0ac7863739ab7c53"></span><div class="line-block">
<div class="line"> <strong>NullOptimizer</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1ab7a27ee7863466bcb954d6ee34d448d0"></span><div class="line-block">
<div class="line">int <strong>Optimize</strong>(const ObjectiveFunctionEvaluator &amp; OL_UNUSED, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_null_parameters"><em>ParameterStruct</em></a>  &amp; OL_UNUSED, const DomainType &amp; OL_UNUSED, typename ObjectiveFunctionEvaluator::StateType * OL_UNUSED)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Perform a null optimization: this does nothing.</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a properly configured state object for the ObjectiveFunctionEvaluator template parameter
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a state object whose temporary data members may have been modified
objective_state.GetCurrentPoint() will return the point as the intial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>number of errors, always 0</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_null_optimizer_1ab4a4798a291ca4e435ef43ad84beb1c8"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_null_optimizer"><em>NullOptimizer</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_gradient_descent_optimizer"><em>class</em> <strong>GradientDescentOptimizer</strong></p>
<blockquote>
<div><p></p>
<p><p>Gradient descent (GD) optimization.  This class optimizes using restarted GD (see comments on the Optimize()) function.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1a6fed47dd785041f085659250e69b9367"></span>typedef ObjectiveFunctionEvaluator_ <strong>ObjectiveFunctionEvaluator</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1adf9bb78287d628579f0de84a5bd2b949"></span>typedef DomainType_ <strong>DomainType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1a19d35e11ee0400f716659f58e5c28106"></span>typedef <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>GradientDescentParameters</em></a> <strong>ParameterStruct</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1aa289b7f8394996b53de30a1d49bb79b9"></span><div class="line-block">
<div class="line"> <strong>GradientDescentOptimizer</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1a765cd72296fd7e08755fe9c2bde1b083"></span><div class="line-block">
<div class="line">int <strong>Optimize</strong>(const ObjectiveFunctionEvaluator &amp; objective_evaluator, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_gradient_descent_parameters"><em>ParameterStruct</em></a>  &amp; gd_parameters, const DomainType &amp; domain, typename ObjectiveFunctionEvaluator::StateType * objective_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Optimize a given objective function (represented by ObjectiveFunctionEvaluator; see file comments for what this must provide)
using restarted gradient descent (GD).</p>
<p>See section 2a) and 3b, i) in the header docs and the docs for GradientDescentOptimization() for more details.</p>
<p>Guaranteed to call GradientDescentOptimization() AT MOST max_num_restarts times.
GradientDescentOptimization() implements gradient descent; see function comments above for details.
This method calls gradient descent, then restarts (by calling GD again) from the GD&#8217;s result point.  This is done until
max_num_restarts is reached or the result point stops changing (compared to tolerance).</p>
<p>Note that we are using an absolute tolerance, based on the size of the most recent step*.  Here, &#8216;step&#8217; is the
distance covered by the last restart, not the last GD iteration (as in GradientDescentOptimization()).
The suggested value is 1.0e-7, although this may need to be loosened for problems with &#8216;difficult&#8217; optima (e.g., the shape
is not locally very peaked).  Setting too high of a tolerance can cause wrong answers&#8211;e.g., we stop at a point
that is not an optima but simply an region with small gradient.  Setting the tolerance too low may make convergence impossible;
GD could get stuck (bouncing between the same few points) or numerical effects could make it impossible to satisfy tolerance.</p>
<p>* As opposed to say based on changes in the objective function.</p>
<p>Solution is guaranteed to lie within the region specified by &#8220;domain&#8221;; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<p>problem_size refers to objective_state-&gt;GetProblemSize(), the number of dimensions in a &#8220;point&#8221; aka the number of
variables being optimized.  (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.)</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_evaluator:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">reference to object that can compute the objective function and its gradient</td>
</tr>
<tr class="field-even field"><th class="field-name">gd_parameters:</th><td class="field-body">GradientDescentParameters object that describes the parameters controlling gradient descent optimization
(e.g., number of iterations, tolerances, learning rate)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a properly configured state object for the ObjectiveFunctionEvaluator template parameter
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a state object whose temporary data members may have been modified
objective_state.GetCurrentPoint() will return the point yielding the best objective function value
according to gradient descent</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>number of errors, always 0</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_gradient_descent_optimizer_1a3d0c7d17f7b629c9acb5fbd6f917c4c3"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_gradient_descent_optimizer"><em>GradientDescentOptimizer</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_newton_optimizer"><em>class</em> <strong>NewtonOptimizer</strong></p>
<blockquote>
<div><p></p>
<p><p>Newton optimization.  This class optimizes using Newton&#8217;s method with a refinement step (see comments on the Optimize()) function.</p>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1a726c4bd21a58acee61799bd854ec9141"></span>typedef ObjectiveFunctionEvaluator_ <strong>ObjectiveFunctionEvaluator</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1a2c8ff4c1c3c3a1e53b10cfafe9833c01"></span>typedef DomainType_ <strong>DomainType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1aa6d382182350c4331c7c8d7afb42359e"></span>typedef <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_newton_parameters"><em>NewtonParameters</em></a> <strong>ParameterStruct</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1aac39f5f8d2403ef2d29f3a44dd2106f2"></span><div class="line-block">
<div class="line"> <strong>NewtonOptimizer</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1a04edf0db5fdc61589ae0e73ef2de01df"></span><div class="line-block">
<div class="line">int <strong>Optimize</strong>(const ObjectiveFunctionEvaluator &amp; objective_evaluator, const  <a class="reference internal" href="gpp_optimization_parameters.html#project0structoptimal__learning_1_1_newton_parameters"><em>ParameterStruct</em></a>  &amp; newton_parameters, const DomainType &amp; domain, typename ObjectiveFunctionEvaluator::StateType * objective_state)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Uses Newton&#8217;s Method to optimize the value of an objective function, f (e.g., log marginal likelihood).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">this function wraps NewtonOptimization(), see above.  It first calls that function directly, then calls it again
with a modified newton_parameters struct: the param struct is modified to run newton with a small number
of iterations at a huge time_factor (to remove the diagonal dominance adjustment entirely).  We do this
to ensure that Newton has converged.</p>
</div>
<p>See section 2b) and 3b, ii) in the header docs and the docs for NewtonOptimization() for more details.</p>
<p>Solution is guaranteed to lie within the region specified by &#8220;domain&#8221;; note that this may not be a
true optima (i.e., the gradient may be substantially nonzero).</p>
<p>problem_size refers to objective_state-&gt;GetProblemSize(), the number of dimensions in a &#8220;point&#8221; aka the number of
variables being optimized.  (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.)</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_evaluator:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">reference to object that can compute the objective function and its gradient</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">newton_parameters:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">NewtonParameters object that describes the parameters newton optimization
(e.g., number of iterations, tolerances, additional diagonal dominance)</td>
</tr>
<tr class="field-odd field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">a properly configured state object for the ObjectiveFunctionEvaluator template parameter
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">a state object whose temporary data members may have been modified
objective_state.GetCurrentPoint() will return the point yielding the best objective function value
according to newton</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Returns</strong>:</dt>
<dd>number of errors</dd>
</dl>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_newton_optimizer_1a0f96ea38c80aece003c386b620e01c7f"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_newton_optimizer"><em>NewtonOptimizer</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
<p><p id="project0classoptimal__learning_1_1_multistart_optimizer"><em>class</em> <strong>MultistartOptimizer</strong></p>
<blockquote>
<div><p></p>
<p><p>This is a general, template class for multistart optimization.  It is designed to be used with the various Optimizer
classes in this file (e.g., NullOptimizer, GradientDescentOptimizer, NewtonOptimizer).  The multistart process is
multithreaded using OpenMP so that we can start from multiple initial guesses across multiple threads simultaneously.
See section 2c) and 3b, iii) in the header docs at the top of the file for more details.</p>
<p>The use with GradientDescentOptimizer, NewtonOptimizer, etc. are standard practice in nonlinear optimization.  In particular,
without special properties like convexity, single-start optimizers can converge to local optima.  In general, a nonlinear
function can have many local optima, so the only way to improve* your chances of finding the global optimum is to start
from many different locations.  This will be the typical use case for MultistartOptimizer&lt;...&gt;::MultistartOptimize().</p>
<p>* Improve is intentional here.  In the general case, you are not <em>guaranteed</em> (in finite time) to find the global optimum.</p>
<p>Use with NullOptimizer requires special mention here as it might seem silly. This case reduces to evaluating the
objective function at every point of initial_guesses.  Through function_values, you can get the objective value at each
of point of initial_guesses too (e.g., for plotting).  So use MultistartOptimize with NullOptimzer to perform a
&#8216;dumb&#8217; search (e.g., initial_guesses can be obtained from a grid, random sampling, etc.).  NullOptimizer allows &#8216;dumb&#8217; search
to use the same code as multistart optimization.  &#8216;Dumb&#8217; search is inaccurate but it never fails, so we often use it as a
fall-back when more advanced (e.g., gradient descent) techniques fail.</p>
<p>This class provides just one method (for now), MultistartOptimize(); see below.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments copied to MultistartOptimizer in python_version/optimization.py.</p>
</div>
 </p>
<em>Public Type</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1a15e8c90a62a64d2aaaab6da92f35f163"></span>typedef Optimizer_ <strong>Optimizer</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1a382af82d271291b91cc55ce28ffb7b19"></span>typedef typename Optimizer::ObjectiveFunctionEvaluator <strong>ObjectiveFunctionEvaluator</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1aa29854a073beb8a670e0bc3f6ed1b3e1"></span>typedef typename Optimizer::DomainType <strong>DomainType</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1aee1cd111d26952655fde24ee34fbf389"></span>typedef typename Optimizer::ParameterStruct <strong>ParameterStruct</strong></p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
<em>Public Functions</em><blockquote>
<div><p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1ad8385ef4ae561dc3eb2e084af35e6212"></span><div class="line-block">
<div class="line"> <strong>MultistartOptimizer</strong>()</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1af3dc4e8faefa9bdb0159707bc48d889d"></span><div class="line-block">
<div class="line">void <strong>MultistartOptimize</strong>(const Optimizer &amp; optimizer, const ObjectiveFunctionEvaluator &amp; objective_evaluator, const ParameterStruct &amp; optimizer_parameters, const DomainType &amp; domain, double const *restrict initial_guesses, int num_multistarts, int max_num_threads, int chunk_size, typename ObjectiveFunctionEvaluator::StateType * objective_state_vector, double *restrict function_values, <a class="reference internal" href="#project0structoptimal__learning_1_1_optimization_i_o_container"><em>OptimizationIOContainer</em></a>  *restrict io_container)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p><p>Performs multistart optimization with the specified Optimizer (class template parameter) to optimize the specified
ObjectiveFunctionEvaluator over the specified DomainType. Optimizer behavior is controlled by the specified ParameterStruct.
See class docs and header docs of this file, section 2c and 3b, iii), for more information.</p>
<p>The method allows you to specify what the current best is, so that if optimization cannot beat it, no improvement will be
reported.  It will otherwise report the overall best improvement (through io_container) as well as the result of every
individual multistart run if desired (through function_values).</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">comments copied to MultistartOptimizer.optimize() in python_version/optimization.py.</p>
</div>
<p>Generally, you will not call this function directly.  Instead, it is intended to be used in wrappers that set up state,
chunk_size, etc. for the specific optimization problem at hand.  For examples with Expected Improvement (EI), see gpp_math:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">EvaluateEIAtPointList()</span></tt></li>
<li><tt class="docutils literal"><span class="pre">ComputeOptimalPointsToSampleViaMultistartGradientDescent()</span></tt></li>
</ul>
<p>or gpp_model_selection:</p>
<ul class="simple">
<li><tt class="docutils literal"><span class="pre">EvaluateLogLikelihoodAtPointList()</span></tt></li>
<li><tt class="docutils literal"><span class="pre">MultistartGradientDescentHyperparameterOptimization()</span></tt></li>
<li><tt class="docutils literal"><span class="pre">MultistartNewtonHyperparameterOptimization()</span></tt></li>
</ul>
<p>problem_size refers to objective_state-&gt;GetProblemSize(), the number of dimensions in a &#8220;point&#8221; aka the number of
variables being optimized.  (This might be the spatial dimension for EI or the number of hyperparameters for log likelihood.)</p>
<dl class="docutils">
<dt><strong>Parameters</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">optimizer:</th><td class="field-body">object with the desired Optimize() functionality (e.g., do nothing for &#8216;dumb&#8217; search, gradient descent, etc.)</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">objective_evaluator:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">reference to object that can compute the objective function, its gradient, and/or its hessian,
depending on the needs of optimizer</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">optimizer_parameters:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">Optimizer::ParameterStruct object that describes the parameters for optimization
(e.g., number of iterations, tolerances, scale factors, etc.)</td>
</tr>
<tr class="field-even field"><th class="field-name">domain:</th><td class="field-body">object specifying the domain to optimize over (see gpp_domain.hpp)</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">initial_guesses[problem_size][num_multistarts]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">list of points at which to start optimization runs; all points must lie
INSIDE the specified domain</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">num_multistarts:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">number of random points to use from initial guesses</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">max_num_threads:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">maximum number of threads for use by OpenMP (generally should be &lt;= # cores)</td>
</tr>
<tr class="field-even field"><th class="field-name">chunk_size:</th><td class="field-body">(a guide to) how much work to give each thread at a time, see gpp_common.hpp header comments, item 7</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state_vector[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">properly constructed/configured ObjectiveFunctionEvaluator::State objects,
at least one per thread
objective_state.GetCurrentPoint() will be used to obtain the initial guess</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">io_container[1]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">object with best_objective_value_so_far and corresponding best_point properly initialized.
See struct docs in gpp_optimization.hpp for details.</td>
</tr>
</tbody>
</table>
</dd>
<dt><strong>Outputs</strong>:</dt>
<dd><table class="first last docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name" colspan="2">objective_state_vector[max_num_threads]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">internal states of state objects may be modified</td>
</tr>
<tr class="field-even field"><th class="field-name" colspan="2">function_values[num_multistarts]:</th></tr>
<tr class="field-even field"><td>&nbsp;</td><td class="field-body">objective fcn value at the end of each optimization run, in the same order as
initial_guesses. Can be used to check what each optimization run converged to.
More commonly used only with NullOptimizer to get a list of objective values
at each point of initial_guesses.  Never dereferenced if nullptr</td>
</tr>
<tr class="field-odd field"><th class="field-name" colspan="2">io_container[1]:</th></tr>
<tr class="field-odd field"><td>&nbsp;</td><td class="field-body">object container new best_objective_value_so_far and corresponding best_point IF found_flag is true.
unchanged from input otherwise. See struct docs in gpp_optimization.hpp for details.</td>
</tr>
</tbody>
</table>
</dd>
</dl>
<p>TODO(GH-150): consider having multiple versions of this for static/guided/dynamic scheduling.
Unforutnately openmp doesn&#8217;t let you choose that parameter programmatically. This would be nice for testing.
enough</p>
 </p>
</div></blockquote>
<p><span class="target" id="project0classoptimal__learning_1_1_multistart_optimizer_1aea5884c0885f955dc290d2b0a73aeac5"></span><div class="line-block">
<div class="line"> <strong>OL_DISALLOW_COPY_AND_ASSIGN</strong>(<a class="reference internal" href="#project0classoptimal__learning_1_1_multistart_optimizer"><em>MultistartOptimizer</em></a>)</div>
</div>
</p>
<blockquote>
<div><p></p>
<p></p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</p>
</div></blockquote>
</p>
</div>
</div>


          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="gpp_covariance_test.html" class="btn btn-neutral float-right" title="gpp_covariance_test"/>Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="gpp_core.html" class="btn btn-neutral" title="gpp_core"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2014, Yelp and Cornell Collaboration.
    </p>
  </div>

  <a href="https://github.com/snide/sphinx_rtd_theme">Sphinx theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>
</footer>
        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.1.0',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>